
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="采用Mkdocs-material生成的文档管理网站支持的markdown语法，包括传统语法和扩展语法">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>决策树算法原理(三) - 我的知识笔记</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.9647289d.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="pink">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="我的知识笔记" class="md-header__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            我的知识笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              决策树算法原理(三)
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-tabs__link md-tabs__link--active">
        机器学习
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-tabs__link">
        深度学习
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="我的知识笔记" class="md-nav__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    我的知识笔记
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          01 监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="01 监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          01 监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-nav__link">
        数学符号
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../00_%E6%80%BB%E7%BB%93/" class="md-nav__link">
        线性模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" class="md-nav__link">
        最小二乘法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" class="md-nav__link">
        梯度下降算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8E%9F%E7%90%86/" class="md-nav__link">
        交叉验证原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="md-nav__link">
        模型评估
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        线性回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        逻辑回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        决策树算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        决策树算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          决策树算法原理(三)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        决策树算法原理(三)
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    最优特征选择方法
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    特征处理的改进
  </a>
  
    <nav class="md-nav" aria-label="特征处理的改进">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    连续值处理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    离散值处理
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cart" class="md-nav__link">
    CART分类树的建立
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cart_1" class="md-nav__link">
    CART回归树的建立
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cart_2" class="md-nav__link">
    CART树的剪枝
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        近邻算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        朴素贝叶斯算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        最大熵算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        最大熵算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        最大熵算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        感知机算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        支持向量机算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        支持向量机算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        支持向量机算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../19_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E5%9B%9B%29/" class="md-nav__link">
        支持向量机算法原理(四)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../20_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%94%29/" class="md-nav__link">
        支持向量机算法原理(五)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../21_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        集成学习算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../22_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BAdaboost/" class="md-nav__link">
        集成学习算法之Adaboost
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../23_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/" class="md-nav__link">
        集成学习算法之GBDT
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../24_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BRF/" class="md-nav__link">
        集成学习算法之RF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          02 无监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="02 无监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          02 无监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/25_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BK-Means/" class="md-nav__link">
        聚类算法之K-Means​
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/26_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BBIRCH/" class="md-nav__link">
        聚类算法之BIRCH
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/27_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BDBSCAN/" class="md-nav__link">
        聚类算法之DBSCAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/28_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BMean%20Shift/" class="md-nav__link">
        聚类算法之Mean Shift
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/29_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E8%B0%B1%E8%81%9A%E7%B1%BB/" class="md-nav__link">
        聚类算法之谱聚类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/30_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BPCA/" class="md-nav__link">
        降维算法之PCA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/31_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLDA/" class="md-nav__link">
        降维算法之LDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/32_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BIsomap/" class="md-nav__link">
        降维算法之Isomap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/33_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLLE/" class="md-nav__link">
        降维算法之LLE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/34_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BApriori/" class="md-nav__link">
        关联算法之Apriori
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/35_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BFP-Tree/" class="md-nav__link">
        关联算法之FP-Tree
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/36_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        推荐算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/37_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/" class="md-nav__link">
        推荐算法之矩阵分解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/38_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BSimRank/" class="md-nav__link">
        推荐算法之SimRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/39_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BPersonalRank/" class="md-nav__link">
        推荐算法之PersonalRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/40_EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        EM算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/41_%E5%88%86%E8%A7%A3%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        分解机算法原理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="深度学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        神经网络基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01_DNN%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        DNN前馈神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        CNN卷积神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        RNN循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04_%E5%9F%BA%E4%BA%8E%E9%97%A8%E6%8E%A7%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        基于门控的循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%281%29/" class="md-nav__link">
        神经网络优化概述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%282%29/" class="md-nav__link">
        数据预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06_%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" class="md-nav__link">
        网络正则化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="md-nav__link">
        07 注意力机制
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    最优特征选择方法
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    特征处理的改进
  </a>
  
    <nav class="md-nav" aria-label="特征处理的改进">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    连续值处理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    离散值处理
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cart" class="md-nav__link">
    CART分类树的建立
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cart_1" class="md-nav__link">
    CART回归树的建立
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cart_2" class="md-nav__link">
    CART树的剪枝
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="_1">决策树算法原理(三)<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<p>分类与回归树(classification and regression tree, CART)模型是应用广泛的决策树学习算法。CART由特征选择、树的生成以及树的剪枝三部分组成，既可以用于分类也可以用于回归。</p>
<p>CART是在给定输入随机变量<span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>条件下输出随机变量<span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span>的条件概率分布的学习方法。CART算法中决策树是二叉树，内部节点特征的取值为"是"和"否"，左分支的取值为"是"的分支，右分支是取值为"否"的分支。这样的决策树等价于递归地二分每个特征，将输入的特征空间划分为有限个单元，并在这些单元上确定预测的概率分布。</p>
<h2 id="_2">最优特征选择方法<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>在ID3算法中使用信息增益来选择特征，信息增益大的优先选择。为了减少信息增益容易选择特征值较多的特征的问题，在C4.5算法中采用了信息增益比来选择特征。但是无论ID3还是C4.5，都是基于信息论的熵模型，计算过程涉及了大量的对数运算。CART分类树算法使用基尼系数来代替信息增益比，即可以简化模型又不至于完全丢失熵模型的优点。基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这个信息增益(比)是相反的。</p>
<p>在分类问题中，假设有<span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>个类别，第<span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>个类别的概率为<span class="arithmatex"><span class="MathJax_Preview">p_k</span><script type="math/tex">p_k</script></span>，则基尼系数的表达式：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
</div>
<script type="math/tex; mode=display">
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
</script>
</div>
<p>如果是二分类问题，计算公式(1)可以进一步简化，假设属于第一个样本输出的概率是<span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>，那么基尼系数的表达式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Gini(p)=1-\sum_{k=1}^{2}p_k^2=1-p^2-(1-p)^2=2p(1-p)
</div>
<script type="math/tex; mode=display">
Gini(p)=1-\sum_{k=1}^{2}p_k^2=1-p^2-(1-p)^2=2p(1-p)
</script>
</div>
<p>对于给定的样本<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>，假设有<span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>个类别，第<span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>个类别的数量为<span class="arithmatex"><span class="MathJax_Preview">C_k</span><script type="math/tex">C_k</script></span>，则样本<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>的基尼系数表达式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2
</div>
<script type="math/tex; mode=display">
Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2
</script>
</div>
<p>特别地，对于样本<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>，如果根据特征<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>的某个值<span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>将数据集<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>划分为了<span class="arithmatex"><span class="MathJax_Preview">D_1</span><script type="math/tex">D_1</script></span>和<span class="arithmatex"><span class="MathJax_Preview">D_2</span><script type="math/tex">D_2</script></span>，则在特征<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>的条件下，<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>的基尼系数表达式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
</div>
<script type="math/tex; mode=display">
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
</script>
</div>
<p>可以看出基尼系数表达式相比于熵模型的表达式，二次运算比对数运算简单得多，尤其是二类分类的计算。</p>
<h2 id="_3">特征处理的改进<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<h3 id="_4">连续值处理<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<p>对于CART分类树处理连续值的问题，其思想和C4.5是相同的，都是将连续的特征离散化。唯一的区别在于选择划分点时的度量方式不同：C4.5算法使用的是信息增益比，CART算法使用的是基尼系数。</p>
<p>如有<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>个样本的连续特征<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>，从小到达排列为<span class="arithmatex"><span class="MathJax_Preview">a_1,a_2,...,a_m</span><script type="math/tex">a_1,a_2,...,a_m</script></span>，则CART算法取相邻两样本的平均数，一共取得<span class="arithmatex"><span class="MathJax_Preview">m-1</span><script type="math/tex">m-1</script></span>个划分点，其中第<span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>个划分点<span class="arithmatex"><span class="MathJax_Preview">T_i</span><script type="math/tex">T_i</script></span>表示为<span class="arithmatex"><span class="MathJax_Preview">T_i=\frac{a_i+a_{i+1}}{2}</span><script type="math/tex">T_i=\frac{a_i+a_{i+1}}{2}</script></span>。对于这<span class="arithmatex"><span class="MathJax_Preview">m-1</span><script type="math/tex">m-1</script></span>个点，分别计算以改点作为二元分类点时的基尼系数，选择基尼系数最小的点作为该连续特征的二元离散分类点。这样就可以将连续特征离散化。</p>
<p>需要注意的是：<strong>与离散特征不同的是，如果当前节点是连续特征，则该属性后面还可以参与子节点的产生选择过程</strong>。</p>
<h3 id="_5">离散值处理<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>在ID3算法或者C4.5算法中，如果某个特征<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>被选取建立决策树节点，如果它有<span class="arithmatex"><span class="MathJax_Preview">a_1,a_2,a_3</span><script type="math/tex">a_1,a_2,a_3</script></span>三种类别，那么在决策树上会建立一个三叉的节点，最后的决策树往往是一个多叉树。但是在CART决策树中采用的方法是不停的二分。</p>
<p>比如在CART决策树中会考虑将<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>分成<span class="arithmatex"><span class="MathJax_Preview">\{ a_1\}</span><script type="math/tex">\{ a_1\}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\{a_2,a_3\}</span><script type="math/tex">\{a_2,a_3\}</script></span>，<span class="arithmatex"><span class="MathJax_Preview">\{ a_2\}</span><script type="math/tex">\{ a_2\}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\{a_1,a_3\}，</span><script type="math/tex">\{a_1,a_3\}，</script></span><span class="arithmatex"><span class="MathJax_Preview">\{a_3\}</span><script type="math/tex">\{a_3\}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\{a_1,a_2\}</span><script type="math/tex">\{a_1,a_2\}</script></span>三种情况，找到基尼系数最小的组合如<span class="arithmatex"><span class="MathJax_Preview">\{ a_2\}</span><script type="math/tex">\{ a_2\}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\{a_1,a_3\}</span><script type="math/tex">\{a_1,a_3\}</script></span>，然后建立二叉树节点，一个节点对应样本<span class="arithmatex"><span class="MathJax_Preview">a_2</span><script type="math/tex">a_2</script></span>，另一个节点对应是<span class="arithmatex"><span class="MathJax_Preview">\{a_1,a_3\}</span><script type="math/tex">\{a_1,a_3\}</script></span>的节点。由于这次并没有把特征<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>的值完全分开，后续我们还有机会在子节点中继续选择特征<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>来划分<span class="arithmatex"><span class="MathJax_Preview">a_1</span><script type="math/tex">a_1</script></span>和<span class="arithmatex"><span class="MathJax_Preview">a_3</span><script type="math/tex">a_3</script></span>。</p>
<p>这里和ID3或者C4.5不同，在ID3或者C4.5中，离散特征只会参与一次节点的建立。</p>
<h2 id="cart">CART分类树的建立<a class="headerlink" href="#cart" title="Permanent link">&para;</a></h2>
<p>CART算法由两步组成：①决策树的生成：基于训练数据集生成决策树，生成的决策树要尽量大；②决策树的剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，采用损失函数最小作为剪枝的标准。如下给出了CART分类树的建立算法：</p>
<blockquote>
<p>输入：训练数据集<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>，基尼系数的阈值<span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>，样本个数阈值</p>
<p>输出：决策树<span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span></p>
<p>(1)对于当前节点的数据集<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归；</p>
<p>(2)计算样本集<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>的基尼系数，如果基尼系数大于阈值，则返回决策子树，当前节点停止递归；</p>
<p>(3)计算当前节点现有的各个特征的各个特征值对数据集<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>的基尼系数，对于离散值、连续值和缺失值的处理如上介绍；</p>
<p>(4)在计算出来的各个特征值对数据集<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>的基尼系数中，选择基尼系数最小的特征<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>和对应的特征值<span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span>。根据这个最优特征和最优特征值，将数据集<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>划分为<span class="arithmatex"><span class="MathJax_Preview">D_1</span><script type="math/tex">D_1</script></span>和<span class="arithmatex"><span class="MathJax_Preview">D_2</span><script type="math/tex">D_2</script></span>，同时建立当前节点的左右节点，左节点的数据集<span class="arithmatex"><span class="MathJax_Preview">D_1</span><script type="math/tex">D_1</script></span>，右节点的数据集<span class="arithmatex"><span class="MathJax_Preview">D_2</span><script type="math/tex">D_2</script></span>。</p>
<p>(5)对左右的子节点递归调用(1)~(4)，生成决策树。</p>
</blockquote>
<p>使用生成的决策树做预测的时候，假设测试集里的样本<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>落到了某个叶子节点，而节点里面有很多的训练样本。则将这个预测样本预测为这个叶子节点里概率最大的类别。</p>
<h2 id="cart_1">CART回归树的建立<a class="headerlink" href="#cart_1" title="Permanent link">&para;</a></h2>
<p>CART回归树和CART分类树的建立算法大部分都是类似的，不同的地方主要有：</p>
<p>(1)分类树的样本输出是离散值，回归树的样本输出是连续值</p>
<p>(2)连续值的处理方法不同</p>
<p>(3)决策树建立后样本预测的方式不同</p>
<p>对于连续值的处理，CART分类树采用的是基尼系数的大小来度量特征的各个划分点的优劣情况。对于回归模型，采用的是常见的方差度量方式，选择第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>个变量<span class="arithmatex"><span class="MathJax_Preview">x_j</span><script type="math/tex">x_j</script></span>和它取值<span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span>，作为划分变量和切分点，并定义两个区域：
$$
\begin{align}
D_1(j,s)={ x|x_j\le s }\
D_2(j,s)={ x|x_j\gt s }
\end{align}
$$
求出使<span class="arithmatex"><span class="MathJax_Preview">D_1</span><script type="math/tex">D_1</script></span>和<span class="arithmatex"><span class="MathJax_Preview">D_2</span><script type="math/tex">D_2</script></span>各自集合的均方差最小，同时<span class="arithmatex"><span class="MathJax_Preview">D_1</span><script type="math/tex">D_1</script></span>和<span class="arithmatex"><span class="MathJax_Preview">D_2</span><script type="math/tex">D_2</script></span>的均方差之和最小所对应的特征和特征值划分点。表达式如下：
$$
\min_{j,s}[\min_{c_1}{\sum_{x_i\in D_1}(y<sup>i-c_1)</sup>2}+\min_{c_2}{\sum_{x_i\in D_2}(y<sup>i-c_2)</sup>2}]
$$
其中<span class="arithmatex"><span class="MathJax_Preview">c_1</span><script type="math/tex">c_1</script></span>是数据集<span class="arithmatex"><span class="MathJax_Preview">D_1</span><script type="math/tex">D_1</script></span>的样本输出均值，<span class="arithmatex"><span class="MathJax_Preview">c_2</span><script type="math/tex">c_2</script></span>是数据集<span class="arithmatex"><span class="MathJax_Preview">D_2</span><script type="math/tex">D_2</script></span>的样本输出均值。</p>
<p>对于决策树建立后做预测的方式，CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类型。由于回归树输出的不是类别，采用的预测方式是用最终叶子节点的均值或者中位数来作为预测结果。</p>
<h2 id="cart_2">CART树的剪枝<a class="headerlink" href="#cart_2" title="Permanent link">&para;</a></h2>
<p>由于决策时算法很容易对训练集过拟合，从而导致泛泛化能力差。为了解决这个问题，需要对CART树进行剪枝，类似与线性回归的正则化。具体地，剪枝(puring)就是从已生成的树上裁掉一些子树或者叶节点，并将其根节点或者父节点作为新的叶节点，从而简化分类树模型。</p>
<p>剪枝的方法主要有两种：一种是预剪枝，即在生产决策树的时候就决定是否剪枝；另一种是后剪枝，即先生成决策树，在通过交叉验证来剪枝。</p>
<p>CART的剪枝策略主要后剪枝，剪枝采用损失函数作为度量，在剪枝的过程中，对于任意的一颗子树，其损失函数为：
$$
C_\alpha(T_t)=C(T_t)+\alpha|T_t|
$$
其中<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>为正则化参数，<span class="arithmatex"><span class="MathJax_Preview">C(T_t)</span><script type="math/tex">C(T_t)</script></span>为训练数据的预测误差，分类树采用基尼系数度量，回归树采用均方差度量，<span class="arithmatex"><span class="MathJax_Preview">|T_t|</span><script type="math/tex">|T_t|</script></span>是子树<span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>的叶子节点的数量。</p>
<p>当<span class="arithmatex"><span class="MathJax_Preview">\alpha=0</span><script type="math/tex">\alpha=0</script></span>时即没有正则化，原始的生成树CART树即为最优子树。当 <span class="arithmatex"><span class="MathJax_Preview">\alpha =\infty</span><script type="math/tex">\alpha =\infty</script></span> 时即正则化强度达到最大，此时原始的生成树CART树的根节点组成的单节点数为最优子树。这是两种极端的情况。一般来说，<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>越大，剪枝越厉害，生成的最优子树相比原始的的决策树越偏小。</p>
<p>对于节点<span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>的任意子树<span class="arithmatex"><span class="MathJax_Preview">T_t</span><script type="math/tex">T_t</script></span>，如果没有剪枝，其损失函数是：
$$
C_\alpha(T_t)=C(T_t)+\alpha|T_t|
$$
如果对其进行剪枝，仅仅保留根节点，其损失函数是：
$$
C_\alpha(t)=C(t)+\alpha
$$
当<span class="arithmatex"><span class="MathJax_Preview">\alpha=0</span><script type="math/tex">\alpha=0</script></span>或者<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>很小时，<span class="arithmatex"><span class="MathJax_Preview">C_\alpha(T_t)&lt;C_\alpha(t)</span><script type="math/tex">C_\alpha(T_t)<C_\alpha(t)</script></span>；当<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>增大到一定的程度有<span class="arithmatex"><span class="MathJax_Preview">C_\alpha(T_t)=C_\alpha(t)</span><script type="math/tex">C_\alpha(T_t)=C_\alpha(t)</script></span>；当<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>继续增大时不等式反向。当<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>满足下式：
$$
\alpha = \frac{C(t)-C(T_t)}{|T_t|-1}
$$
<span class="arithmatex"><span class="MathJax_Preview">T_t</span><script type="math/tex">T_t</script></span>和<span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>有相同的损失函数，但是<span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>节点更少，因此可以对子树<span class="arithmatex"><span class="MathJax_Preview">T_t</span><script type="math/tex">T_t</script></span>进行剪枝——将它的子节点全部剪掉，变为一个叶子节点<span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>。</p>
<blockquote>
<p>输入：CART算法生成的决策树<span class="arithmatex"><span class="MathJax_Preview">T_0</span><script type="math/tex">T_0</script></span></p>
<p>输出：最优决策树<span class="arithmatex"><span class="MathJax_Preview">T_\alpha</span><script type="math/tex">T_\alpha</script></span></p>
<p>(1)设<span class="arithmatex"><span class="MathJax_Preview">k=0,T=T_0,\alpha=+\infty</span><script type="math/tex">k=0,T=T_0,\alpha=+\infty</script></span></p>
<p>(2)自下而上地对各个内部节点<span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>计算<span class="arithmatex"><span class="MathJax_Preview">C(T_t)</span><script type="math/tex">C(T_t)</script></span>，<span class="arithmatex"><span class="MathJax_Preview">|T_t|</span><script type="math/tex">|T_t|</script></span>以及
$$
\begin{align}
g(t)&amp;=\frac{C(t)-C(T_t)}{|T_t|-1}\
\alpha &amp;=min(\alpha,g(t))
\end{align}
$$
这里<span class="arithmatex"><span class="MathJax_Preview">T_t</span><script type="math/tex">T_t</script></span>表示以<span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>为根节点的子树，<span class="arithmatex"><span class="MathJax_Preview">C(T_t)</span><script type="math/tex">C(T_t)</script></span>是对训练数据的预测误差，<span class="arithmatex"><span class="MathJax_Preview">|T_t|</span><script type="math/tex">|T_t|</script></span>是<span class="arithmatex"><span class="MathJax_Preview">T_t</span><script type="math/tex">T_t</script></span>的叶节点个数</p>
<p>(3)对<span class="arithmatex"><span class="MathJax_Preview">g(t)=\alpha</span><script type="math/tex">g(t)=\alpha</script></span>的内部节点<span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>进行剪枝，并计算叶节点<span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>的值：如果是分类树则选择概率最高的类别，如果是回归树则选择所有样本的均值。得到树<span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>。</p>
<p>(4)设<span class="arithmatex"><span class="MathJax_Preview">k=k+1,\alpha_k=\alpha,T_k=T</span><script type="math/tex">k=k+1,\alpha_k=\alpha,T_k=T</script></span></p>
<p>(5)如果<span class="arithmatex"><span class="MathJax_Preview">T_k</span><script type="math/tex">T_k</script></span>不是由根节点及两个叶节点独构成的树，则返回步骤(2)；否则令<span class="arithmatex"><span class="MathJax_Preview">T_k=T_n</span><script type="math/tex">T_k=T_n</script></span></p>
<p>(6)采用交叉验证法在子树序列<span class="arithmatex"><span class="MathJax_Preview">T_0,T_1,...,T_n</span><script type="math/tex">T_0,T_1,...,T_n</script></span>中选择最优子树<span class="arithmatex"><span class="MathJax_Preview">T_\alpha</span><script type="math/tex">T_\alpha</script></span>。</p>
</blockquote>
<p><strong>CART算法的缺点</strong>：</p>
<p>(1)无论ID3、C4.5还是CART，在特征选择的时候都是选择一个特征来做分类决策，但是大多数情况，分类决策不仅仅由一个特征决定，而是由一组特征决定的。这样的决策树叫做多变量决策树</p>
<p>(2)如果样本发生一点点的改动，会导致树结构剧烈变动。</p>
<p>下面总结了常见的决策树算法有ID3、C4.5、CART等，给出了三者的对比。</p>
<table>
<thead>
<tr>
<th align="center">算法</th>
<th align="center">支持模型</th>
<th align="center">树结构</th>
<th align="center">特征选择</th>
<th align="center">连续值处理</th>
<th align="center">缺失值处理</th>
<th align="center">是否剪枝</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">ID3</td>
<td align="center">分类</td>
<td align="center">多叉树</td>
<td align="center">信息增益</td>
<td align="center">不支持</td>
<td align="center">不支持</td>
<td align="center">否</td>
</tr>
<tr>
<td align="center">C4.5</td>
<td align="center">分类</td>
<td align="center">多叉树</td>
<td align="center">信息增益比</td>
<td align="center">支持</td>
<td align="center">支持</td>
<td align="center">是</td>
</tr>
<tr>
<td align="center">CART</td>
<td align="center">分类/回归</td>
<td align="center">二叉树</td>
<td align="center">基尼系数/均方差</td>
<td align="center">支持</td>
<td align="center">支持</td>
<td align="center">是</td>
</tr>
</tbody>
</table>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../08_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 决策树算法原理(二)" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              决策树算法原理(二)
            </div>
          </div>
        </a>
      
      
        
        <a href="../10_%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-footer__link md-footer__link--next" aria-label="下一页: 近邻算法原理" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              近邻算法原理
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs"], "search": "../../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
        <script src="../../../javascript/config.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>