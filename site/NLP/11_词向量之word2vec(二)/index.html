
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="采用Mkdocs-material生成的文档管理网站支持的markdown语法，包括传统语法和扩展语法">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>词向量之word2vec(二) - 我的知识笔记</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.9647289d.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="pink">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#word2vec" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="我的知识笔记" class="md-header__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            我的知识笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              词向量之word2vec(二)
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../00_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E8%8B%B1%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86/" class="md-tabs__link md-tabs__link--active">
        NLP
      </a>
    </li>
  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-tabs__link">
        机器学习
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-tabs__link">
        深度学习
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="我的知识笔记" class="md-nav__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    我的知识笔记
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          NLP
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          NLP
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../00_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E8%8B%B1%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86/" class="md-nav__link">
        文本挖掘之英文预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86/" class="md-nav__link">
        文本挖掘之中文预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E5%88%86%E8%AF%8D%E5%8E%9F%E7%90%86/" class="md-nav__link">
        文本挖掘之分词原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E5%90%91%E9%87%8F%E5%8C%96/" class="md-nav__link">
        文本挖掘之向量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLSI/" class="md-nav__link">
        主题模型之LSI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BNMF/" class="md-nav__link">
        主题模型之NMF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BpLSA/" class="md-nav__link">
        主题模型之pLSA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLDA%28%E4%B8%80%29/" class="md-nav__link">
        主题模型之LDA(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLDA%28%E4%BA%8C%29/" class="md-nav__link">
        主题模型之LDA(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLDA%28%E4%B8%89%29/" class="md-nav__link">
        主题模型之LDA(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bword2vec%28%E4%B8%80%29/" class="md-nav__link">
        词向量之word2vec(一)
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          词向量之word2vec(二)
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        词向量之word2vec(二)
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#cbow" class="md-nav__link">
    CBOW 模型
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    梯度计算
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#skip-gram" class="md-nav__link">
    Skip-gram模型
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bword2vec%28%E4%B8%89%29/" class="md-nav__link">
        词向量之word2vec(三)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_1" type="checkbox" id="__nav_3_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_1">
          01 监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="01 监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          01 监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-nav__link">
        数学符号
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E6%80%BB%E7%BB%93/" class="md-nav__link">
        线性模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/01_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" class="md-nav__link">
        最小二乘法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/02_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" class="md-nav__link">
        梯度下降算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/03_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8E%9F%E7%90%86/" class="md-nav__link">
        交叉验证原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/04_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="md-nav__link">
        模型评估
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/05_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        线性回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/06_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        逻辑回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/07_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        决策树算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/08_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        决策树算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/09_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        决策树算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/10_%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        近邻算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/11_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        朴素贝叶斯算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/12_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        最大熵算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/13_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        最大熵算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/14_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        最大熵算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/15_%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        感知机算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/16_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        支持向量机算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/17_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        支持向量机算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/18_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        支持向量机算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/19_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E5%9B%9B%29/" class="md-nav__link">
        支持向量机算法原理(四)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/20_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%94%29/" class="md-nav__link">
        支持向量机算法原理(五)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/21_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        集成学习算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/22_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BAdaboost/" class="md-nav__link">
        集成学习算法之Adaboost
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/23_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BRF/" class="md-nav__link">
        集成学习算法之RF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/24_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/" class="md-nav__link">
        集成学习算法之GBDT
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_2">
          02 无监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="02 无监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          02 无监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/25_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BK-Means/" class="md-nav__link">
        聚类算法之K-Means​
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/26_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BBIRCH/" class="md-nav__link">
        聚类算法之BIRCH
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/27_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BDBSCAN/" class="md-nav__link">
        聚类算法之DBSCAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/28_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BMean%20Shift/" class="md-nav__link">
        聚类算法之Mean Shift
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/29_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E8%B0%B1%E8%81%9A%E7%B1%BB/" class="md-nav__link">
        聚类算法之谱聚类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/30_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BPCA/" class="md-nav__link">
        降维算法之PCA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/31_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLDA/" class="md-nav__link">
        降维算法之LDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/32_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BIsomap/" class="md-nav__link">
        降维算法之Isomap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/33_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLLE/" class="md-nav__link">
        降维算法之LLE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/34_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BApriori/" class="md-nav__link">
        关联算法之Apriori
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/35_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BFP-Tree/" class="md-nav__link">
        关联算法之FP-Tree
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/36_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        推荐算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/37_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/" class="md-nav__link">
        推荐算法之矩阵分解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/38_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BSimRank/" class="md-nav__link">
        推荐算法之SimRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/39_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BPersonalRank/" class="md-nav__link">
        推荐算法之PersonalRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/40_EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        EM算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/41_%E5%88%86%E8%A7%A3%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        分解机算法原理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="深度学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        神经网络基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01_DNN%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        DNN前馈神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        CNN卷积神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        RNN循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04_%E5%9F%BA%E4%BA%8E%E9%97%A8%E6%8E%A7%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        基于门控的循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%281%29/" class="md-nav__link">
        神经网络优化概述(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%282%29/" class="md-nav__link">
        神经网络优化概述(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06_%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" class="md-nav__link">
        网络正则化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="md-nav__link">
        注意力机制
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#cbow" class="md-nav__link">
    CBOW 模型
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    梯度计算
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#skip-gram" class="md-nav__link">
    Skip-gram模型
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="word2vec">词向量之word2vec(二)<a class="headerlink" href="#word2vec" title="Permanent link">&para;</a></h1>
<p>在word2vec中的CBOW和Skip-gram模型中，针对神经网络模型的存在的一些问题进行了有两种改进方法，一是基于Hierarchical Softmax的，另一种是基于Negative Sampling的。这里总结了基于Hierarchical Softmax的改进方法。</p>
<h2 id="cbow">CBOW 模型<a class="headerlink" href="#cbow" title="Permanent link">&para;</a></h2>
<p>word2vec的CBOW模型中，包括了三层：输入层、投影层和输出层。样本点为<span class="arithmatex"><span class="MathJax_Preview">(Context(w),w)</span><script type="math/tex">(Context(w),w)</script></span>为例，对这三层进行简要说明。</p>
<p>输入层：包含<span class="arithmatex"><span class="MathJax_Preview">Context(w)</span><script type="math/tex">Context(w)</script></span>中<span class="arithmatex"><span class="MathJax_Preview">2c</span><script type="math/tex">2c</script></span>个词的词向量<span class="arithmatex"><span class="MathJax_Preview">\mathbf{v}(Context(w)_1),\mathbf{v}(Context(w)_2),...,\mathbf{v}(Context(w)_{2c})\in \mathbb{R}^m</span><script type="math/tex">\mathbf{v}(Context(w)_1),\mathbf{v}(Context(w)_2),...,\mathbf{v}(Context(w)_{2c})\in \mathbb{R}^m</script></span>。这里的<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>表示词向量的长度。</p>
<p>投影层：将输入层的<span class="arithmatex"><span class="MathJax_Preview">2c</span><script type="math/tex">2c</script></span>个向量做求和累加，即<span class="arithmatex"><span class="MathJax_Preview">x_w=\sum_{i=1}^{2c}v(Context(w)_i)\in \mathbb{R}^m</span><script type="math/tex">x_w=\sum_{i=1}^{2c}v(Context(w)_i)\in \mathbb{R}^m</script></span>。</p>
<p><img alt="cbow" src="../assets/cbow.png" /></p>
<p>输出层：输出层对应一颗二叉树，是以语料中出现过的词当叶子节点，以各词在语料中出现的次数当权值构造Huffman树，叶子节点共<span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>(等于<span class="arithmatex"><span class="MathJax_Preview">|\mathcal{D}|</span><script type="math/tex">|\mathcal{D}|</script></span>个)分别对应词典<span class="arithmatex"><span class="MathJax_Preview">\mathcal{D}</span><script type="math/tex">\mathcal{D}</script></span>中的词，非叶子节点<span class="arithmatex"><span class="MathJax_Preview">N-1</span><script type="math/tex">N-1</script></span>个。</p>
<p>上面word2vec中CBOW模型和神经网络概率语言模型对比，不同点主要有：</p>
<p>①从输入层到投影层的操作，神经网络是通过拼接，在CBOW中通过累加求和；</p>
<p>②在神经网络中有隐藏层，在CBOW中无隐藏层；</p>
<p>③在输出层中神经网络是线性结构，在CBOW中是树形结构；</p>
<p>在神经网络中大部分的计算集中在隐藏层和输出层之间的矩阵向量计算，以及输出层上的softmax归一化运算。在CBOW模型中对这些计算复杂度高的地方有针对性地进行了改变：首先，去掉了隐藏层。其次，输出层改用了Huffman树，从而为利用Hierarchical softmax技术奠定了基础。</p>
<h2 id="_1">梯度计算<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p>Hierarchical Softmax是word2vec中用于提高性能的一项关键技术。首先引入若干相关的符号，考虑Huffman树中的某个叶子节点，对应词典<span class="arithmatex"><span class="MathJax_Preview">\mathcal{D}</span><script type="math/tex">\mathcal{D}</script></span>中的词<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>，记：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span></td>
<td>从根节点出发到达<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>对应叶子节点的路径</td>
</tr>
<tr>
<td><span class="arithmatex"><span class="MathJax_Preview">l^w</span><script type="math/tex">l^w</script></span></td>
<td>路径<span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span>中包含结点的个数</td>
</tr>
<tr>
<td><span class="arithmatex"><span class="MathJax_Preview">d_2^w,d_3^w,...,d_{l^w}^w\in \{0,1\}</span><script type="math/tex">d_2^w,d_3^w,...,d_{l^w}^w\in \{0,1\}</script></span></td>
<td>路径<span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span>中的<span class="arithmatex"><span class="MathJax_Preview">l^w</span><script type="math/tex">l^w</script></span>个节点，其中<span class="arithmatex"><span class="MathJax_Preview">p_1^w</span><script type="math/tex">p_1^w</script></span>表示根节点<span class="arithmatex"><span class="MathJax_Preview">p^w_{l^w}</span><script type="math/tex">p^w_{l^w}</script></span>表示词<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>对应的节点</td>
</tr>
<tr>
<td><span class="arithmatex"><span class="MathJax_Preview">\theta_1^w,\theta_2^w,...,\theta_{l^{w}-1}^w \in \mathbb{R}^m</span><script type="math/tex">\theta_1^w,\theta_2^w,...,\theta_{l^{w}-1}^w \in \mathbb{R}^m</script></span></td>
<td>路径<span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span>中非叶子节点对应的向量，<span class="arithmatex"><span class="MathJax_Preview">\theta_j^w</span><script type="math/tex">\theta_j^w</script></span>表示路径<span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span>中第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>个非叶子节点对应向量</td>
</tr>
</tbody>
</table>
<p>若考虑词<span class="arithmatex"><span class="MathJax_Preview">w=</span><script type="math/tex">w=</script></span>"足球"的情形，下图有4条红色边穿起来的5个节点就构成了路径<span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span>，其长度<span class="arithmatex"><span class="MathJax_Preview">l^w=5</span><script type="math/tex">l^w=5</script></span>。<span class="arithmatex"><span class="MathJax_Preview">p_1^w,p_2^w,p_3^w,p_4^w,p_5^w</span><script type="math/tex">p_1^w,p_2^w,p_3^w,p_4^w,p_5^w</script></span>为路径<span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span>上的5个结点，其中<span class="arithmatex"><span class="MathJax_Preview">p_1^w</span><script type="math/tex">p_1^w</script></span>对应根结点，<span class="arithmatex"><span class="MathJax_Preview">d_2^w,d_3^w,d_4^w,d_5^w</span><script type="math/tex">d_2^w,d_3^w,d_4^w,d_5^w</script></span>分别为<span class="arithmatex"><span class="MathJax_Preview">1,0,0,1</span><script type="math/tex">1,0,0,1</script></span>。所以"足球"的Huffman编码为1001，此外<span class="arithmatex"><span class="MathJax_Preview">\theta_1^w,\theta_2^w,\theta_3^w,\theta_4^w</span><script type="math/tex">\theta_1^w,\theta_2^w,\theta_3^w,\theta_4^w</script></span>分别表示路径<span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span>上4个非叶子节点对应的向量。</p>
<p><img alt="cbow_2" src="../assets/cbow_2.png" /></p>
<p>从根节点出发到达"足球"这个叶子节点，中间共经历了4次分支(每条红色的边对应一次分支)，而每一次分支都可视为进行了一次二分类。在word2vec中约定：<strong>将一个节点进行分类时，分到左边的就是负类，分到右边的就是正类。</strong> 在二分类逻辑回归中，一个点被分为正类的概率是：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sigma(x_w^T \theta)=\frac{1}{1+e^{-x_w^T \theta}} \notag
</div>
<script type="math/tex; mode=display">
\sigma(x_w^T \theta)=\frac{1}{1+e^{-x_w^T \theta}} \notag
</script>
</div>
<p>被分为负类的概率就等于<span class="arithmatex"><span class="MathJax_Preview">1-\sigma(x_w^T \theta)</span><script type="math/tex">1-\sigma(x_w^T \theta)</script></span>。式中的<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>是待定的参数向量。回到上面的问题，那么从根结点出发到达"足球"这个叶子结点所经历的4次二分类，将每次分类结果的概率写出来就是：</p>
<p>第一次：<span class="arithmatex"><span class="MathJax_Preview">p(d_2^w|x_w,\theta_1^w)=1-\sigma(x_w^T \theta_1^w)</span><script type="math/tex">p(d_2^w|x_w,\theta_1^w)=1-\sigma(x_w^T \theta_1^w)</script></span></p>
<p>第二次：<span class="arithmatex"><span class="MathJax_Preview">p(d_3^w|x_w,\theta_2^w)=\sigma(x_w^T \theta_2^w)</span><script type="math/tex">p(d_3^w|x_w,\theta_2^w)=\sigma(x_w^T \theta_2^w)</script></span></p>
<p>第三次：<span class="arithmatex"><span class="MathJax_Preview">p(d_4^w|x_w,\theta_3^w)=\sigma(x_w^T \theta_3^w)</span><script type="math/tex">p(d_4^w|x_w,\theta_3^w)=\sigma(x_w^T \theta_3^w)</script></span></p>
<p>第四次：<span class="arithmatex"><span class="MathJax_Preview">p(d_5^w|x_w,\theta_4^w)=1-\sigma(x_w^T \theta_4^w)</span><script type="math/tex">p(d_5^w|x_w,\theta_4^w)=1-\sigma(x_w^T \theta_4^w)</script></span></p>
<p>于是要求的是<span class="arithmatex"><span class="MathJax_Preview">p(足球|Context(足球))</span><script type="math/tex">p(足球|Context(足球))</script></span>可以表达为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
p(足球|Context(足球))=\prod_{j=1}^5 p(d_j^w| x_w,\theta_{j-1}^w) \notag
</div>
<script type="math/tex; mode=display">
p(足球|Context(足球))=\prod_{j=1}^5 p(d_j^w| x_w,\theta_{j-1}^w) \notag
</script>
</div>
<p>综上可以看到Hierarchical Sotfmax的基本思想就是：对于词典<span class="arithmatex"><span class="MathJax_Preview">\mathcal{D}</span><script type="math/tex">\mathcal{D}</script></span>中的任意词<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>，Huffman树中必存在一条从根节点到词<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>对应的路劲<span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span>且这条路径是唯一的，路径<span class="arithmatex"><span class="MathJax_Preview">p^w</span><script type="math/tex">p^w</script></span>上存在<span class="arithmatex"><span class="MathJax_Preview">l^w-1</span><script type="math/tex">l^w-1</script></span>个分支，每个分支可看成一次二分类，每一次分类就产生一个概率，将这些概率乘起来，就是所需的<span class="arithmatex"><span class="MathJax_Preview">p(w|Context(w))</span><script type="math/tex">p(w|Context(w))</script></span>。</p>
<p>条件概率<span class="arithmatex"><span class="MathJax_Preview">p(w|Context(w))</span><script type="math/tex">p(w|Context(w))</script></span>的一般公式可写成：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
p(w|Context(w))= \prod_{j=2}^{l^w}p(d_j^w | x_w,\theta_{j-1}^w)
</div>
<script type="math/tex; mode=display">
p(w|Context(w))= \prod_{j=2}^{l^w}p(d_j^w | x_w,\theta_{j-1}^w)
</script>
</div>
<p>其中</p>
<div class="arithmatex">
<div class="MathJax_Preview">
p(d_j^w | x_w, \theta_{j-1}^w)=\left \{
\begin{array}{}
\sigma(x_w^T \theta_{j-1}^w)  &amp;\quad d_j^w=0
\\
1-\sigma(x_w^T \theta_{j-1}^w)  &amp;\quad d_j^w=1
\end{array}
\right.
</div>
<script type="math/tex; mode=display">
p(d_j^w | x_w, \theta_{j-1}^w)=\left \{
\begin{array}{}
\sigma(x_w^T \theta_{j-1}^w)  &\quad d_j^w=0
\\
1-\sigma(x_w^T \theta_{j-1}^w)  &\quad d_j^w=1
\end{array}
\right.
</script>
</div>
<p>上式(2)可以写成整体表达式</p>
<div class="arithmatex">
<div class="MathJax_Preview">
p(d_j^w | x_w,\theta_{j-1}^w)=[ \sigma(x_w^T \theta_{j-1}^w)]^{1-d_j^w}\cdot [ 1-\sigma(x_w^T \theta_{j-1}^w ]^{d_j^w}
</div>
<script type="math/tex; mode=display">
p(d_j^w | x_w,\theta_{j-1}^w)=[ \sigma(x_w^T \theta_{j-1}^w)]^{1-d_j^w}\cdot [ 1-\sigma(x_w^T \theta_{j-1}^w ]^{d_j^w}
</script>
</div>
<p>将式(3)带入到式(1)取对数得到对数似然函数为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align}
\mathcal{L}
&amp;=\sum_{w\in \mathcal{C}} \log \prod_{j=2}^{l^w} \bigg( [ \sigma(x_w^T \theta_{j-1}^w)]^{1-d_j^w}\cdot [ 1-\sigma(x_w^T \theta_{j-1}^w ]^{d_j^w}    \bigg)
\\
&amp;=\sum_{c\in \mathcal{C}} \sum_{j=2}^{l^w}\bigg[ (1-d_j^w)\cdot\log \sigma(x_w^T \theta_{j-1}^w)+d_j^w\cdot (1-\sigma(x_w^T\theta_{j-1}^w))\bigg]
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{L}
&=\sum_{w\in \mathcal{C}} \log \prod_{j=2}^{l^w} \bigg( [ \sigma(x_w^T \theta_{j-1}^w)]^{1-d_j^w}\cdot [ 1-\sigma(x_w^T \theta_{j-1}^w ]^{d_j^w}    \bigg)
\\
&=\sum_{c\in \mathcal{C}} \sum_{j=2}^{l^w}\bigg[ (1-d_j^w)\cdot\log \sigma(x_w^T \theta_{j-1}^w)+d_j^w\cdot (1-\sigma(x_w^T\theta_{j-1}^w))\bigg]
\end{align}
</script>
</div>
<p>word2vec中采用的是随机梯度法，目标函数为<span class="arithmatex"><span class="MathJax_Preview">\mathcal{L}</span><script type="math/tex">\mathcal{L}</script></span>，参数向量有<span class="arithmatex"><span class="MathJax_Preview">x_w,\theta_{j-1}^w,w\in \mathcal{C},j=2,3,...,l^w</span><script type="math/tex">x_w,\theta_{j-1}^w,w\in \mathcal{C},j=2,3,...,l^w</script></span> 。首先计算<span class="arithmatex"><span class="MathJax_Preview">\mathcal{L}(x_w,\theta_{j-1}^w)</span><script type="math/tex">\mathcal{L}(x_w,\theta_{j-1}^w)</script></span>关于参数<span class="arithmatex"><span class="MathJax_Preview">\theta_{j-1}^w</span><script type="math/tex">\theta_{j-1}^w</script></span>的表达式</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align}
\frac{\partial \mathcal{L}(x_w,\theta_{j-1}^w)}{\partial \theta_{j-1}^w}
&amp;=\frac{\partial}{\partial \theta_{j-1}^w} \bigg [ (1-d_j^w)\cdot\log \sigma(x_w^T \theta_{j-1}^w)+d_j^w\cdot (1-\sigma(x_w^T\theta_{j-1}^w)) \bigg ]
\\
&amp;=(1-d_j^w)\frac{\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w))}{\sigma(x_w^T\theta_{j-1}^w)}x_w
- d_j^w\frac{\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w))}{1-\sigma(x_w^T\theta_{j-1}^w)}x_w
\\
&amp;=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))x_w - d_j^w\sigma(x_w^T\theta_{j-1}^w)x_w
\\
&amp;=(1-d_j^w-\sigma(x_w^T \theta_{j-1}^w))x_w
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial \mathcal{L}(x_w,\theta_{j-1}^w)}{\partial \theta_{j-1}^w}
&=\frac{\partial}{\partial \theta_{j-1}^w} \bigg [ (1-d_j^w)\cdot\log \sigma(x_w^T \theta_{j-1}^w)+d_j^w\cdot (1-\sigma(x_w^T\theta_{j-1}^w)) \bigg ]
\\
&=(1-d_j^w)\frac{\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w))}{\sigma(x_w^T\theta_{j-1}^w)}x_w
- d_j^w\frac{\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w))}{1-\sigma(x_w^T\theta_{j-1}^w)}x_w
\\
&=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))x_w - d_j^w\sigma(x_w^T\theta_{j-1}^w)x_w
\\
&=(1-d_j^w-\sigma(x_w^T \theta_{j-1}^w))x_w
\end{align}
</script>
</div>
<p>式(6)~(7)主要利用了<span class="arithmatex"><span class="MathJax_Preview">\sigma'(x)=\sigma(x)(1-\sigma(x))</span><script type="math/tex">\sigma'(x)=\sigma(x)(1-\sigma(x))</script></span> ，式(7)~(9)主要是合并同类项。于是，<span class="arithmatex"><span class="MathJax_Preview">\theta_{j-1}^w</span><script type="math/tex">\theta_{j-1}^w</script></span>的更新公式为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\theta_{j-1}^w  = \theta_{j-1}^w + \eta [1- d_j^w-\sigma(x_w^T \theta_{j-1}^w)]
</div>
<script type="math/tex; mode=display">
\theta_{j-1}^w  = \theta_{j-1}^w + \eta [1- d_j^w-\sigma(x_w^T \theta_{j-1}^w)]
</script>
</div>
<p><span class="arithmatex"><span class="MathJax_Preview">\mathcal{L}(x_w,\theta_{j-1}^w)</span><script type="math/tex">\mathcal{L}(x_w,\theta_{j-1}^w)</script></span>关于参数<span class="arithmatex"><span class="MathJax_Preview">x_w</span><script type="math/tex">x_w</script></span>的表达式</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align}
\frac{\partial \mathcal{L}(x_w,\theta_{j-1}^w)}{\partial x_w}
&amp;=\frac{\partial}{\partial x_w} \bigg [ (1-d_j^w)\cdot\log \sigma(x_w^T \theta_{j-1}^w)+d_j^w\cdot (1-\sigma(x_w^T\theta_{j-1}^w)) \bigg ]
\\
&amp;=(1-d_j^w)\frac{\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w))}{\sigma(x_w^T\theta_{j-1}^w)}\theta_{j-1}^w
- d_j^w\frac{\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w))}{1-\sigma(x_w^T\theta_{j-1}^w)}\theta_{j-1}^w
\\
&amp;=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))\theta_{j-1}^w - d_j^w\sigma(x_w^T\theta_{j-1}^w)\theta_{j-1}^w
\\
&amp;=(1-d_j^w-\sigma(x_w^T \theta_{j-1}^w))\theta_{j-1}^w
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial \mathcal{L}(x_w,\theta_{j-1}^w)}{\partial x_w}
&=\frac{\partial}{\partial x_w} \bigg [ (1-d_j^w)\cdot\log \sigma(x_w^T \theta_{j-1}^w)+d_j^w\cdot (1-\sigma(x_w^T\theta_{j-1}^w)) \bigg ]
\\
&=(1-d_j^w)\frac{\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w))}{\sigma(x_w^T\theta_{j-1}^w)}\theta_{j-1}^w
- d_j^w\frac{\sigma(x_w^T\theta_{j-1}^w)(1-\sigma(x_w^T\theta_{j-1}^w))}{1-\sigma(x_w^T\theta_{j-1}^w)}\theta_{j-1}^w
\\
&=(1-d_j^w)(1-\sigma(x_w^T\theta_{j-1}^w))\theta_{j-1}^w - d_j^w\sigma(x_w^T\theta_{j-1}^w)\theta_{j-1}^w
\\
&=(1-d_j^w-\sigma(x_w^T \theta_{j-1}^w))\theta_{j-1}^w
\end{align}
</script>
</div>
<p>这里需要注意的是：现在的最终目的是要求词典<span class="arithmatex"><span class="MathJax_Preview">\mathcal{D}</span><script type="math/tex">\mathcal{D}</script></span>中每个词的词向量，而这里的<span class="arithmatex"><span class="MathJax_Preview">x_w</span><script type="math/tex">x_w</script></span>表示的是<span class="arithmatex"><span class="MathJax_Preview">Context(w)</span><script type="math/tex">Context(w)</script></span>中各词词向量的累加，的处理方式是</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{v}(\widetilde{w})=\mathbf{v}(\widetilde{w}) + \eta \cdot \frac{\partial \mathcal{L}(x_w,\theta_{j-1}^w)}{\partial x_w}\quad \widetilde{w} \in Context(w)
</div>
<script type="math/tex; mode=display">
\mathbf{v}(\widetilde{w})=\mathbf{v}(\widetilde{w}) + \eta \cdot \frac{\partial \mathcal{L}(x_w,\theta_{j-1}^w)}{\partial x_w}\quad \widetilde{w} \in Context(w)
</script>
</div>
<p>基于Hierarchical Softmax的CBOW模型算法流程，使用随机梯度上升算法流程如下：</p>
<blockquote>
<p>输入：基本CBOW语料训练样本，词向量维度<span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span>，CBOW上下文大小<span class="arithmatex"><span class="MathJax_Preview">2c</span><script type="math/tex">2c</script></span>，步长<span class="arithmatex"><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span></p>
<p>输出：Huffman树内部节点模型参数，所有的词向量<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span></p>
<p>（1）基于预料训练样本建立Huffman树</p>
<p>（2）随机初始化所有的模型参数<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>,所有的词向量<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span></p>
<p>（3）进行梯度上升迭代过程，对于训练集中每一个样本<span class="arithmatex"><span class="MathJax_Preview">(Context(w),w)</span><script type="math/tex">(Context(w),w)</script></span>做如下处理：</p>
<p>​ ①<span class="arithmatex"><span class="MathJax_Preview">e=0</span><script type="math/tex">e=0</script></span>，计算<span class="arithmatex"><span class="MathJax_Preview">x_w=\frac{1}{2c}\sum_{i=1}^{2c}x_i</span><script type="math/tex">x_w=\frac{1}{2c}\sum_{i=1}^{2c}x_i</script></span></p>
<p>​ ②for j=2 to <span class="arithmatex"><span class="MathJax_Preview">l^w</span><script type="math/tex">l^w</script></span> 计算：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
f &amp;=\sigma(x_w^T \theta_{j-1}^w)
\\
g &amp;= \eta(1-d_j^w-f)
\\
e &amp;= e+ g\theta_{j-1}^w
\\
\theta_{j-1}^w &amp;= \theta_{j-1}^w+ gx_w
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
f &=\sigma(x_w^T \theta_{j-1}^w)
\\
g &= \eta(1-d_j^w-f)
\\
e &= e+ g\theta_{j-1}^w
\\
\theta_{j-1}^w &= \theta_{j-1}^w+ gx_w
\end{aligned}
</script>
</div>
<p>​ ③对于<span class="arithmatex"><span class="MathJax_Preview">Context(w)</span><script type="math/tex">Context(w)</script></span>中的每一个词向量<span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>（共<span class="arithmatex"><span class="MathJax_Preview">2c</span><script type="math/tex">2c</script></span>个）进行更新：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
x_i = x_i +e \notag
</div>
<script type="math/tex; mode=display">
x_i = x_i +e \notag
</script>
</div>
<p>（4）如果梯度收敛，则结束梯度迭代；否则回到步骤（3）继续迭代</p>
</blockquote>
<h2 id="skip-gram">Skip-gram模型<a class="headerlink" href="#skip-gram" title="Permanent link">&para;</a></h2>
<p>Skip-gram模型的网络结构同CBOW模型的网络结构一样，也包括了三层：输入层，投影层和输出层。</p>
<p>输入层：只含当前样本的中心词<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>的词向量<span class="arithmatex"><span class="MathJax_Preview">\mathbf{v}(w) \in \mathbb{R}^m</span><script type="math/tex">\mathbf{v}(w) \in \mathbb{R}^m</script></span>；</p>
<p>投影层：把<span class="arithmatex"><span class="MathJax_Preview">\mathbf{v}(w)</span><script type="math/tex">\mathbf{v}(w)</script></span>投影到<span class="arithmatex"><span class="MathJax_Preview">\mathbf{v}(w)</span><script type="math/tex">\mathbf{v}(w)</script></span>，在Skip-gram模型中这个投影层其实是多余。这里之所有保留投影层主要是方便和CBOW模型作对比；</p>
<p>输出层：和CBOW模型一样，输出层也是一颗Huffman树，输出的是<span class="arithmatex"><span class="MathJax_Preview">2c</span><script type="math/tex">2c</script></span>个词向量<span class="arithmatex"><span class="MathJax_Preview">Context(w)</span><script type="math/tex">Context(w)</script></span> ；</p>
<p><img alt="skip_gram" src="../assets/skip_gram.png" /></p>
<p>通过梯度上升法来更新<span class="arithmatex"><span class="MathJax_Preview">\theta_{j-1}^w</span><script type="math/tex">\theta_{j-1}^w</script></span>和<span class="arithmatex"><span class="MathJax_Preview">x_w</span><script type="math/tex">x_w</script></span>，由于<span class="arithmatex"><span class="MathJax_Preview">x_w</span><script type="math/tex">x_w</script></span>的周围有<span class="arithmatex"><span class="MathJax_Preview">2c</span><script type="math/tex">2c</script></span>个词向量，此时希望<span class="arithmatex"><span class="MathJax_Preview">p(x_i|x_w),i=1,2,...,2c</span><script type="math/tex">p(x_i|x_w),i=1,2,...,2c</script></span>最大。此时由于上下文是互相的，在期望<span class="arithmatex"><span class="MathJax_Preview">p(x_i|x_w),i=1,2,...,2c</span><script type="math/tex">p(x_i|x_w),i=1,2,...,2c</script></span>最大化的同时，反过来也期望<span class="arithmatex"><span class="MathJax_Preview">p(x_w|x_i),i=1,2,...,2c</span><script type="math/tex">p(x_w|x_i),i=1,2,...,2c</script></span>最大。所以现在的评判标准就有两个：<span class="arithmatex"><span class="MathJax_Preview">p(x_i|x_w)</span><script type="math/tex">p(x_i|x_w)</script></span>和<span class="arithmatex"><span class="MathJax_Preview">p(x_w|x_i)</span><script type="math/tex">p(x_w|x_i)</script></span> 。在word2vec中选择了后者即<span class="arithmatex"><span class="MathJax_Preview">p(x_w|x_i)</span><script type="math/tex">p(x_w|x_i)</script></span>，这样做的好处就是在一次迭代时，不是更新<span class="arithmatex"><span class="MathJax_Preview">x_w</span><script type="math/tex">x_w</script></span>一个词，而是<span class="arithmatex"><span class="MathJax_Preview">x_i,i=1,2,...,2c</span><script type="math/tex">x_i,i=1,2,...,2c</script></span>个词，保证了整个迭代更加的均衡。由于这个原因Skip-gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对<span class="arithmatex"><span class="MathJax_Preview">2c</span><script type="math/tex">2c</script></span>个输出进行迭代更新。</p>
<p>基于Hierarchical Softmax的Skip-gram模型算法流程，使用随机梯度上升算法流程如下：</p>
<blockquote>
<p>输入：基本CBOW语料训练样本，词向量维度<span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span>，CBOW上下文大小<span class="arithmatex"><span class="MathJax_Preview">2c</span><script type="math/tex">2c</script></span>，步长<span class="arithmatex"><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span></p>
<p>输出：Huffman树内部节点模型参数<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>，所有的词向量<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span></p>
<p>（1）基于预料训练样本建立Huffman树</p>
<p>（2）随机初始化所有的模型参数<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>,所有的词向量<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span></p>
<p>（3）进行梯度上升迭代过程，对于训练集中每一个样本<span class="arithmatex"><span class="MathJax_Preview">(w,Context(w))</span><script type="math/tex">(w,Context(w))</script></span>做如下处理：</p>
<p>​ ① for i = 1 to 2c:</p>
<p>​     (a) e=0</p>
<p>​     (b) for j=2 to <span class="arithmatex"><span class="MathJax_Preview">l^w</span><script type="math/tex">l^w</script></span> 计算：
$$
\begin{aligned}
f &amp;=\sigma(x_w^T \theta_{j-1}^w)
\
g &amp;= \eta(1-d_j^w-f)
\
e &amp;= e+ g\theta_{j-1}^w
\
\theta_{j-1}^w &amp;= \theta_{j-1}^w+ gx_w
\end{aligned}
$$
​ &copy;<span class="arithmatex"><span class="MathJax_Preview">x_i = x_i +e</span><script type="math/tex">x_i = x_i +e</script></span>   </p>
<p>（4）如果梯度收敛，则结束梯度迭代；否则回到步骤（3）继续迭代</p>
</blockquote>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../10_%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bword2vec%28%E4%B8%80%29/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 词向量之word2vec(一)" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              词向量之word2vec(一)
            </div>
          </div>
        </a>
      
      
        
        <a href="../12_%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bword2vec%28%E4%B8%89%29/" class="md-footer__link md-footer__link--next" aria-label="下一页: 词向量之word2vec(三)" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              词向量之word2vec(三)
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs"], "search": "../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>