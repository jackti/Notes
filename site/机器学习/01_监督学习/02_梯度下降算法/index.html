
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="采用Mkdocs-material生成的文档管理网站支持的markdown语法，包括传统语法和扩展语法">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>梯度下降算法 - 我的知识笔记</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.9647289d.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="pink">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="我的知识笔记" class="md-header__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            我的知识笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              梯度下降算法
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-tabs__link md-tabs__link--active">
        机器学习
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-tabs__link">
        深度学习
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="我的知识笔记" class="md-nav__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    我的知识笔记
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          01 监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="01 监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          01 监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-nav__link">
        数学符号
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../00_%E6%80%BB%E7%BB%93/" class="md-nav__link">
        线性模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" class="md-nav__link">
        最小二乘法
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          梯度下降算法
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        梯度下降算法
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    基本概念
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    梯度上升和梯度下降
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    梯度下降算法详解
  </a>
  
    <nav class="md-nav" aria-label="梯度下降算法详解">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    直观解释
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    代数解释
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    算法调优
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    梯度下降算法对比
  </a>
  
    <nav class="md-nav" aria-label="梯度下降算法对比">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bgd" class="md-nav__link">
    批量梯度下降法BGD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd" class="md-nav__link">
    随机梯度下降法SGD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mbgd" class="md-nav__link">
    小批量梯度下降法MBGD
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8E%9F%E7%90%86/" class="md-nav__link">
        交叉验证原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="md-nav__link">
        模型评估
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        线性回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        逻辑回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        决策树算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        决策树算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        决策树算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        近邻算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        朴素贝叶斯算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        最大熵算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        最大熵算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        最大熵算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        感知机算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        支持向量机算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        支持向量机算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        支持向量机算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../19_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E5%9B%9B%29/" class="md-nav__link">
        支持向量机算法原理(四)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../20_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%94%29/" class="md-nav__link">
        支持向量机算法原理(五)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../21_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        集成学习算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../22_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BAdaboost/" class="md-nav__link">
        集成学习算法之Adaboost
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../23_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BRF/" class="md-nav__link">
        集成学习算法之RF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../24_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/" class="md-nav__link">
        集成学习算法之GBDT
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          02 无监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="02 无监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          02 无监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/25_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BK-Means/" class="md-nav__link">
        聚类算法之K-Means​
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/26_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BBIRCH/" class="md-nav__link">
        聚类算法之BIRCH
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/27_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BDBSCAN/" class="md-nav__link">
        聚类算法之DBSCAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/28_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BMean%20Shift/" class="md-nav__link">
        聚类算法之Mean Shift
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/29_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E8%B0%B1%E8%81%9A%E7%B1%BB/" class="md-nav__link">
        聚类算法之谱聚类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/30_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BPCA/" class="md-nav__link">
        降维算法之PCA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/31_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLDA/" class="md-nav__link">
        降维算法之LDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/32_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BIsomap/" class="md-nav__link">
        降维算法之Isomap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/33_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLLE/" class="md-nav__link">
        降维算法之LLE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/34_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BApriori/" class="md-nav__link">
        关联算法之Apriori
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/35_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BFP-Tree/" class="md-nav__link">
        关联算法之FP-Tree
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/36_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        推荐算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/37_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/" class="md-nav__link">
        推荐算法之矩阵分解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/38_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BSimRank/" class="md-nav__link">
        推荐算法之SimRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/39_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BPersonalRank/" class="md-nav__link">
        推荐算法之PersonalRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/40_EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        EM算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/41_%E5%88%86%E8%A7%A3%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        分解机算法原理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="深度学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        神经网络基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01_DNN%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        DNN前馈神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        CNN卷积神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        RNN循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04_%E5%9F%BA%E4%BA%8E%E9%97%A8%E6%8E%A7%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        基于门控的循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%281%29/" class="md-nav__link">
        神经网络优化概述(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%282%29/" class="md-nav__link">
        神经网络优化概述(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06_%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" class="md-nav__link">
        网络正则化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="md-nav__link">
        注意力机制
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    基本概念
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    梯度上升和梯度下降
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    梯度下降算法详解
  </a>
  
    <nav class="md-nav" aria-label="梯度下降算法详解">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    直观解释
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    代数解释
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    算法调优
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    梯度下降算法对比
  </a>
  
    <nav class="md-nav" aria-label="梯度下降算法对比">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bgd" class="md-nav__link">
    批量梯度下降法BGD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgd" class="md-nav__link">
    随机梯度下降法SGD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mbgd" class="md-nav__link">
    小批量梯度下降法MBGD
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="_1">梯度下降算法<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<p>在求解机器学习算法的模型参数中即无约束优化问题，梯度下降（Gradient Descent）是最常用的方法之一。这里对梯度下降算法做一个总结。</p>
<h2 id="_2">基本概念<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p><mark>方向导数</mark> ：是一个数，反映的是<span class="arithmatex"><span class="MathJax_Preview">f(x,y)</span><script type="math/tex">f(x,y)</script></span>在一点<span class="arithmatex"><span class="MathJax_Preview">P_0</span><script type="math/tex">P_0</script></span>沿着某一方向<span class="arithmatex"><span class="MathJax_Preview">l</span><script type="math/tex">l</script></span>的变化率，记做<span class="arithmatex"><span class="MathJax_Preview">\frac{\partial f}{\partial l}</span><script type="math/tex">\frac{\partial f}{\partial l}</script></span>。</p>
<blockquote>
<p>例：求函数<span class="arithmatex"><span class="MathJax_Preview">z=xe^{ey}</span><script type="math/tex">z=xe^{ey}</script></span>在点<span class="arithmatex"><span class="MathJax_Preview">P(1,0)</span><script type="math/tex">P(1,0)</script></span>处沿从点<span class="arithmatex"><span class="MathJax_Preview">P(1,0)</span><script type="math/tex">P(1,0)</script></span>到点<span class="arithmatex"><span class="MathJax_Preview">Q(2,-1)</span><script type="math/tex">Q(2,-1)</script></span>的方向的方向导数。</p>
<p>解：这里方向<span class="arithmatex"><span class="MathJax_Preview">\vec{l}</span><script type="math/tex">\vec{l}</script></span>即为：<span class="arithmatex"><span class="MathJax_Preview">\vec{PQ} = (1,-1)</span><script type="math/tex">\vec{PQ} = (1,-1)</script></span> </p>
<p><span class="arithmatex"><span class="MathJax_Preview">cos\alpha=\frac{1}{\sqrt{1^2+1^2}}=\frac{\sqrt{2}}{2}</span><script type="math/tex">cos\alpha=\frac{1}{\sqrt{1^2+1^2}}=\frac{\sqrt{2}}{2}</script></span>       <span class="arithmatex"><span class="MathJax_Preview">cos\beta=\frac{-1}{\sqrt{1^2+1^2}}=-\frac{\sqrt{2}}{2}</span><script type="math/tex">cos\beta=\frac{-1}{\sqrt{1^2+1^2}}=-\frac{\sqrt{2}}{2}</script></span></p>
<p><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial z}{\partial x}|_{(1,0)}=e^{2y}  |_{(1,0)}=1</span><script type="math/tex">\frac{\partial z}{\partial x}|_{(1,0)}=e^{2y}  |_{(1,0)}=1</script></span>;    <span class="arithmatex"><span class="MathJax_Preview">\frac{\partial z}{\partial y}|_{(1,0)}=2xe^{2y}  |_{(1,0)}=2</span><script type="math/tex">\frac{\partial z}{\partial y}|_{(1,0)}=2xe^{2y}  |_{(1,0)}=2</script></span></p>
<p>所以方向导数<span class="arithmatex"><span class="MathJax_Preview">\frac{\partial z}{\partial \vec{l}}=cos\alpha+2cos\beta=-\frac{\sqrt{2}}{2}</span><script type="math/tex">\frac{\partial z}{\partial \vec{l}}=cos\alpha+2cos\beta=-\frac{\sqrt{2}}{2}</script></span></p>
</blockquote>
<p><mark>偏导数</mark> 是指多元函数沿坐标轴方向的方向导数，因此<span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>元函数就有<span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>个偏导数。</p>
<p><mark>梯度</mark> 是一个向量，既有大小又有方向。梯度的每个元素是函数对一元变量的偏导数。记做<span class="arithmatex"><span class="MathJax_Preview">gradf</span><script type="math/tex">gradf</script></span>或者<span class="arithmatex"><span class="MathJax_Preview">\nabla f</span><script type="math/tex">\nabla f</script></span></p>
<p>梯度的几何意义：设<span class="arithmatex"><span class="MathJax_Preview">\vec{v}={(v_1,v_2)}(|\vec{v}|=1)</span><script type="math/tex">\vec{v}={(v_1,v_2)}(|\vec{v}|=1)</script></span>是任一给定的方向，则对<span class="arithmatex"><span class="MathJax_Preview">\nabla f</span><script type="math/tex">\nabla f</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\vec{v}</span><script type="math/tex">\vec{v}</script></span>的夹角<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>有：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align}
\frac{\partial f}{\partial \vec{v}}
&amp;=f'_x(x_0,y_0)v_1+f'_y(x_0,y_0)v_2\\
&amp;=\{f'_x(x_0,y_0),f'_y(x_0,y_0) \}\cdot \{ v_1,v_2 \}\\
&amp;=\nabla f |_{p_0}\cdot \vec{v}\\
&amp;=|\nabla f|_{p_0}cos\theta
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial f}{\partial \vec{v}}
&=f'_x(x_0,y_0)v_1+f'_y(x_0,y_0)v_2\\
&=\{f'_x(x_0,y_0),f'_y(x_0,y_0) \}\cdot \{ v_1,v_2 \}\\
&=\nabla f |_{p_0}\cdot \vec{v}\\
&=|\nabla f|_{p_0}cos\theta
\end{align}
</script>
</div>
<p>函数<span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span>在<span class="arithmatex"><span class="MathJax_Preview">p_0</span><script type="math/tex">p_0</script></span>处的梯度方式是函数变化率最大的方向。因此，梯度的方向就是函数<span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> 在这点增长最快的方向，梯度的模为方向导数的最大值。</p>
<h2 id="_3">梯度上升和梯度下降<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<p>在机器学习算法中，当最小化损失函数时，可以通过梯度下降来一步步的迭代求解，最终得到最小化损失函数和模型的参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来求解了。</p>
<p>梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数<span class="arithmatex"><span class="MathJax_Preview">f(\theta)</span><script type="math/tex">f(\theta)</script></span>的最小值，这时我们需要用梯度下降法来迭代求解。但是实际上，我们可以反过来求解损失函数<span class="arithmatex"><span class="MathJax_Preview">-f(\theta)</span><script type="math/tex">-f(\theta)</script></span>的最大值，这时梯度上升法就可以派上用场了。</p>
<h2 id="_4">梯度下降算法详解<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<h3 id="_5">直观解释<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>假设现在我们在一座大山上的某处位置，由于我们不知道怎么下山，于是只能走一步算一步，也就是每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向即当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在的位置沿着最陡峭最快的位置走一步。这样一步步的走下去，一直走到我们觉得已经到了山脚的位置。当然这样走下去，也有可能我们不能走到山脚，而是到了某一个局部的山峰低处。</p>
<p>从上面的解释可以看出，梯度下降算法不一定能找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是一个凸函数，梯度下降算法得到的一定是全局最优解。</p>
<h3 id="_6">代数解释<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<p>对于线性回归，假设函数为<span class="arithmatex"><span class="MathJax_Preview">h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+\theta_2 x_2+...+\theta_nx_n</span><script type="math/tex">h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+\theta_2 x_2+...+\theta_nx_n</script></span>的矩阵表达形式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
h_\theta(X)=X\theta
</div>
<script type="math/tex; mode=display">
h_\theta(X)=X\theta
</script>
</div>
<p>其中函数<span class="arithmatex"><span class="MathJax_Preview">h_\theta(X)\in \mathbb{R}^m</span><script type="math/tex">h_\theta(X)\in \mathbb{R}^m</script></span>的向量，<span class="arithmatex"><span class="MathJax_Preview">\theta\in \mathbb{R}^n</span><script type="math/tex">\theta\in \mathbb{R}^n</script></span>的向量，<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>向量中每个元素表示模型的参数。<span class="arithmatex"><span class="MathJax_Preview">X\in\mathbb{R}^{m×n}</span><script type="math/tex">X\in\mathbb{R}^{m×n}</script></span>的矩阵，<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>代表样本的个数，<span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>代表样本的特征数。</p>
<p>损失函数的表达式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
J(\theta)=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)
</div>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">Y\in \mathbb{R}^m</span><script type="math/tex">Y\in \mathbb{R}^m</script></span>是样本的输出向量。</p>
<p>算法过程：</p>
<ol>
<li>
<p>算法输入：<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>向量初始化为默认值或者调优后的值，<span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> 算法的终止距离，<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 步长（或学习率）</p>
</li>
<li>
<p>确定当前位置的损失函数梯度，对于<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>向量，梯度表达式为：</p>
</li>
</ol>
<p>$$
   \frac{\partial}{\partial \theta}J(\theta)=X^T(X\theta-Y)
   $$</p>
<ol>
<li>
<p>用步长乘以损失函数的梯度，得到当前位置下降的距离即<span class="arithmatex"><span class="MathJax_Preview">\alpha \frac{\partial}{\partial \theta}J(\theta)</span><script type="math/tex">\alpha \frac{\partial}{\partial \theta}J(\theta)</script></span></p>
</li>
<li>
<p>确定<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>向量里面每个值，梯度下降的距离都小于<span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> ，如果下于<span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>则算法终止，当前<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>向量即为最终结果。否则进入步骤5</p>
</li>
<li>
<p>更新<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>向量，更新表达式如下：</p>
</li>
</ol>
<p>$$
   \theta = \theta - \alpha \frac{\partial}{\partial \theta}J(\theta)
   $$</p>
<h3 id="_7">算法调优<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h3>
<p>在使用梯度下降算法时候，有以下几点需要注意的：</p>
<ul>
<li>
<p>步长的选择：步长太大会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p>
</li>
<li>
<p>初始值选择：初始值不同获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，选择损失函数最小化的初值。</p>
</li>
<li>
<p>归一化：由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，需要对特征数据进行归一化处理。常用的有<span class="arithmatex"><span class="MathJax_Preview">Min-Max</span><script type="math/tex">Min-Max</script></span>标准化和<span class="arithmatex"><span class="MathJax_Preview">Z-score</span><script type="math/tex">Z-score</script></span>标准化。</p>
</li>
</ul>
<h2 id="_8">梯度下降算法对比<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h2>
<p>常用的梯度下降算法具体包含了三种不同的形式，它们各自具有不同的优缺点。</p>
<h3 id="bgd">批量梯度下降法BGD<a class="headerlink" href="#bgd" title="Permanent link">&para;</a></h3>
<p>BGD是梯度下降算的最原始形式，其特点是每次更新参数<span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>时，都需要使用整个训练集数据。</p>
<p>我们上面所推导的就是批量梯度下降算法。</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\theta_i=\theta_i-\alpha\sum_{j=0}^{m}(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y^{(j)})x_i^{(j)}
</div>
<script type="math/tex; mode=display">
\theta_i=\theta_i-\alpha\sum_{j=0}^{m}(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y^{(j)})x_i^{(j)}
</script>
</div>
<p>由于有<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>个样本，这里求梯度的时候就使用了所有<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>个样本的梯度数据。</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">bgd</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="n">x_transpose</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span> <span class="n">hypothesis</span><span class="o">-</span><span class="n">y</span>

        <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_transpose</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
        <span class="n">theta</span> <span class="o">=</span><span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="n">gradient</span>

    <span class="k">return</span> <span class="n">theta</span>
</code></pre></div>
<h3 id="sgd">随机梯度下降法SGD<a class="headerlink" href="#sgd" title="Permanent link">&para;</a></h3>
<p>随机梯度下降法和批量梯度下降法的原理类似，区别在于求梯度时没有使用所有<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>个样本数据，而是仅仅选取了一个样本<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>来求梯度。</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\theta_i= \theta_i-\alpha(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y^{(j)})x_i^{(j)}
</div>
<script type="math/tex; mode=display">
\theta_i= \theta_i-\alpha(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y^{(j)})x_i^{(j)}
</script>
</div>
<p>随机梯度下降法和批量梯度下降法是两个极端，一个采用了所有数据来梯度下降，一个只用一个样本来梯度下降，各自的优缺点都很明显。于是出现了小批量梯度下降法来综合两种方法的优缺点。</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="n">x_transpose</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span> <span class="n">hypothesis</span><span class="o">-</span><span class="n">y</span>

        <span class="n">idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">m</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">loss</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">theta</span> <span class="o">=</span><span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="n">gradient</span>

    <span class="k">return</span> <span class="n">theta</span>
</code></pre></div>
<h3 id="mbgd">小批量梯度下降法MBGD<a class="headerlink" href="#mbgd" title="Permanent link">&para;</a></h3>
<p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折中，对于<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>个样本，采用<span class="arithmatex"><span class="MathJax_Preview">x（1&lt;x&lt;m）</span><script type="math/tex">x（1<x<m）</script></span>个样本来迭代，一般可以取<span class="arithmatex"><span class="MathJax_Preview">x=10</span><script type="math/tex">x=10</script></span>，当然也可以根据样本的数据，调整这个<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的值。</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\theta_i=\theta_i-\alpha\sum_{j=t}^{t+x-1}(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})x_i^{(j)}
</div>
<script type="math/tex; mode=display">
\theta_i=\theta_i-\alpha\sum_{j=t}^{t+x-1}(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})x_i^{(j)}
</script>
</div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mbgd</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">iterations</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">theta</span> <span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="n">x_transpose</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">itr</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span> <span class="n">hypothesis</span><span class="o">-</span><span class="n">y</span>

        <span class="n">idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">m</span><span class="p">),</span><span class="n">k</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">loss</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span><span class="o">/</span><span class="n">k</span>
        <span class="n">theta</span> <span class="o">=</span><span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">*</span><span class="n">gradient</span>      

    <span class="k">return</span> <span class="n">theta</span>
</code></pre></div>
<p>三种梯度下降算法的比较：</p>
<p><img alt="梯度下降比较" src="../assets/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%AF%94%E8%BE%83.jpg" /></p>
<p>红色曲线是BGD算法，是一条光滑的曲线，因为BGD每一次迭代都是求的全局最优。</p>
<p>绿色曲线是SGD算法，有比较明显的抖动，会出现局部最小解。</p>
<p>蓝色曲线是MBGD算法，相对比较平滑，但是还有会有微小的抖动。</p>
<p>完整的代码如下：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span>  <span class="nn">pylab</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="k">def</span> <span class="nf">bgd</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">J_list</span><span class="o">=</span><span class="p">[]</span>

    <span class="n">x_transpose</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span> <span class="n">hypothesis</span><span class="o">-</span><span class="n">y</span>

        <span class="n">J</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>
        <span class="n">J_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J</span><span class="p">)</span>

        <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_transpose</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
        <span class="n">theta</span> <span class="o">-=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">gradient</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">),</span> <span class="n">J_list</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;BGD&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">theta</span>

<span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">J_list</span><span class="o">=</span><span class="p">[]</span>

    <span class="n">x_transpose</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span> <span class="n">hypothesis</span><span class="o">-</span><span class="n">y</span>

        <span class="n">J</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>
        <span class="n">J_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J</span><span class="p">)</span>

        <span class="n">idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">m</span><span class="p">),</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">loss</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">theta</span> <span class="o">-=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">gradient</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">),</span> <span class="n">J_list</span><span class="p">,</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">theta</span>

<span class="k">def</span> <span class="nf">mbgd</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">iterations</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">theta</span> <span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">J_list</span><span class="o">=</span><span class="p">[]</span>

    <span class="n">x_transpose</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">itr</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span> <span class="n">hypothesis</span><span class="o">-</span><span class="n">y</span>

        <span class="n">J</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>
        <span class="n">J_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J</span><span class="p">)</span>

        <span class="n">idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">m</span><span class="p">),</span><span class="n">k</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">loss</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span><span class="o">/</span><span class="n">k</span>
        <span class="n">theta</span> <span class="o">-=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">gradient</span>      

    <span class="n">pylab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">),</span> <span class="n">J_list</span><span class="p">,</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;MBGD&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">theta</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">35</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">m</span><span class="p">),</span> <span class="n">x</span><span class="p">]</span>  <span class="c1"># insert column, bias</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># learning rate</span>

<span class="c1">#     pylab.plot(x[:, 1], y, &#39;o&#39;)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">#***BGD***#</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">theta_bgd</span> <span class="o">=</span> <span class="n">bgd</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">y_bgd_predict</span> <span class="o">=</span> <span class="n">theta_bgd</span> <span class="o">*</span> <span class="n">x</span>
<span class="c1">#     pylab.plot(x, y_bgd_predict, &#39;r--&#39;)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">#***SGD***#</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">theta_sgd</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">y_sgd_predict</span> <span class="o">=</span> <span class="n">theta_sgd</span> <span class="o">*</span> <span class="n">x</span>
<span class="c1">#     pylab.plot(x, y_sgd_predict, &#39;g--&#39;)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">#***MBGD***#</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">theta_mbgd</span> <span class="o">=</span> <span class="n">mbgd</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">y_mbgd_predict</span> <span class="o">=</span> <span class="n">theta_mbgd</span> <span class="o">*</span> <span class="n">x</span>
<span class="c1">#     pylab.plot(x, y_mbgd_predict, &#39;b--&#39;)</span>

    <span class="n">pylab</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done!&quot;</span><span class="p">)</span>
</code></pre></div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../01_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 最小二乘法" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              最小二乘法
            </div>
          </div>
        </a>
      
      
        
        <a href="../03_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8E%9F%E7%90%86/" class="md-footer__link md-footer__link--next" aria-label="下一页: 交叉验证原理" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              交叉验证原理
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs"], "search": "../../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
        <script src="../../../javascript/config.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>