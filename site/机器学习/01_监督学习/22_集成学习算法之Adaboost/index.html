
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="采用Mkdocs-material生成的文档管理网站支持的markdown语法，包括传统语法和扩展语法">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>集成学习算法之Adaboost - 我的知识笔记</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.9647289d.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="pink">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#adaboost" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="我的知识笔记" class="md-header__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            我的知识笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              集成学习算法之Adaboost
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-tabs__link md-tabs__link--active">
        机器学习
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-tabs__link">
        深度学习
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="我的知识笔记" class="md-nav__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    我的知识笔记
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          01 监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="01 监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          01 监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-nav__link">
        数学符号
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../00_%E6%80%BB%E7%BB%93/" class="md-nav__link">
        线性模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" class="md-nav__link">
        最小二乘法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" class="md-nav__link">
        梯度下降算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8E%9F%E7%90%86/" class="md-nav__link">
        交叉验证原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="md-nav__link">
        模型评估
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        线性回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        逻辑回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        决策树算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        决策树算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        决策树算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        近邻算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        朴素贝叶斯算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        最大熵算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        最大熵算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        最大熵算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        感知机算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        支持向量机算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        支持向量机算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        支持向量机算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../19_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E5%9B%9B%29/" class="md-nav__link">
        支持向量机算法原理(四)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../20_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%94%29/" class="md-nav__link">
        支持向量机算法原理(五)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../21_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        集成学习算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          集成学习算法之Adaboost
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        集成学习算法之Adaboost
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#adaboost_1" class="md-nav__link">
    Adaboost算法原理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adaboost_2" class="md-nav__link">
    Adaboost算法的解释
  </a>
  
    <nav class="md-nav" aria-label="Adaboost算法的解释">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    前向分步算法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaboost_3" class="md-nav__link">
    前向分步算法与AdaBoost
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adaboost_4" class="md-nav__link">
    Adaboost算法误差分析
  </a>
  
    <nav class="md-nav" aria-label="Adaboost算法误差分析">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adaboost_5" class="md-nav__link">
    Adaboost算法扩展
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../23_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/" class="md-nav__link">
        集成学习算法之GBDT
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../24_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BRF/" class="md-nav__link">
        集成学习算法之RF
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          02 无监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="02 无监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          02 无监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/25_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BK-Means/" class="md-nav__link">
        聚类算法之K-Means​
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/26_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BBIRCH/" class="md-nav__link">
        聚类算法之BIRCH
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/27_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BDBSCAN/" class="md-nav__link">
        聚类算法之DBSCAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/28_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BMean%20Shift/" class="md-nav__link">
        聚类算法之Mean Shift
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/29_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E8%B0%B1%E8%81%9A%E7%B1%BB/" class="md-nav__link">
        聚类算法之谱聚类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/30_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BPCA/" class="md-nav__link">
        降维算法之PCA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/31_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLDA/" class="md-nav__link">
        降维算法之LDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/32_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BIsomap/" class="md-nav__link">
        降维算法之Isomap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/33_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLLE/" class="md-nav__link">
        降维算法之LLE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/34_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BApriori/" class="md-nav__link">
        关联算法之Apriori
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/35_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BFP-Tree/" class="md-nav__link">
        关联算法之FP-Tree
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/36_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        推荐算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/37_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/" class="md-nav__link">
        推荐算法之矩阵分解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/38_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BSimRank/" class="md-nav__link">
        推荐算法之SimRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/39_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BPersonalRank/" class="md-nav__link">
        推荐算法之PersonalRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/40_EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        EM算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/41_%E5%88%86%E8%A7%A3%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        分解机算法原理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="深度学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        神经网络基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01_DNN%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        DNN前馈神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        CNN卷积神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        RNN循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04_%E5%9F%BA%E4%BA%8E%E9%97%A8%E6%8E%A7%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        基于门控的循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%281%29/" class="md-nav__link">
        神经网络优化概述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%282%29/" class="md-nav__link">
        数据预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06_%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" class="md-nav__link">
        网络正则化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="md-nav__link">
        07 注意力机制
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#adaboost_1" class="md-nav__link">
    Adaboost算法原理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adaboost_2" class="md-nav__link">
    Adaboost算法的解释
  </a>
  
    <nav class="md-nav" aria-label="Adaboost算法的解释">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    前向分步算法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaboost_3" class="md-nav__link">
    前向分步算法与AdaBoost
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adaboost_4" class="md-nav__link">
    Adaboost算法误差分析
  </a>
  
    <nav class="md-nav" aria-label="Adaboost算法误差分析">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adaboost_5" class="md-nav__link">
    Adaboost算法扩展
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="adaboost">集成学习算法之Adaboost<a class="headerlink" href="#adaboost" title="Permanent link">&para;</a></h1>
<p>boosting提升方法的基本思路：对于一个复杂任务，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断要好。</p>
<p>对于分类问题而言，提升方法就是从弱学习器算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权重分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</p>
<h2 id="adaboost_1">Adaboost算法原理<a class="headerlink" href="#adaboost_1" title="Permanent link">&para;</a></h2>
<blockquote>
<p>输入：训练数据集<span class="arithmatex"><span class="MathJax_Preview">T=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(N)},y^{(N)})\}</span><script type="math/tex">T=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(N)},y^{(N)})\}</script></span> 和弱学习器算法，其中<span class="arithmatex"><span class="MathJax_Preview">x^{(i)}\in\mathbb{R}^n,y^{(i)}\in\{+1,-1\}</span><script type="math/tex">x^{(i)}\in\mathbb{R}^n,y^{(i)}\in\{+1,-1\}</script></span></p>
<p>输出：最终的分类器<span class="arithmatex"><span class="MathJax_Preview">G(x)</span><script type="math/tex">G(x)</script></span></p>
<p>(1)初始化训练数据的分布
$$
D_1=(w_{11},...,w_{1i},...,w_{1N}),w_{1i}=\frac{1}{N}\quad i=1,2,...,N \notag
$$
(2)对于<span class="arithmatex"><span class="MathJax_Preview">m=1,2,...,M</span><script type="math/tex">m=1,2,...,M</script></span></p>
<p>​ ①使用具有权值分布<span class="arithmatex"><span class="MathJax_Preview">D_m</span><script type="math/tex">D_m</script></span>的训练数据集学习，得到基本分类器
$$
G_m(x):\mathcal{X}\Rightarrow{-1,+1}\notag
$$
​ ②计算<span class="arithmatex"><span class="MathJax_Preview">G_m(x)</span><script type="math/tex">G_m(x)</script></span>在训练数据集上的分类误差率
$$
e_m = P(G_m(x^{(i)})\ne y<sup>{(i)})=\sum_{i=1}</sup>Nw_{mi}I(G_m(x^{(i)})\ne y<sup>{(i)})=\sum_{G_m(x</sup>{(i)})\ne y^{(i)}}w_{mi}\notag
$$
​ 这里<span class="arithmatex"><span class="MathJax_Preview">w_{mi}</span><script type="math/tex">w_{mi}</script></span>表示第<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>轮中第<span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>个实例的权值，且<span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^N w_{mi}=1</span><script type="math/tex">\sum_{i=1}^N w_{mi}=1</script></span>。这表明<span class="arithmatex"><span class="MathJax_Preview">G_m(x)</span><script type="math/tex">G_m(x)</script></span>在加权的训练数据集上的分类误差率是<span class="arithmatex"><span class="MathJax_Preview">G_m(x)</span><script type="math/tex">G_m(x)</script></span>误分类样本的权值之和。</p>
<p>​ ③计算<span class="arithmatex"><span class="MathJax_Preview">G_m(x)</span><script type="math/tex">G_m(x)</script></span>的系数
$$
\alpha_m=\frac{1}{2}\ln\frac{1-e_m}{e_m}\notag
$$
<span class="arithmatex"><span class="MathJax_Preview">\alpha_m</span><script type="math/tex">\alpha_m</script></span>表示<span class="arithmatex"><span class="MathJax_Preview">G_m(x)</span><script type="math/tex">G_m(x)</script></span>在最终分类器中的重要性。当<span class="arithmatex"><span class="MathJax_Preview">e_m\le\frac{1}{2}</span><script type="math/tex">e_m\le\frac{1}{2}</script></span>时，<span class="arithmatex"><span class="MathJax_Preview">\alpha_m\ge0</span><script type="math/tex">\alpha_m\ge0</script></span>并且<span class="arithmatex"><span class="MathJax_Preview">\alpha_m</span><script type="math/tex">\alpha_m</script></span>随着<span class="arithmatex"><span class="MathJax_Preview">e_m</span><script type="math/tex">e_m</script></span>的减小而增大，所以误分类率越小的基本分类器在最终的分类器中的作用越大。</p>
<p>​ ④跟新训练数据集的权值分布
$$
\begin{array}{}
D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,N}) \notag \
w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp(-\alpha_my<sup>{(i)}G_m(x</sup>{(i)})) \qquad i=1,2,...,N
\end{array}
$$
这里<span class="arithmatex"><span class="MathJax_Preview">Z_m</span><script type="math/tex">Z_m</script></span>是规范化因子
$$
Z_m=\sum_{i=1}^N w_{mi}\exp(-\alpha_my<sup>{(i)}G_m(x</sup>{(i)}))\notag
$$
它使<span class="arithmatex"><span class="MathJax_Preview">D_{m+1}</span><script type="math/tex">D_{m+1}</script></span>称为一个概率分布。可知被基本分类器<span class="arithmatex"><span class="MathJax_Preview">G_m(x)</span><script type="math/tex">G_m(x)</script></span>误分类的样本权重得以扩大，而被正确分类样本的权重得以缩小。</p>
<p>(3)构建基本分类器的线性组合
$$
f(x)=\sum_{i=1}^M\alpha_mG_m(x) \notag
$$
线性组合<span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span>实现<span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span>个基本分类器的加权表决。系数<span class="arithmatex"><span class="MathJax_Preview">\alpha_m</span><script type="math/tex">\alpha_m</script></span>表示基本分类器<span class="arithmatex"><span class="MathJax_Preview">G_m(x)</span><script type="math/tex">G_m(x)</script></span>的重要性(但是这里所有<span class="arithmatex"><span class="MathJax_Preview">\alpha_m</span><script type="math/tex">\alpha_m</script></span>之和并不为1)，<span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span>的符号决定实例<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>的类，<span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span>的绝对值表示分类的确信度。</p>
</blockquote>
<h2 id="adaboost_2">Adaboost算法的解释<a class="headerlink" href="#adaboost_2" title="Permanent link">&para;</a></h2>
<p>在Adaboost算法中直接给出了弱学习器系数公式和样本权重更新迭代公式，并没有给出推导。至于推导过程则可以从Adaboost算法的另一种解释得到：<strong>Adaboost算法是模型为加法模型，损失函数为指数函数，学习算法为前向分步算法的分类问题。</strong></p>
<p>模型为加法模型很好理解：最终的强分类器是若干个弱分类器加权平均而得到的。</p>
<h3 id="_1">前向分步算法<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<p>考虑加法模型
$$
f(x)=\sum_{i=1}^M\beta_mb(x;\gamma_m)
$$
其中<span class="arithmatex"><span class="MathJax_Preview">b(x;\gamma_m)</span><script type="math/tex">b(x;\gamma_m)</script></span>为基函数，<span class="arithmatex"><span class="MathJax_Preview">\gamma_m</span><script type="math/tex">\gamma_m</script></span>是基函数的参数，<span class="arithmatex"><span class="MathJax_Preview">\beta_m</span><script type="math/tex">\beta_m</script></span>为基函数的系数。式(1)是一个明显的加法模型。</p>
<p>在给定训练数据以及损失函数<span class="arithmatex"><span class="MathJax_Preview">L(y,f(x))</span><script type="math/tex">L(y,f(x))</script></span>的条件下，学习加法模型<span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span>可以看成是经验风险极小化即损失函数极小化问题：
$$
\begin{align}
&amp;\min_{\beta_m,\gamma_m}\sum_{i=1}<sup>NL(y</sup>{(i)},f(x^{(i)}))\
\Rightarrow &amp;\min_{\beta_m,\gamma_m} \sum_{i=1}<sup>NL(y</sup>{(i)},\sum_{m=1}<sup>M\beta_mb(x</sup>{(i)},\gamma_m))
\end{align}
$$
通常式(3)是一个复杂的优化问题，需要同时求解出从<span class="arithmatex"><span class="MathJax_Preview">m=1</span><script type="math/tex">m=1</script></span>到<span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span>的所有参数<span class="arithmatex"><span class="MathJax_Preview">\beta_m,\gamma_m</span><script type="math/tex">\beta_m,\gamma_m</script></span>。前向分布算法求解这一优化问题的思想是：由于是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数，那么就可以简化优化的复杂度。即每步只需优化损失函数：
$$
\min_{\beta,\gamma}\sum_{i=1}^N L(y^{(i)},\beta b(x^{(i)};\gamma))
$$
前向分步算法步骤如下：</p>
<blockquote>
<p>输入：训练数据集<span class="arithmatex"><span class="MathJax_Preview">T=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(N)},y^{(N)})\}</span><script type="math/tex">T=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(N)},y^{(N)})\}</script></span> ；损失函数<span class="arithmatex"><span class="MathJax_Preview">L(y,f(x))</span><script type="math/tex">L(y,f(x))</script></span>；基函数集<span class="arithmatex"><span class="MathJax_Preview">\{ b(x;\gamma)\}</span><script type="math/tex">\{ b(x;\gamma)\}</script></span></p>
<p>输出：加法模型<span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span></p>
<p>(1)初始化<span class="arithmatex"><span class="MathJax_Preview">f_0(x)=0</span><script type="math/tex">f_0(x)=0</script></span></p>
<p>(2)对<span class="arithmatex"><span class="MathJax_Preview">m=1,2,...,M</span><script type="math/tex">m=1,2,...,M</script></span></p>
<p>​ ①极小化损失函数
$$
(\beta_m,\gamma_m)= \arg \min_{\beta,\gamma}\sum_{i=1}^N L(y^{(i)},f_{m-1}(x)+\beta b(x^{(i)};\gamma)) \notag
$$
​     得到参数<span class="arithmatex"><span class="MathJax_Preview">\beta_m,\gamma_m</span><script type="math/tex">\beta_m,\gamma_m</script></span></p>
<p>​ ②更新
$$
f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)\notag
$$
​ ③得到加法模型
$$
f(x)=f_M(x)=\sum_{m=1}^M \beta_mb(x;\gamma_m) \notag
$$
</p>
</blockquote>
<p>前向分步算法的的优势在将同时求解出从<span class="arithmatex"><span class="MathJax_Preview">m=1</span><script type="math/tex">m=1</script></span>到<span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span>的所有参数<span class="arithmatex"><span class="MathJax_Preview">\beta_m,\gamma_m</span><script type="math/tex">\beta_m,\gamma_m</script></span>的优化问题简化为逐次求解各个<span class="arithmatex"><span class="MathJax_Preview">\beta_m,\gamma_m</span><script type="math/tex">\beta_m,\gamma_m</script></span>的优化问题。</p>
<h3 id="adaboost_3">前向分步算法与AdaBoost<a class="headerlink" href="#adaboost_3" title="Permanent link">&para;</a></h3>
<p>通过前向分步算法是可以推导出AdaBoost算法的。其中加法模型为
$$
f(x)=\sum_{m=1}^M(\alpha_mG_m(x))
$$
损失函数为指数函数
$$
L(y,f(x))=\exp(-yf(x))
$$
假设经过<span class="arithmatex"><span class="MathJax_Preview">m-1</span><script type="math/tex">m-1</script></span>轮迭代前向分步算法已经得到<span class="arithmatex"><span class="MathJax_Preview">f_{m-1}(x)</span><script type="math/tex">f_{m-1}(x)</script></span>：
$$
\begin{align}
f_{m-1}(x)
&amp;=f_{m-2}(x)+\alpha_{m-1}G_{m-1}(x)\
&amp;=f_{m-3}(x)+\alpha_{m-2}G_{m-2}(x)+\alpha_{m-1}G_{m-1}(x)\
&amp;=\alpha_{1}G_{1}(x)+\alpha_{2}G_{2}(x)+...+\alpha_{m-1}G_{m-1}(x)\
\end{align}
$$
在第<span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>轮迭代得到<span class="arithmatex"><span class="MathJax_Preview">\alpha_m,G_m(x)</span><script type="math/tex">\alpha_m,G_m(x)</script></span>和<span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span>
$$
f_{m}(x)=f_{m-1}(x)+\alpha_mG_m(x)
$$
目标是使前向分步算法得到<span class="arithmatex"><span class="MathJax_Preview">\alpha_m</span><script type="math/tex">\alpha_m</script></span>和<span class="arithmatex"><span class="MathJax_Preview">G_m(x)</span><script type="math/tex">G_m(x)</script></span>使<span class="arithmatex"><span class="MathJax_Preview">f_m(x)</span><script type="math/tex">f_m(x)</script></span>在训练数据集上的指数损失最小，即
$$
\begin{align}
(\alpha_m,G_m(x))
&amp;=\arg\min_{\alpha,G}\sum_{i=1}<sup>N\exp(-y</sup>{(i)}f_m(x^{(i)}))\
&amp;=\arg\min_{\alpha,G}\sum_{i=1}<sup>N\exp[-y</sup>{(i)}(f_{m-1}(x^{(i)})+\alpha G(x^{(i)}))]\
&amp;=\arg\min_{\alpha,G}\sum_{i=1}<sup>N\exp[-y</sup>{(i)}f_{m-1}(x^{(i)}) - y^{(i)}\alpha G(x^{(i)})]\
&amp;=\arg\min_{\alpha,G}\sum_{i=1}<sup>N{\exp(-y</sup>{(i)}f_{m-1}(x^{(i)})   \cdot  \exp(-y^{(i)}\alpha G(x^{(i)}))}\
\end{align}
$$</p>
<p>令<span class="arithmatex"><span class="MathJax_Preview">\bar{w}_{mi}=\exp[-y^{(i)}f_{m-1}(x^{(i)})]</span><script type="math/tex">\bar{w}_{mi}=\exp[-y^{(i)}f_{m-1}(x^{(i)})]</script></span> ，由于<span class="arithmatex"><span class="MathJax_Preview">\bar{w}_{mi}</span><script type="math/tex">\bar{w}_{mi}</script></span> 既不依赖<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>也不依赖<span class="arithmatex"><span class="MathJax_Preview">G</span><script type="math/tex">G</script></span>，是一个与最小化无关，但是<span class="arithmatex"><span class="MathJax_Preview">\bar{w}_{mi}</span><script type="math/tex">\bar{w}_{mi}</script></span>是依赖于<span class="arithmatex"><span class="MathJax_Preview">f_{m-1}(x)</span><script type="math/tex">f_{m-1}(x)</script></span>，随着每一轮迭代而发生改变。</p>
<p>由于<span class="arithmatex"><span class="MathJax_Preview">y^{(i)},G(x^{(i)})\in \{+1,-1\}</span><script type="math/tex">y^{(i)},G(x^{(i)})\in \{+1,-1\}</script></span>，所以<span class="arithmatex"><span class="MathJax_Preview">y\alpha G(x)</span><script type="math/tex">y\alpha G(x)</script></span>可以写成另一种形式
$$
y\alpha G(x)=
\left{
\begin{array}{}
    \alpha &amp;\quad y\cdot G(x)=1 \
    -\alpha &amp;\quad y\cdot G(x)=-1
\end{array}
\right.
$$
损失函数可以写成
$$
\begin{align}
L(\alpha,G)
&amp;=\sum_{i=1}<sup>N{\exp(-y</sup>{(i)}f_{m-1}(x^{(i)})   \cdot  \exp(-y^{(i)}\alpha G(x^{(i)}))}\
&amp;=\sum_{i=1}<sup>N\bar{w}_{mi}\cdot\exp(-y</sup>{(i)}\alpha G(x^{(i)}))\
&amp;=\sum_{i=1}<sup>N\bar{w}_{mi}\cdot\exp(-y</sup>{(i)}\alpha G(x^{(i)}))\
&amp;=\sum_{y<sup>{(i)}=G_m(x</sup>{(i)})}\bar{w}<em>{mi} e<sup>{-\alpha}+\sum_{y</sup>{(i)}\ne G_m(x^{(i)})}\bar{w}</em>{mi} e^{\alpha}\
&amp;= e^{-\alpha} \sum_{y<sup>{(i)}=G_m(x</sup>{(i)})}\bar{w}<em>{mi} + e^{\alpha} \sum</em>{y^{(i)}\ne G_m(x^{(i)})}\bar{w}<em>{mi} \
&amp;= e^{-\alpha} \sum</em>{i=1}^N \bar{w}<em>{mi} I(y<sup>{(i)}=G_{m}(x</sup>{(i)})) + e^{\alpha} \sum</em>{i=1}^N \bar{w}<em>{mi} I(y^{(i)}\ne G</em>{m}(x^{(i)}))\
&amp;=e^{-\alpha} \sum_{i=1}^N \bar{w}<em>{mi} +(e<sup>{\alpha}+e</sup>{-\alpha}) \sum</em>{i=1}^N \bar{w}<em>{mi} I(y^{(i)}\ne G_m(x^{(i)}))\
\end{align}
$$
对<span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>求导，得到：
$$
\frac{\partial L}{\partial \alpha}=-e<sup>{-\alpha}\sum_{i=1}</sup>N\bar{w}</em>{mi}+(e<sup>{\alpha}+e</sup>{-\alpha})\sum_{i=1}^N \bar{w}<em>{mi}I(y^{(i)}\ne G_m(x^{(i)}))
$$
令式(24)等于0，且定义<span class="arithmatex"><span class="MathJax_Preview">e_m=\sum_{i=1}^N \bar{w}_{mi}I(y^{(i)}\ne G_m(x^{(i)}))</span><script type="math/tex">e_m=\sum_{i=1}^N \bar{w}_{mi}I(y^{(i)}\ne G_m(x^{(i)}))</script></span>，那么有：
$$
\begin{align}
(e<sup>{\alpha}+e</sup>{-\alpha})\sum</em>{i=1}^N \bar{w}<em>{mi}I(y^{(i)}\ne G_m(x<sup>{(i)}))&amp;=e</sup>{-\alpha}\sum</em>{i=1}^N\bar{w}<em>{mi}\
(e<sup>{\alpha}+e</sup>{-\alpha})e_m&amp;=e<sup>{-\alpha}\sum_{i=1}</sup>N\bar{w}</em>{mi}\
(e<sup>{\alpha}+e</sup>{-\alpha})e_m&amp;=e^{-\alpha}\</p>
<p>\end{align}
$$
式(25)进行移项操作，式(25)-(26)利用了定义<span class="arithmatex"><span class="MathJax_Preview">e_m=\sum_{i=1}^N \bar{w}_{mi}I(y^{(i)}\ne G_m(x^{(i)}))</span><script type="math/tex">e_m=\sum_{i=1}^N \bar{w}_{mi}I(y^{(i)}\ne G_m(x^{(i)}))</script></span> ，式(26)-(27)使用<span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^N\bar{w}_{mi}=1</span><script type="math/tex">\sum_{i=1}^N\bar{w}_{mi}=1</script></span>。最终可以求解得到：
$$
\alpha = \frac{1}{2}\ln \frac{1-e_m}{e_m}
$$
这里得到的<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>和Adaboost算法中给出的公式完全一致。</p>
<p>最后来看每一轮样本权值的更新，由于<span class="arithmatex"><span class="MathJax_Preview">\bar{w}_{mi}=\exp(-y^{(i)}f_{m-1}(x^{(i)}))</span><script type="math/tex">\bar{w}_{mi}=\exp(-y^{(i)}f_{m-1}(x^{(i)}))</script></span>，于是得到：
$$
\begin{align}
\bar{w}<em>{m+1,i}
&amp;=\exp(-y<sup>{(i)}f_{m}(x</sup>{(i)}))\
&amp;=\exp[-y<sup>{(i)}(f_{m-1}(x</sup>{(i)})+\alpha_m G_m(x^{(i)}))]\
&amp;=\bar{w}</em>{mi}\exp(-y^{(i)}\alpha_m G_m(x^{(i)}))\
\end{align}
$$
式(31)和Adaboost算法中样本权值的更新，只是相差了规范化因子。</p>
<h2 id="adaboost_4">Adaboost算法误差分析<a class="headerlink" href="#adaboost_4" title="Permanent link">&para;</a></h2>
<p>Adaboost最基本的性质是在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。</p>
<blockquote>
<p>定理：Adaboost算法最终分类器的训练误差界为
$$
\frac{1}{N}\sum_{i=1}^N I(G(x^{(i)})\ne y^{(i)})\le \frac{1}{N}\sum_{i}\exp(-y<sup>{(i)}f(x</sup>{(i)}))=\prod_m Z_m
$$
</p>
</blockquote>
<p>证明：首先证明第一个不等式：当<span class="arithmatex"><span class="MathJax_Preview">G(x^{(i)})\ne y^{(i)}</span><script type="math/tex">G(x^{(i)})\ne y^{(i)}</script></span>时，<span class="arithmatex"><span class="MathJax_Preview">y^{(i)}f(x^{(i)})\lt 0</span><script type="math/tex">y^{(i)}f(x^{(i)})\lt 0</script></span>，因而<span class="arithmatex"><span class="MathJax_Preview">\exp(-y^{(i)}f(x^{(i)}))\ge 1</span><script type="math/tex">\exp(-y^{(i)}f(x^{(i)}))\ge 1</script></span>。</p>
<p>得到
$$
\sum_i \exp(-y<sup>{(i)}f(x</sup>{(i)}))\ge N \
\Rightarrow \quad
\frac{1}{N}\sum_i \exp(-y<sup>{(i)}f(x</sup>{(i)}))\ge \frac{1}{N}\cdot N =1 \ge  \frac{1}{N}\sum_{i=1}^N I(G(x^{(i)})\ne y^{(i)})\
$$
证明第二个等式需要使用到<span class="arithmatex"><span class="MathJax_Preview">Z_m</span><script type="math/tex">Z_m</script></span>的定义和样本权重初始值定义：
$$
\begin{align}
w_{1i}&amp;=\frac{1}{N}\
Z_m w_{m+1,i}&amp;=w_{mi}\exp(-\alpha_m y<sup>{(i)}G_m(x</sup>{(i)}))
\end{align}
$$
那么有：
$$
\begin{align}
\frac{1}{N}&amp;\sum_i \exp(y<sup>{(i)}f(x</sup>{(i)}))\
&amp;=\frac{1}{N}\sum_i \exp(  \sum_{m=1}^M\alpha_m y<sup>{(i)}G_m(x</sup>{(i)}) )\
&amp;=\sum_i \frac{1}{N} \exp(  \sum_{m=1}^M\alpha_m y<sup>{(i)}G_m(x</sup>{(i)}) )\
&amp;=\sum_i w_{1i} \exp(  \sum_{m=1}^M\alpha_m y<sup>{(i)}G_m(x</sup>{(i)}) )\
&amp;=\sum_i [ w_{1i}  \alpha_m y<sup>{(1)}G_m(x</sup>{(1)}) ]  \exp(  \sum_{i=2}^M\alpha_m y<sup>{(i)}G_m(x</sup>{(i)}) )\
&amp;=\sum_i [ w_{1i}  \alpha_m y<sup>{(1)}G_m(x</sup>{(1)}) ]  \exp(  \sum_{i=2}^M\alpha_m y<sup>{(i)}G_m(x</sup>{(i)}) )\
&amp;=\sum_i [ Z_1 w_{2i} ]  \exp(  \sum_{i=2}^M\alpha_m y<sup>{(i)}G_m(x</sup>{(i)}) )\
&amp;=Z_1\sum_i w_{2i}   \exp(  \sum_{i=2}^M\alpha_m y<sup>{(i)}G_m(x</sup>{(i)}) )\
&amp;=Z_1Z_2\sum_i w_{3i}   \exp(  \sum_{i=3}^M\alpha_m y<sup>{(i)}G_m(x</sup>{(i)}) )\
&amp;=...\
&amp;=Z_1Z_2 ... Z_{M-1}\sum_i w_{Mi}   \exp(  \sum_{i}\alpha_M y<sup>{(i)}G_M(x</sup>{(i)}) )\
&amp;=Z_1Z_2 ... Z_{M-1}Z_{M}\
&amp;=\prod_{i=1}^M Z_i\
\end{align}
$$
这一定理说明，可以在每一轮选取适当的<span class="arithmatex"><span class="MathJax_Preview">G_m</span><script type="math/tex">G_m</script></span>使得<span class="arithmatex"><span class="MathJax_Preview">Z_m</span><script type="math/tex">Z_m</script></span>最小，从而使训练误差下降最快。</p>
<blockquote>
<p>对于<span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>的推导方式有两种：第一种是最小化损失函数进行推导（上面的式(28)的推导方式）；第二种是最小化训练误差界进行推导。现在对第二种方式进行推导：
$$
\begin{array}{}
Z_m
&amp;=\sum_{i=1}^N w_{mi}\exp(-\alpha_my<sup>{(i)}G_m(x</sup>{(i)}))\
&amp;=\sum_\limits{y<sup>{(i)}=G_m(x</sup>{(i)})} w_{mi} e<sup>{-\alpha}+\sum_\limits{y</sup>{(i)}\ne G_m(x^{(i)})}w_{mi} e^{\alpha}\
&amp;=(1-e_m)\cdot e^{-\alpha_m}+e_m\cdot e^{\alpha_m}
\end{array}
$$
对式(48)求导等于0，可以得到：
$$
\frac{\partial Z_m}{\partial \alpha_m}=-(1-e_m)\cdot e^{-\alpha_m}+e_m\cdot e^{\alpha_m}=0
$$
求得
$$
\alpha_m = \frac{1}{2}\ln \frac{1-e_m}{e_m}
$$
</p>
</blockquote>
<h3 id="adaboost_5">Adaboost算法扩展<a class="headerlink" href="#adaboost_5" title="Permanent link">&para;</a></h3>
<p>对于Adaboost多元分类算法，原理和二元分类类似，主要的区别在于弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器系数
$$
\alpha_m =\frac{1}{2}\ln\frac{1-e_m}{e_m}+\ln(C-1)
$$
其中<span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span>是类别数。如果是二元分类则<span class="arithmatex"><span class="MathJax_Preview">C=2</span><script type="math/tex">C=2</script></span>，和二元分类算法中的系数是一致的。</p>
<p>Adaboost算法也可以处理回归问题，常见的有Adaboost R2回归等都类似。</p>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../21_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 集成学习算法之综述" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              集成学习算法之综述
            </div>
          </div>
        </a>
      
      
        
        <a href="../23_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/" class="md-footer__link md-footer__link--next" aria-label="下一页: 集成学习算法之GBDT" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              集成学习算法之GBDT
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs"], "search": "../../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
        <script src="../../../javascript/config.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>