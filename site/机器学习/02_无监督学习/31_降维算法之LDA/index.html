
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="采用Mkdocs-material生成的文档管理网站支持的markdown语法，包括传统语法和扩展语法">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>降维算法之LDA - 我的知识笔记</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.9647289d.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="pink">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lda" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="我的知识笔记" class="md-header__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            我的知识笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              降维算法之LDA
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-tabs__link md-tabs__link--active">
        机器学习
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-tabs__link">
        深度学习
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="我的知识笔记" class="md-nav__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    我的知识笔记
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          01 监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="01 监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          01 监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-nav__link">
        数学符号
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E6%80%BB%E7%BB%93/" class="md-nav__link">
        线性模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/01_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" class="md-nav__link">
        最小二乘法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/02_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" class="md-nav__link">
        梯度下降算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/03_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8E%9F%E7%90%86/" class="md-nav__link">
        交叉验证原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/04_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="md-nav__link">
        模型评估
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/05_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        线性回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/06_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        逻辑回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/07_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        决策树算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/08_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        决策树算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/09_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        决策树算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/10_%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        近邻算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/11_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        朴素贝叶斯算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/12_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        最大熵算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/13_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        最大熵算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/14_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        最大熵算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/15_%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        感知机算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/16_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        支持向量机算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/17_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        支持向量机算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/18_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        支持向量机算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/19_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E5%9B%9B%29/" class="md-nav__link">
        支持向量机算法原理(四)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/20_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%94%29/" class="md-nav__link">
        支持向量机算法原理(五)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/21_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        集成学习算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/22_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BAdaboost/" class="md-nav__link">
        集成学习算法之Adaboost
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/23_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BRF/" class="md-nav__link">
        集成学习算法之RF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/24_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/" class="md-nav__link">
        集成学习算法之GBDT
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          02 无监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="02 无监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          02 无监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../25_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BK-Means/" class="md-nav__link">
        聚类算法之K-Means​
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../26_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BBIRCH/" class="md-nav__link">
        聚类算法之BIRCH
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../27_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BDBSCAN/" class="md-nav__link">
        聚类算法之DBSCAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../28_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BMean%20Shift/" class="md-nav__link">
        聚类算法之Mean Shift
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../29_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E8%B0%B1%E8%81%9A%E7%B1%BB/" class="md-nav__link">
        聚类算法之谱聚类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../30_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BPCA/" class="md-nav__link">
        降维算法之PCA
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          降维算法之LDA
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        降维算法之LDA
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lda_1" class="md-nav__link">
    LDA算法思想
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    瑞利商和广义瑞利商
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lda_2" class="md-nav__link">
    二类LDA原理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lda_3" class="md-nav__link">
    多类LDA原理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lda_4" class="md-nav__link">
    LDA算法流程
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lda_5" class="md-nav__link">
    LDA算法小结
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../32_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BIsomap/" class="md-nav__link">
        降维算法之Isomap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../33_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLLE/" class="md-nav__link">
        降维算法之LLE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../34_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BApriori/" class="md-nav__link">
        关联算法之Apriori
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../35_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BFP-Tree/" class="md-nav__link">
        关联算法之FP-Tree
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../36_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        推荐算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../37_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/" class="md-nav__link">
        推荐算法之矩阵分解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../38_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BSimRank/" class="md-nav__link">
        推荐算法之SimRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../39_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BPersonalRank/" class="md-nav__link">
        推荐算法之PersonalRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../40_EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        EM算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../41_%E5%88%86%E8%A7%A3%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        分解机算法原理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="深度学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        神经网络基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01_DNN%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        DNN前馈神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        CNN卷积神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        RNN循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04_%E5%9F%BA%E4%BA%8E%E9%97%A8%E6%8E%A7%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        基于门控的循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%281%29/" class="md-nav__link">
        神经网络优化概述(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%282%29/" class="md-nav__link">
        神经网络优化概述(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06_%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" class="md-nav__link">
        网络正则化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="md-nav__link">
        注意力机制
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lda_1" class="md-nav__link">
    LDA算法思想
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    瑞利商和广义瑞利商
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lda_2" class="md-nav__link">
    二类LDA原理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lda_3" class="md-nav__link">
    多类LDA原理
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lda_4" class="md-nav__link">
    LDA算法流程
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lda_5" class="md-nav__link">
    LDA算法小结
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="lda">降维算法之LDA<a class="headerlink" href="#lda" title="Permanent link">&para;</a></h1>
<p>线性判别分析(Linear Discriminant Analysis，简称LDA)是一种经典的分类和降维算法。注意这里需要和自然语言处理领域的LDA分区开，在自然语言处理领域，LDA是隐含狄利克雷分布(Latent Dirichlet Allocation，简称LDA)是一种处理文档的主题模型。</p>
<h2 id="lda_1">LDA算法思想<a class="headerlink" href="#lda_1" title="Permanent link">&para;</a></h2>
<p>LDA是一种监督学习的降维技术，每个样本都是有类别信息的。而PCA不同，PCA是不考虑样本类别信息的无监督降维技术。LDA的基本思想就是：投影后类内方差最小，类间方差最大。也就是保证数据在低维度上进行投影，投影后每一种类别的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。</p>
<p>如图所示，有两类二维数据，分别标记为红色和蓝色，现在将数据投影到一维的一条直线，让每个类别数据的投影尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。</p>
<p><img alt="lda" src="../assets/lda.png" /></p>
<p>从直观上认为，右图比左图的投影效果好，因为右图中的红色数据和蓝色数据较为集中，且类别之间的距离明显。而左图的数据在交界处混杂。</p>
<h2 id="_1">瑞利商和广义瑞利商<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p>瑞利商(Rayleigh Quotient)的定义函数<span class="arithmatex"><span class="MathJax_Preview">R(A,x)</span><script type="math/tex">R(A,x)</script></span>表达式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R(A,x)=\frac{x^H A x}{x^H x}
</div>
<script type="math/tex; mode=display">
R(A,x)=\frac{x^H A x}{x^H x}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>是非零向量，<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>是<span class="arithmatex"><span class="MathJax_Preview">n\times n</span><script type="math/tex">n\times n</script></span>的Hermitan矩阵(厄米特矩阵)。所谓的Hermitan矩阵是指满足共轭转置矩阵和原始矩阵相等的矩阵，即<span class="arithmatex"><span class="MathJax_Preview">A^H = A</span><script type="math/tex">A^H = A</script></span>。如果矩阵<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>是实矩阵，则满足<span class="arithmatex"><span class="MathJax_Preview">A^T=A</span><script type="math/tex">A^T=A</script></span>的矩阵即为Hermitan矩阵。</p>
<p>瑞利商<span class="arithmatex"><span class="MathJax_Preview">R(A,x)</span><script type="math/tex">R(A,x)</script></span>的一个重要性质：它的最大值等于矩阵<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>的最大特征值，而最小值等于矩阵<span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>的最小特征值，也就是满足</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\lambda_{min} \le \frac{x^H A x}{x^H x} \le \lambda_{max}
</div>
<script type="math/tex; mode=display">
\lambda_{min} \le \frac{x^H A x}{x^H x} \le \lambda_{max}
</script>
</div>
<p>当向量<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>是标准正交基时，即满足<span class="arithmatex"><span class="MathJax_Preview">x^H x=1</span><script type="math/tex">x^H x=1</script></span>时，瑞利商<span class="arithmatex"><span class="MathJax_Preview">R(A,x)=x^H A x</span><script type="math/tex">R(A,x)=x^H A x</script></span> （注意这个形式在谱聚类和PCA都有出现）</p>
<p>广义瑞利商(Genralized Rayleigh Quotient)的定义函数<span class="arithmatex"><span class="MathJax_Preview">R(A,B,x)</span><script type="math/tex">R(A,B,x)</script></span>表达式为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R(A,B,x)=\frac{x^H A x}{x^H B x}
</div>
<script type="math/tex; mode=display">
R(A,B,x)=\frac{x^H A x}{x^H B x}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>是非零向量，<span class="arithmatex"><span class="MathJax_Preview">A,B</span><script type="math/tex">A,B</script></span>为<span class="arithmatex"><span class="MathJax_Preview">n\times n</span><script type="math/tex">n\times n</script></span>的Hermitan矩阵，<span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span>为正定矩阵。</p>
<p>令<span class="arithmatex"><span class="MathJax_Preview">x=B^{-1/2} x^{'}</span><script type="math/tex">x=B^{-1/2} x^{'}</script></span>，则分子分母转化为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align}
&amp;x^H B x = (B^{-1/2}x^{'})^H B (B^{-1/2}x^{'})=x^{'H}B^{-1/2H}B B^{-1/2} x^{'}=x^{'H}B^{-1/2}B B^{-1/2} x^{'}=x^{'H}x^{'}\\
&amp;x^H A x = (B^{-1/2}x^{'})^H A (B^{-1/2}x^{'})=x^{'H}B^{-1/2H}A B^{-1/2} x^{'}=x^{'H}B^{-1/2}A B^{-1/2} x^{'}
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
&x^H B x = (B^{-1/2}x^{'})^H B (B^{-1/2}x^{'})=x^{'H}B^{-1/2H}B B^{-1/2} x^{'}=x^{'H}B^{-1/2}B B^{-1/2} x^{'}=x^{'H}x^{'}\\
&x^H A x = (B^{-1/2}x^{'})^H A (B^{-1/2}x^{'})=x^{'H}B^{-1/2H}A B^{-1/2} x^{'}=x^{'H}B^{-1/2}A B^{-1/2} x^{'}
\end{align}
</script>
</div>
<p>根据式(4)和式(5)，广义瑞利商可以写成：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R(A,B,x)=\frac{x^H A x}{x^H B x}=\frac{x^{'H}B^{-1/2}A B^{-1/2} x^{'}}{x^{'H}x^{'}}
</div>
<script type="math/tex; mode=display">
R(A,B,x)=\frac{x^H A x}{x^H B x}=\frac{x^{'H}B^{-1/2}A B^{-1/2} x^{'}}{x^{'H}x^{'}}
</script>
</div>
<p>利用前面瑞利商的性质，可知<span class="arithmatex"><span class="MathJax_Preview">R(A,B,x)</span><script type="math/tex">R(A,B,x)</script></span>的最大值为矩阵<span class="arithmatex"><span class="MathJax_Preview">B^{-1/2}AB^{-1/2}</span><script type="math/tex">B^{-1/2}AB^{-1/2}</script></span>的最大特征值，或者说矩阵<span class="arithmatex"><span class="MathJax_Preview">B^{-1}A</span><script type="math/tex">B^{-1}A</script></span>的最大特征值，而最小值为矩阵<span class="arithmatex"><span class="MathJax_Preview">B^{-1}A</span><script type="math/tex">B^{-1}A</script></span>的最小特征值。</p>
<h2 id="lda_2">二类LDA原理<a class="headerlink" href="#lda_2" title="Permanent link">&para;</a></h2>
<p>假设数据集<span class="arithmatex"><span class="MathJax_Preview">D=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})  \}</span><script type="math/tex">D=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})  \}</script></span>，其中<span class="arithmatex"><span class="MathJax_Preview">x^{(i)} \in \mathbb{R}^n,y^{(i)} \in \{-1,+1\}</span><script type="math/tex">x^{(i)} \in \mathbb{R}^n,y^{(i)} \in \{-1,+1\}</script></span>。现在定义<span class="arithmatex"><span class="MathJax_Preview">N_j(j=0,1)</span><script type="math/tex">N_j(j=0,1)</script></span>为第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>个样本的个数，<span class="arithmatex"><span class="MathJax_Preview">X_j(j=0,1)</span><script type="math/tex">X_j(j=0,1)</script></span>为第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>类样本的集合，<span class="arithmatex"><span class="MathJax_Preview">\mu_j(j=0,1)</span><script type="math/tex">\mu_j(j=0,1)</script></span>为第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>类样本的均值向量，<span class="arithmatex"><span class="MathJax_Preview">\Sigma_j(j=1,2)</span><script type="math/tex">\Sigma_j(j=1,2)</script></span>为第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>类样本的协方差矩阵（准确说是缺少分母部分的协方差矩阵）。</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align}
\mu_j &amp;= \frac{1}{N_j}\sum_{x \in X_j}x \qquad j=0,1\\
\Sigma_j &amp;= \sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T \qquad j=0,1
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
\mu_j &= \frac{1}{N_j}\sum_{x \in X_j}x \qquad j=0,1\\
\Sigma_j &= \sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T \qquad j=0,1
\end{align}
</script>
</div>
<p>由于是两类数据，因此只需要将数据投影到一条直线即可。假设投影直线是向量<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>，则对任意一个样本<span class="arithmatex"><span class="MathJax_Preview">x^{(i)}</span><script type="math/tex">x^{(i)}</script></span>在直线<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>的投影为<span class="arithmatex"><span class="MathJax_Preview">w^T x^{(i)}</span><script type="math/tex">w^T x^{(i)}</script></span>。对于两个类别的中心点<span class="arithmatex"><span class="MathJax_Preview">\mu_0,\mu_1</span><script type="math/tex">\mu_0,\mu_1</script></span>，在直线<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>的投影为<span class="arithmatex"><span class="MathJax_Preview">w^T \mu_0</span><script type="math/tex">w^T \mu_0</script></span>和<span class="arithmatex"><span class="MathJax_Preview">w^T \mu_1</span><script type="math/tex">w^T \mu_1</script></span>。由于LDA需要让不同类别的数据的类别中心之间的距离尽可能的大，即最大化<span class="arithmatex"><span class="MathJax_Preview">||w^T \mu_0-w^T \mu_1||_2^2</span><script type="math/tex">||w^T \mu_0-w^T \mu_1||_2^2</script></span>，同时需要让同一种类别数据的投影尽可能的接近，即最小化协方差<span class="arithmatex"><span class="MathJax_Preview">(w^T \Sigma_0 w</span><script type="math/tex">(w^T \Sigma_0 w</script></span>+<span class="arithmatex"><span class="MathJax_Preview">w^T \Sigma_1 w)</span><script type="math/tex">w^T \Sigma_1 w)</script></span>尽可能的小。综上优化目标为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\arg \max_{w} J(w)=\frac{||w^T \mu_0 - w^T \mu_1||_2^2}{w^T \Sigma_0 w+w^T \Sigma_1 w}=\frac{w^T(\mu_0-\mu_1)(\mu_0 - \mu_1)^Tw}{w^T \Sigma_0 w+w^T \Sigma_1 w}
</div>
<script type="math/tex; mode=display">
\arg \max_{w} J(w)=\frac{||w^T \mu_0 - w^T \mu_1||_2^2}{w^T \Sigma_0 w+w^T \Sigma_1 w}=\frac{w^T(\mu_0-\mu_1)(\mu_0 - \mu_1)^Tw}{w^T \Sigma_0 w+w^T \Sigma_1 w}
</script>
</div>
<p>一般定义类内散度矩阵<span class="arithmatex"><span class="MathJax_Preview">S_w</span><script type="math/tex">S_w</script></span>为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
S_w=\Sigma_0+\Sigma_1=\sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T+\sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T
</div>
<script type="math/tex; mode=display">
S_w=\Sigma_0+\Sigma_1=\sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T+\sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T
</script>
</div>
<p>定义类间散度矩阵<span class="arithmatex"><span class="MathJax_Preview">S_b</span><script type="math/tex">S_b</script></span>为：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T
</div>
<script type="math/tex; mode=display">
S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T
</script>
</div>
<p>最终优化目标可以写成：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\arg \max_{w} J(w)=\frac{w^T S_b w}{w^T S_w w}
</div>
<script type="math/tex; mode=display">
\arg \max_{w} J(w)=\frac{w^T S_b w}{w^T S_w w}
</script>
</div>
<p>可以发现式(12)就是广义瑞利商。利用广义瑞利商的性质可知，<span class="arithmatex"><span class="MathJax_Preview">J(w)</span><script type="math/tex">J(w)</script></span>的最大值为矩阵<span class="arithmatex"><span class="MathJax_Preview">S^{-1}_w S_b</span><script type="math/tex">S^{-1}_w S_b</script></span>的最大特征值，而对应的<span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>为<span class="arithmatex"><span class="MathJax_Preview">S^{-1}_wS_b</span><script type="math/tex">S^{-1}_wS_b</script></span>的最大特征值对应的特征向量。</p>
<h2 id="lda_3">多类LDA原理<a class="headerlink" href="#lda_3" title="Permanent link">&para;</a></h2>
<p>假设数据集<span class="arithmatex"><span class="MathJax_Preview">D=\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),..., (x^{(m)},y^{(m)}) \}</span><script type="math/tex">D=\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),..., (x^{(m)},y^{(m)}) \}</script></span>，其中<span class="arithmatex"><span class="MathJax_Preview">x^{(i)} \in \mathbb{R}^n,y^{(i)} \in \{C_1,C_2,...,C_k\}，定义</span><script type="math/tex">x^{(i)} \in \mathbb{R}^n,y^{(i)} \in \{C_1,C_2,...,C_k\}，定义</script></span><span class="arithmatex"><span class="MathJax_Preview">N_j(j=1,2,...,k)</span><script type="math/tex">N_j(j=1,2,...,k)</script></span>为第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>类样本的个数，<span class="arithmatex"><span class="MathJax_Preview">X_j(j=1,2,...,k)</span><script type="math/tex">X_j(j=1,2,...,k)</script></span>为第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>类样本的集合，<span class="arithmatex"><span class="MathJax_Preview">\mu_j(j=1,2,...,k)</span><script type="math/tex">\mu_j(j=1,2,...,k)</script></span>为第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>类样本的均值向量，<span class="arithmatex"><span class="MathJax_Preview">\Sigma_j(j=1,2,..,k)</span><script type="math/tex">\Sigma_j(j=1,2,..,k)</script></span>为第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>类样本的协方差矩阵。</p>
<p>由于多类向低维投影，此时投影到的低维空间就不是一条直线，而是一个超平面。假设投影到的低维空间的维度为<span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>，对应的向量为<span class="arithmatex"><span class="MathJax_Preview">(w_1,w_2,...,w_d)</span><script type="math/tex">(w_1,w_2,...,w_d)</script></span>，基向量组成的矩阵为<span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>，是一个<span class="arithmatex"><span class="MathJax_Preview">n\times d</span><script type="math/tex">n\times d</script></span>的矩阵。</p>
<p>此时的优化目标可以变成：
$$
\frac{W^T S_b W}{W^T S_w W}
$$
其中，<span class="arithmatex"><span class="MathJax_Preview">S_b=\sum_{j=1}^kN_j(\mu_j - \mu)(\mu_j-\mu)^T,S_w=\sum_{j=1}^k S_{wj}=\sum_{j=1}^k\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T</span><script type="math/tex">S_b=\sum_{j=1}^kN_j(\mu_j - \mu)(\mu_j-\mu)^T,S_w=\sum_{j=1}^k S_{wj}=\sum_{j=1}^k\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T</script></span>，<span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span>为所有样本均值向量。</p>
<p>由于式(13)中的分子和分母都是矩阵，不是标量，无法直接作为一个标量函数来优化。因此无法直接使用二类LDA的优化方法。一般来说，可以用其他的一些替代优化目标来实现，比如常见的一个LDA多类优化目标函数定义为：
$$
\begin{align}
\arg \max_{W} J(W)
&amp;=\frac{\prod_\limits{diag} W^T S_b W}{\prod_\limits{diag} W^T S_w W}\
&amp;=\frac{\prod_\limits{i=1}^d w_i^T S_b w_i}{\prod_\limits{i=1}^d w_i^T S_w w_i}\
&amp;=\prod_\limits{i=1}^d \frac{w_i^T S_b w_i}{ w_i^T S_w w_i}\
\end{align}
$$
观察式(16)就是广义瑞利商。根据瑞利商的性质可知，最大值是矩阵<span class="arithmatex"><span class="MathJax_Preview">S^{-1}_w S_b</span><script type="math/tex">S^{-1}_w S_b</script></span>的最大特征值，最大的<span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>个值的乘积就是矩阵<span class="arithmatex"><span class="MathJax_Preview">S_w^{-1}S_b</span><script type="math/tex">S_w^{-1}S_b</script></span>的最大的<span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>个特征值乘积，此时对应的矩阵<span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>为对应最大的<span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>个特征值对应的特征向量张成的矩阵。</p>
<h2 id="lda_4">LDA算法流程<a class="headerlink" href="#lda_4" title="Permanent link">&para;</a></h2>
<p>LDA算法流程：</p>
<blockquote>
<p>输入：数据集<span class="arithmatex"><span class="MathJax_Preview">D=\{ (x^{(1)},y^{(1))}),  (x^{(2)},y^{(2))}),..., (x^{(m)},y^{(m))})\},x^{(i)}\in \mathbb{R}^n,y\in \{C_1,C_2,...,C_k \}</span><script type="math/tex">D=\{ (x^{(1)},y^{(1))}),  (x^{(2)},y^{(2))}),..., (x^{(m)},y^{(m))})\},x^{(i)}\in \mathbb{R}^n,y\in \{C_1,C_2,...,C_k \}</script></span>和降维<span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span></p>
<p>输出：<span class="arithmatex"><span class="MathJax_Preview">x^{(i)}\in \mathbb{R}^d</span><script type="math/tex">x^{(i)}\in \mathbb{R}^d</script></span></p>
<p>（1）计算类内散度矩阵<span class="arithmatex"><span class="MathJax_Preview">S_w</span><script type="math/tex">S_w</script></span></p>
<p>（2）计算类间散度矩阵<span class="arithmatex"><span class="MathJax_Preview">S_b</span><script type="math/tex">S_b</script></span></p>
<p>（3）计算矩阵<span class="arithmatex"><span class="MathJax_Preview">S^{-1}_wS_b</span><script type="math/tex">S^{-1}_wS_b</script></span>最大的<span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>个特征值和对应的<span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>个特征向量<span class="arithmatex"><span class="MathJax_Preview">(w_1,w_2,...,w_d)</span><script type="math/tex">(w_1,w_2,...,w_d)</script></span>得到投影矩阵<span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span></p>
<p>（4）对样本中的每一个样本<span class="arithmatex"><span class="MathJax_Preview">x^{(i)}</span><script type="math/tex">x^{(i)}</script></span>，转化为新的样本<span class="arithmatex"><span class="MathJax_Preview">z^{(i)}=W^T x^{(i)}</span><script type="math/tex">z^{(i)}=W^T x^{(i)}</script></span></p>
<p>（5）输出降维后的样本集 <span class="arithmatex"><span class="MathJax_Preview">D^{'}=(z^{(1)},y^{(1))}),  (z^{(2)},y^{(2))}),..., (z^{(m)},y^{(m)})</span><script type="math/tex">D^{'}=(z^{(1)},y^{(1))}),  (z^{(2)},y^{(2))}),..., (z^{(m)},y^{(m)})</script></span>  </p>
</blockquote>
<p>LDA除了可以用于降维以外，还可以用于分类。一个常见的LDA分类基本思想是假设各个类别的样本数据符合高斯分布，这样利用LDA进行投影后，可以利用极大似然估计计算各个类别投影数据的均值和方差，进而得到该类别高斯分布的概率密度函数。当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别带入各个类别的高斯分布概率密度函数，计算它属于这个类别的概率，最大的概率对应的类别即为预测类别。</p>
<h2 id="lda_5">LDA算法小结<a class="headerlink" href="#lda_5" title="Permanent link">&para;</a></h2>
<p>LDA算法既可以用来降维，又可以用来分类，但是目前来说，主要还是用于降维。在我们进行图像识别图像识别相关的数据分析时，LDA是一个有力的工具。</p>
<p>LDA算法的主要优点有：</p>
<p>（1）在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习则无法使用类别先验知识。</p>
<p>（2）LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。</p>
<p>LDA算法的主要缺点有：</p>
<p>（1）LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。</p>
<p>（2）LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。当然目前有一些LDA的进化版算法可以绕过这个问题。</p>
<p>（3）LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。</p>
<p>（4）LDA可能过度拟合数据。</p>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../30_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BPCA/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 降维算法之PCA" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              降维算法之PCA
            </div>
          </div>
        </a>
      
      
        
        <a href="../32_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BIsomap/" class="md-footer__link md-footer__link--next" aria-label="下一页: 降维算法之Isomap" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              降维算法之Isomap
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs"], "search": "../../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
        <script src="../../../javascript/config.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>