# 文本挖掘之分词原理

在做文本挖掘的时候，首先需要处理的就是分词。词是最小的能够独立活动的有意义的语言成分。英文单词有空格隔开容易按照空格分词，但是有时候需要把多个单词作为一个分词，比如一些名词如"New York"，就需要当做一个词看待。而中文由于没有空格，词语之间没有明显的区分标识。因此中文分词是中文信息处理的基础和关键。

常见的分词算法可分为三大类：

(1)**基于词典**：基于词典/词库匹配的分词方法；（也叫作字符串匹配、机械分词法）

(2)**基于统计**：基于词频统计的分词方法；

(3)**基于规则**：基于知识理解的分词方法；



## 基于词典的分词算法

基于词典的分词算法也叫字符串匹配或者机械分词法，这种方法按照一定策略将待分析的汉字串与一个"充分大"的机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。

按照扫描方向可以分为：正向匹配和逆向匹配

按照长度可以分为：最大匹配和最小匹配


### 正向最大匹配算法(FMM)

FMM算法的基本流程：

> (1)从左向右取待切分汉语句的$m$个字符作为匹配字段，$m$为最大机器词典中最长词的长度。
>
> (2)查找机器词典并进行匹配：
>
> ​	①若匹配成功，则将这个匹配字段作为一个词切分出来；
>
> ​	②若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，重复步骤(2)直到切出所有词为止



### 逆向最大匹配算法RMM

RMM是FMM算法的逆向思维，RMM算法的基本流程：

RMM算法的基本流程：

> (1)从右向左取待切分汉语句的$m$个字符作为匹配字段，$m$为最大机器词典中最长词的长度。
>
> (2)查找机器词典并进行匹配：
>
> ​	①若匹配成功，则将这个匹配字段作为一个词切分出来；
>
> ​	②若匹配不成功，则将这个匹配字段的最前一个字去掉，剩下的字符串作为新的匹配字段，重复步骤(2)直到切出所有词为止

实验表明，逆向最大匹配算法RMM是要优于正向最大匹配算法FMM的。



### 双向最大匹配算法BMM

双向最大匹配法BMM是将正向最大匹配法FMM得到的结果和逆向最大匹配法RMM的结果进行比较，从而决定正确的分词方法。如果两中分词方法得到的匹配结果相同，则认为分词正确；否则按最小集处理。BMM在实用中文信息处理系统中有着广泛的应用。



现在对"**南京市长江大桥**"这个句子进行分词：

（1）FMM算法

首先拿出前5个字符"南京市长江"，匹配不成功；去掉最后一个字符,剩下"南京市长",匹配成功；

对剩下的"江大桥"再次进行正向最大匹配，会切出"江"、"大桥"；

（2）RMM算法

首先取出后5个字符"市长江大桥"，匹配不成功；去掉最前一个字符，剩下"长江大桥",匹配成功；

对剩下的"南京市"再次进行逆向最大匹配，会切出"南京市"；

（3）BMM算法

根据前面的FMM和RMM的分词结果，如果按最小集的评判标准，最后的结果取"南京市"、"长江大桥"。BMM算法还能有一个优势，即可以把所有可能的最大词都分出来，上面的句子可以得到："南京市","南京市长","长江大桥","江","大桥"



## 基于统计的分词算法

基于统计的分词算法在给定大量语料库的前提下，利用统计机器学习模型学习词语切分规律，从而实现对未知文本的切分。随着大规模语料库的建立，基于统计的分词算法逐渐成为主流方法。

主要的统计模型有：$N$元文法模型(N-gram)、隐马尔可夫模型(HMM)、最大熵模型(MaxEnt)、条件随机场模型(CRF)等等。



### 语言模型

统计语言模型是一个单词序列上的概率分布，对于一个给定长度为$m$的序列，它可以为整个序列产生一个概率为$P(w_1,w_2,...,w_m)$。利用语言模型可以确定哪个词序列的可能性更大，或者给定若干个词，预测下一个最可能出现的词语。

给定句子(词语序列)$S=w_1w_2...w_m$，它的概率可以表示为
$$
P(S)=P(w_1w_2...w_m)=p(w_1)p(w_2|w_1)...p(w_m|w_1w_2...w_{m-1}) \tag{1}
$$
式(1)中参数太多，因此需要近似的计算方法。

n-gram模型也称为$n-1$阶马尔科夫模型，有一个有限历史假设：当前词的出现概率仅仅与前面的$n-1$个词有关，那么式(1)可以近似为：

$$
\begin{align}
P(S)
&=P(w_1w_2...w_m)\\
&=p(w_1)p(w_2|w_1)...p(w_m|w_1w_2...w_{m-1})\\
&\approx\prod_{i=1}^m p(w_i|w_{i-n+1},...,w_{i-1})
\end{align}
$$

这里的条件概率可以用词频来计算得到：
$$
 p(w_i|w_{i-n+1},...,w_{i-1})=\frac{count(w_{i-n+1},...,w_{i-1}w_i )}{count(w_{i-n+1},...,w_{i-1})}
$$
当$n=1,2,3$时n-gram模型可以分别称为unigram、bigram和trigram语言模型。$n$越大模型越准确同时需要的计算量越大。最常用的就是bigram模型。



### 维特比分词算法

有了统计语言模型，下一步要做的就是划分句子求出概率最高的分词，即对句子进行划分，最原始直接的方式，就是对句子的所有可能的分词方式进行遍历然后求出概率最高的分词组合。但是这种穷举法显而易见非常耗费性能。于是采用维特比算法简化求出最优分词的时间。

假如把每一个字当做一个节点，每两个字之间的连线看做边的话，对于句子“人生如梦”，可以构造一个如下的分词结构： 

![fenci](assets/fenci.png)

设$C_t(k)$表示在句子$S$中的第$t$个位置是词$k$的最大概率，那么可以写出状态转移方程：

$$
\begin{align}
C_t(k)
&= \max P(S_1S_2,...,S_{t-1},S_t=k)\\
&= \max_{l \in M} (\max P(S_1S_2,...,S_{t-2},S_{t-1}=l,S_t=k))\\
&= \max_{l \in M} (\max [P(S_t=k | S_1S_2,...,S_{t-2},S_{t-1}=l )P(S_1S_2,...,S_{t-2},S_{t-1}=l)]\\
&= \max_{l \in M} (\max [P(S_t=k | S_{t-1}=l )P(S_1S_2,...,S_{t-2},S_{t-1}=l)]\\
&= \max_{l \in M} (P(S_t=k | S_{t-1}=l ) \max P(S_1S_2,...,S_{t-2},S_{t-1}=l))\\
&= \max_{l \in M} P(S_t=k | S_{t-1}=l ) \cdot C_{t-1}(l) \\
\end{align}
$$

这个方程就是著名的Viterbi算法，常用在隐马尔可夫模型中。这里也可以用于最优分词算法中。





## 基于规则的分词算法

基于规则的分词也就做基于理解的分词，他通过让计算机模拟人对句子的理解，达到识别词的效果。基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。这种分词需要使用大量的语言知识和信息，模型了人对句子的理解，但是由于汉语语言的笼统性和复杂性，难以将各种语言信息组织成计算机可读取形式，因此该种分词方法还在试验阶段。





























