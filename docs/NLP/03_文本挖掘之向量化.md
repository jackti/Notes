# 文本挖掘之向量化

在文本挖掘中处理完分词以后，需要对文本进行向量化，才能成为后续分类和聚类的输入数据。

## 词袋模型

词袋模型(Bag of Words，简称BoW)假设不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重，权重与词在文本中出现的频率有关。

词袋模型首先会进行分词，在分词之后统计每个词在文本中出现的次数，从而得到该文本基于词的特征，如果将各个文本样本的这些词与对应的词频放在一起，就得到了文本的向量化。向量化完成后一般不直接使用而是使用TF-IDF进行特征的权重修正，并将特征进行标准化。

词袋模型有很大的局限性，因为它仅仅考虑了词频，没有考虑上下文的关系，因此会丢失一部分文本的语义。



## Hash Trick

词袋模型向量化的方法很好用，但是在大规模文本处理中，由于特征的维度对应分词词汇表的大小，所以维度很大，需要进行降维，最常用的文本降维方法是Hash Trick。

在Hash Trick里，会定义一个特征Hash后对应的哈希表的大小，这个哈希表的维度会远远小于词汇表的特征维度，因此可以看成是降维。具体的方法是：对应任意一个特征名，会用Hash函数找到对应哈希表的位置，然后将该特征名对应的词频统计值累加到该哈希表位置。假设哈希函数$h$使第$i$个特征哈希到位置$j$，即$h(i)=j$，则第$i$个原始特征的词频数值$\phi(i)$将累加到哈希后的第$j$个特征的词频数值$\bar{\phi}$上，即：
$$
\bar{\phi}(j)=\sum_{i\in \mathcal{J};h(i)=j} \phi(i)
$$
其中$\mathcal{J}$是原始特征的维度。

然而上面的方法有一个问题：有可能两个原始特征的哈希后位置在一起导致词频累加特征值突然变大，为了解决这一问题，出现了Hash Trick的变种Signed Hash Trick，此时除了哈希函数$h$，还引入另一个哈希函数$\xi$
$$
\xi : \mathbb{N} \rightarrow \pm1
$$
那么有：
$$
\bar{\phi}(j)=\sum_{i\in \mathcal{J};h(i)=j}\xi(i) \phi(i)
$$
这样处理的好处是哈希后的特征任然是一个无偏估计，且不会导致某些哈希位置的值过大。



## TF-IDF

TF-IDF是Term Frequency- Inverse Document Frequency的缩写，即"词频-逆文档频率"，有TF和IDF部分组成。

TF就是词频，即之前在向量化时候统计各个词的出现频率，作为文本的特征。
$$
TF= \frac{某个词在文本中出现次数}{文本总词数}
$$
IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如"to"这样的英文单词，对于文本是没有区分度的，但是在各个文本中很频繁出现。反过来如果一个词在较少的文本中出现，那么它的IDF值应该很多，如一些专有名词"Machine Learning"等，这样的词IDF值则应该很高。

对某个单词$x$的IDF计算公式如下：
$$
IDF(x)= \log \frac{N+1}{N(x)+1}+1
$$
其中$N$代表语料库中文本的总数，$N(x)$代表语料库中包含词$x$的文本总数。这里没有直接采用$N/N(x)$为了防止避免某些生僻词在预料中没有，导致公式分母为0，IDF没有意义，同时进行了平滑处理。

有了IDF的定义，就可以计算某一个词的TF-IDF值了：
$$
TF-IDF(x)=TF(x)\cdot IDF(x)
$$


## 总结

TF-IDF是文本挖掘中预处理的基本步骤，但如果在预处理中使用了Hash Trick，则一般就无法使用了TF-IDF。因为Hash Trick后就无法得到哈希后各特征的IDF值，使用TF-IDF并标准化后，就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。

















