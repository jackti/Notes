# 文本挖掘之中文预处理

在对文本做数据分析时，很多时间都会花在文本预处理上，而中文和英文的预处理流程少有不同。


## 中文文本挖掘预处理特点

（1）中文文本是没有像英文的单词空格那样隔开，因此不能直接使用最简单的空格和标点符号完成分词，所以中文文本挖掘过程中一般需要用分词算法来完成分词；

（2）中文的编码不是UTF-8，而是Unicode。所以在分词的时候需要处理编码的问题；

这两点构成了中文处理相比于英文处理的一些不同点。



## 中文文本挖掘预处理一：数据收集

在文本挖掘之前，首先需要得到文本数据，文本数据的获取方法一般有两种：①使用别人做好的语料库；②使用爬虫在网上爬取自己的语料数据。

对于第一种方法，常用的文本预料在网上有很多，可以直接下载免费使用

对于第二种方法，常用的开源工具很多，常用的有beautifulsoup和ache。ache可以用关键词或者分类算法来过滤得到所需的主题预料。



## 中文文本挖掘预处理二：除去非文本

收集的预料可能包含了一些不需要的内容，如用爬虫收集的语料数据中有很多的HTML标签等，需要去掉。少量的非文本数据可以直接用Python的正则表达式re模块删除，复杂的则需要使用beautsoup来去除。去掉这些非文本内容后，就可以正真的进行文本处理了。



## 中文文本挖掘预处理三：中文编码

由于Python2不支持Unicode处理，因此使用Python2做中文文本预处理时需要遵循的原始是：存储数据都用UTF-8，读出来进行中文相关处理时，使用GBK之类的中文编码。



## 中文文本挖掘预处理四：中外分词

常用的分词软件有很多，比较推荐结巴分词。jieba分词的`cut`函数有是三个模式：全模式、精准模式和搜索引擎模式。

（1）全模式：把句子中所有的可以生成词的词语都扫描出来，速度快但是不能解决歧义；

（2）精确模式：试图将句子最精确地切开，适合文本分析；

（3）搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合于搜索引擎分词；



## 中文文本挖掘预处理五：引入停用词

在分词完后的解析文本中会有很多无效的词，如"着","和"等等还有一些标点符号，这些在文本分析中不想引入，因此需要去掉，这些词就是停用词。常用的中文停用词表有1208个。



## 中文文本挖掘预处理六：特征处理

现在可以用scikit-learn对文本特征进行处理，常用的方法有BoW、HashTrick和TF-IDF。





## 中文文本挖掘预处理七：建立分析模型

有了每段文本的TF-IDF特征向量以后，就可以利用这些数据建立分类模型、聚类模型或者主题模型进行后续分析了。此时向量化后的文本数据和正常的结构化数据都是一样的了，分类聚类模型都可以直接使用。



































