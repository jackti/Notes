# 聚类算法之K-Means​

K-Means算法是无监督的聚类算法，实现起来比较简单且聚类效果也很不错，因此应用很广泛。

## K-Means算法原理

K-Means算法的思想很简单：对于给定的样本数据，按照样本之间的距离大小，将样本数据划分为$k$个簇。尽量让簇内的点紧密连在一起，而让簇之间的距离尽量大。

假设簇划分为$(C_1,C_2,...,C_k)$，则优化目标是最小化平方误差$E$：
$$
E=\sum_{i=1}^k\sum_{x\in C_i}||x-\mu_i||_2^2
$$
其中$u_i$是簇$C_i$的均值向量，称为质心，表达式为：
$$
\mu_i=\frac{1}{|C_i|}\sum_{x\in C_i}x
$$
这是一个NP难问题，直接求解最小化不容易实现。因此只能采用启发式的迭代方法。

![kmeans](assets/kmeans.png)

(a)初始数据集，假设簇的个数$K=2$；

(b)随机选择两个样本点作为两个簇的质心，即图中的红色点和蓝色点；

(c)计算样本中所有点到这两个质心的距离，并标记每个样本点的类别为该样本距离最小的质心的类别；

(d)对当前所标记为红色和蓝色的点分别求其新的质心，新的红色质心和蓝色质心位置发生改变

(e)-(f)重复上面(c)-(d)的过程

在实际应用K-Means算法中，会根据对数据的先验经验选择一个合适的$k$值，如果没有这样的先验知识，则可以通过交叉验证选择一个合适的$k$值。K-Means算法流程如下：

>输入：样本数据$D=\{x^{(1)},x^{(2)}, ...,x^{(m)}\}$，聚类的簇数$k$，最大迭代次数$N$
>
>输出：簇划分$C=\{C_1,C_2,...,C_k\}$
>
>(1)从数据集$D$中随机选择$k$个样本作为初始的质心向量$(\mu_1,\mu_2,...,\mu_k)$
>
>(2)对于$n=1,2,...,N$
>
>​	①将簇划分$C$初始化为$C_j=\varnothing\quad j=1,2,...,k$
>
>​	②对于$i=1,2,...,m$ 计算样本$x^{(i)}$和各个质心$\mu_j(j=1,2,...,k)$的距离
>$$
>d_{ij}=||x^{(i)}-\mu_j||_2^2 \notag
>$$
>​	   根据距离最近的均值向量确定$x^{(i)}$的簇标记$\lambda_i=\arg\min_\limits{j} d_{ij}$将样本$x^{(i)}$划入相应的簇$C_{\lambda_i}=C_{\lambda_i}\cup x^{(i)}$ 
>
>​	③对$j=1,2,...,k$对$C_j$中所有的样本点重新计算新的质心
>$$
>\mu_j = \frac{1}{|C_j|}\sum_{x\in C_j}x\notag
>$$
>​	④如果所有的$k$个质心向量都没有发生变化，则转到步骤(3)
>
>(3)输出簇划分$C=\{C_1,C_2,...,C_k\}$



## K-Means算法优化

### 初始质心优化

在标准的K-Means算法中，初始的$k$个质心位置对最后的聚类结果和运行时间都有很大的影响。因此需要选择合适的$k$个质心。在原始的K-Means算法中完全随机的选择，有可能导致算法收敛很慢。

K-Means算法就是对K-Means随机初始化质心的优化，其基本思想就是：初始的质心互相距离应该尽可能的远。

> (1)从输入的数据点集合随机选择一个点作为第一个聚类中心$\mu_1$
>
> (2)对于数据集中的每一个点$x$，计算它与已选择的聚类中心**最近聚类中心**的距离
> $$
> D(x)=\arg \min \sum_{r=1}^{K_{selected}}||x-\mu_r||_2^2 \notag
> $$
> (3)选择一个新的数据点作为新的聚类中心，选择的原则为：$D(x)$较大的点，被选择为聚类中心的概率较大
>
> (4)重复步骤(2)和(3)直到选择出$k$个聚类质心
>
> (5)利用这$k$个质心作为初始化质心运行标准的K-Means算法



### 距离计算优化

在标准的K-Means算法中，每轮迭代需要计算所有的样本点到所有质心的距离，比较耗时。elkan K-Means算法从这里进行改进，减少了不必要的距离计算呢。

elkan K-Means利用了两边之和大于等于第三边，以及两边之差小于等于第三边的性质，来减少距离的计算。

假设一个样本点$x$和两个质心$\mu_{j1},\mu_{j2}$，利用三角形性质有：

(1)首先计算两质心的距离$D(j1,j2)$，如果计算发现$2D(x,j1)\le D(j1,j2)$，那么可以有$D(x,j1)\le D(x,j2)$，就可以不用在计算$D(x,j2)$；

(2)$D(x,j2)\ge \max\{ 0, D(x,j1)-D(j1,j2) \}$



### 大样本优化

在标准的K-Means算法中，要计算所有的样本到所有质心的距离。如果样本量很大，即使计算加上elkan K-Means优化也非常耗时。于是就出现了 Mini Batch K-Means算法。

在 Mini Batch K-Means算法中，选择一个合适的批样本大小batch size（一般通过无放回的随机采样）。仅仅使用这batch size个样本来做K-Means聚类。这样就可以避免样本量太大的计算问题，算法收敛速度加快。但同时聚类的精确度也会有所降低，为了增加算法的准确性，一般会多跑几次Mini Batch K-Means算法，用不同的随机采样来得到聚类簇，选择其中最优的聚类簇。









