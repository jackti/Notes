# 聚类算法之谱聚类

谱聚类(Spectral Clustering)是广泛应用的聚类算法，比起传统的K-Means算法，谱聚类对数据分布的适应性强，聚类效果也很好且聚类的计算量小很多。在处理实际问题时，谱聚类可以作为首先考虑的算法之一。

谱聚类是从图论中演化过来的，主要思想是把所有的数据看做空间中点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重较高。通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重尽可能的低，而子图的边权重和尽可能的高，从而达到聚类的目的。



## 谱聚类基础之一：无向权重图

对于一个图$G$，一般用点集合$V$和边集合$E$来描述，记作$G(V,E)$。其中$V$包含了数据集中所有的点$(v_1,v_2,...,v_n)$，对于$V$中任意两个点，可以有边连接，也可以没有边连接，定义点$v_i$和$v_j$之间的权重为$w_{ij}$。

对于有边连接的两个点$v_i$和$v_j$，权重$w_{ij}>0$；对于没有边连接的两个点$v_i$和$v_j$，权重$w_{ij}=0$。

利用所有点之间的权重图，可以得到一个图的邻接矩阵$W$，是一个$n\times n$的矩阵，第$i$行的第$j$个值对应了权重$w_{ij}$
$$
W=\left(
\begin{matrix}
w_{11} & w_{12}  &...&w_{1n} \\
w_{21} & w_{22}  &...&w_{2n} \\
\vdots &\vdots  &\vdots&\vdots \\
w_{n1} & w_{n2}  &...&w_{nn} \\
\end{matrix}
\right)\notag
$$
对于图中的任意一个点$v_i$，它的度$d_i$定义为所有和它相连的边权重之和，即
$$
d_i=\sum_{j=1}^n w_{ij}
$$
利用每个点度的定义，可以得到一个$n\times n$的对角矩阵$D$，只有对角线有值，对应第$i$行的第$i$个点度数，即
$$
D=\left(
\begin{matrix}
d_1 &0  &...&0 \\
0 &d_2  &...&0 \\
\vdots &\vdots  &\vdots&\vdots \\
0 & 0  &...&d_n \\
\end{matrix}
\right)\notag
$$
此外还有两个符号定义：$|A|$表示集合$A$中点的个数，$vol(A):=\sum_\limits{i\in A} d_i$表示集合$A$中所有点的度之和。





## 谱聚类基础之二：相似矩阵

邻接矩阵$W$由任意两点之间的权重值$w_{ij}$组成。而$w_{ij}$的取值一般满足距离较远的两个点之间的边权重较低，距离较近的两个点之间的边权重较高。通常构建邻接矩阵的$W$的方法有三类：$\epsilon-$近邻法、$k$近邻法和全连接法。



**$\epsilon-$近邻法**

首先利用欧式距离$s_{ij}$度量任意两个点$x_i$和$x_j$的距离，设置一个距离阈值$\epsilon$，然后根据$s_{ij}$和$\epsilon$的大小关系定义邻接矩阵$W$如下：
$$
W_{ij}=
\left\{
\begin{array}{}
0  \qquad s_{ij}\gt \epsilon \\
\epsilon  \qquad s_{ij}\le \epsilon
\end{array}
    
\right.
$$
从式(2)可以看出两点之间的权重不是$\epsilon$就是0，没有其他的信息了。距离远近的度量不是很精确，因此使用较少。



**$k$近邻法**

利用KNN算法遍历所有的样本点，取每个样本最近的$k$个点作为近邻，只有和样本距离最近的$k$个点之间的$w_{ij}\gt 0$但是这样的方法会导致邻接矩阵$W$非对称。为了解决这个问题，通常采用下面两种方法之一：

第一种$k$近邻是只要一点在另一点的$k$近邻中，则保留$s_{ij}$
$$
W_{ij}=W_{ji}=\left\{
    \begin{matrix}
    0 & x^{(i)}\notin KNN(x^{(j)}) \; and \; x^{(j)}\notin KNN(x^{(i)})  \\
    \exp(-\frac{||x^{(i)}-x^{(j)}||^2}{2\sigma^2}) & x^{(i)}\in KNN(x^{(j)}) \; or \; x^{(j)}\in KNN(x^{(i)}) 
    \end{matrix}
\right.
$$
第二种$k$近邻是必须两点互为$k$近邻，才能保留$s_{ij}$
$$
W_{ij}=W_{ji}=\left\{
    \begin{matrix}
    0 & x^{(i)}\notin KNN(x^{(j)}) \; or \; x^{(j)}\notin KNN(x^{(i)})  \\
    \exp(-\frac{||x^{(i)}-x^{(j)}||^2}{2\sigma^2}) & x^{(i)}\in KNN(x^{(j)}) \; and \; x^{(j)}\in KNN(x^{(i)}) 
    \end{matrix}
\right.
$$


**全连接**

全连接的方法中所有的点之间的权重都大于0，可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和Sigmoid核函数。常用的是高斯核函数RBF：
$$
W_{ij}=\exp(-\frac{||x^{(i)}-x^{(j)}||_2^2}{2\sigma^2})
$$
在实际应用中，使用全连接的方法来建立邻接矩阵最普遍。





## 谱聚类基础之三：拉普拉斯矩阵

定义拉普拉斯矩阵$L=D-W$，其中$D$为度矩阵，是一个对角矩阵，$W$是邻接矩阵。

拉普拉斯矩阵具有的性质：

（1）拉普拉斯矩阵是对称的，因为$W$和$D$都是对称矩阵；

（2）由于拉普拉斯矩阵是对称矩阵，则它所有特征值都是实数；

（3）对于任意向量$f$，都有
$$
\begin{align}
f^T L f
&=f^T(D-W)f\\
&=f^T Df -f^TWf\\
&=\sum_{i=1}^n d_i f_i^2-\sum_{i=1,j=1}^nw_{ij}f_if_j\\
&=\frac{1}{2}(\sum_{i=1}^n d_i f_i^2-2\sum_{i=1,j=1}^nw_{ij}f_if_j+\sum_{j=1}^n d_j f_j^2)\\
&=\frac{1}{2}\sum_{i=1,j=1}^n w_{ij}(f_i-f_j)^2
\end{align}
$$
（4）拉普拉斯矩阵是正定的，且对应的$n$个实数特征值都大于等于0，即$0=\lambda_1\le\lambda_2\le...\le\lambda_n$，且最小的特征值为0





## 谱聚类基础之四：无向图切图

对于无向图$G(V,E)$，切分为互相没有连接的$k$个子图，每个子图的集合为$A_1,A_2,...,A_k$，满足$A_i\cap A_j=\empty$，且$A_1\cup A_2\cup...\cup A_k=V$。

对于任意两个子图点的集合$A,B\in V,A\cap B=\empty$，于是定义$A$和$B$之间的切图权重为：
$$
W(A,B)=\sum_{i\in A,j\in B}w_{ij}
$$
对于$k$个子图点的集合：$A_1,A_2,...,A_k$，定义切图cut为：
$$
cut(A_1,A_2,...,A_k)=\frac{1}{2}\sum_{i=1}^k W(A_i,\bar{A}_i)
$$
其中$\bar{A}_i$为$A_i$的补集。

由于切图的目的是使得每个子图内部结构相似，这个相似表现为连边的权重平均都较大，且互相连接，而每个子图间则尽量没有边相连，或者连边的权重很低，所以优化目标可以表述为：
$$
\min cut(A_1,A_2,...,A_k)=\min\frac{1}{2}\sum_{i=1}^k W(A_i,\bar{A}_i)
$$
但是很容易发现这种极小化的切图存在问题，如下图

![cut](assets/cut.jpg)

为了最小化$cut(A_1,A_2,...,A_k)$，需要选择C和H之间进行cut，但是却不是最优的切图。



## 谱聚类之切图聚类

为了避免最小切图导致的切图效果不佳，需要对每个子图的规模做出限制。一般来说，有两种切图方式，风别是RatioCut和Ncut。

### RatioCut切图

对每个切图，不仅考虑最小化$cut(A_1,A_2,...,A_k)$，同时还要考虑最大化每个子图的个数即：
$$
RatioCut(A_1,A_2,...,A_k)=\frac{1}{2}\sum_{i=1}^k \frac{W(A_i,\bar{A}_i)}{|A_i|}
$$
为了最小化这个$RationCut(\cdot)$函数，引入指示向量$\{h_1,h_2,...,h_j,...,h_k\},j=1,2,...,k$，对于任意向量$h_j\in \mathbb{R}^n$，这里$n$是样本数
$$
h_{ji}=\left\{
    \begin{aligned}
    &0 & v_i \notin A_j\\
    &\frac{1}{\sqrt{|A_j|}}& v_i \in A_j
    \end{aligned}
\right.
$$
现在考虑$h_i^T L h_i$有：
$$
\begin{align}
h_i^T L h_i
&=\frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2\\
&=\frac{1}{2}(\sum_{m\in A_i,n\notin A_i}w_{mn}(\frac{1}{\sqrt{|A_{i}|}}-0)^2
+\sum_{m\notin A_i,n\in A_i}w_{mn}(0-\frac{1}{\sqrt{|A_{i}|}})^2)\\
&=\frac{1}{2}(\sum_{m\in A_i,n\notin A_i}w_{mn}\frac{1}{|A_i|} + \sum_{m\notin A_i,n\in A_i}w_{mn}\frac{1}{|A_i|})\\
&=\frac{1}{2}(cut(A_i,\bar{A}_i)\frac{1}{|A_i|}+cut(\bar{A}_i,A_i)\frac{1}{|A_i|})\\
&=\frac{cut(A_i,\bar{A}_i)}{|A_i|}\\
&=RationCut(A_i,\bar{A}_i)
\end{align}
$$
公式(15)利用了拉普拉斯矩阵的性质；

公式(15)~(17)主要利用$m,n$是否属于$A_i$有四中情况：

①$m\in A_i \; n\in A_i$ 这时候$h_{im}=h_{in}=\frac{1}{\sqrt{|A_j|}}$

②$m\notin A_i \; n\in A_i$ 这时候$h_{im}=0\quad h_{in}=\frac{1}{\sqrt{|A_j|}}$

③$m\in A_i \; n\notin A_i$ 这时候$h_{im}=\frac{1}{\sqrt{|A_j|}} \quad h_{in}=0$

④$m\notin A_i \; n\notin A_i$ 这时候$h_{im}= h_{in}=0$

公式(17)~(19)利用$cut(\cdot)$的定义，且$cut(A_i,\bar{A}_i)=cut(\bar{A}_i,A_i)$。

所以最终式(14)可以等价于：
$$
RatioCut(A_1,A_2,...,A_k)=\sum_{i=1}^k h_i^T L h_i = \sum_{i=1}^k(H^TL H)_{ii}=tr(H^TLH)
$$
其中$tr(H^TLH)$为矩阵的迹。优化目标可以写成：
$$
\arg \min_{H} tr(H^T L H) \quad s.t. \quad H^TH=I
$$
由于$H$矩阵里面的每一个指示向量都是$n$维的，向量中每个变量的取值为0或者$\frac{1}{\sqrt{|A_j|}}$，就有$2^n$中取值，有$k$个子图就有$k$个指示向量，共计$k2^n$中可能取值的$H$。这是一个NP难的问题。

但是观察发现$tr(H^TLH)$中的每一个优化子目标$h_i^T L h_i$，其中$h$是单位正交基，$L$是对称矩阵，此时$h_i^T L h_i$的最大值为$L$的最大特征值，最小值是$L$的最小特征值。

对于$h_i^T L h_i$，目标是找到最小的$L$的特征值，而$tr(H^TLH)=\sum_{i=1}^k h_i^T L h_i $的优化目标就是找到$k$个最小的特征值，一般而言$k$是远远小于$n$，从而近似解决了这个NP难的问题。

通过找到$L$的最小$k$个特征值，可以得到对应的$k$个特征向量，这$k$个特征向量组成一个$n\times k$的矩阵，即$H$。一般需要对$H$矩阵按行做标准化，即
$$
h_{ij}^*=\frac{h_{ij}}{\sqrt{\sum_{t=1}^k h_{it}^2}}
$$
由于在使用维度规约的时候损失了少量信息，得到的优化后指示向量$h$对应的$H$现在不能完全指示各样本所属类别，因此一般在得到$n\times k$维度的矩阵$H$后还需要对每一行进行依次传统的聚类，如K-Means聚类等。



### NCut切图

NCut切图和RatioCut切图类似，只是将RatioCut中分母$|A_i|$换成了$vol(A_i)$。由于子图样本的个数多并不一定权重就大，所以基于权重的切图更加符合目标。因此一般而言NCut切图是优于RatioCut切图的。
$$
NCut(A_1,A_2,...,A_k)=\frac{1}{2}\sum_{i=1}^k\frac{W(A_i,\bar{A}_i)}{vol(A_i)}
$$
NCut切图对指示向量$h$做了改进，定义如下：
$$
h_{ji}=\left\{
    \begin{aligned}
    &0 & v_i \notin A_j\\
    &\frac{1}{\sqrt{vol(A_j)}}& v_i \in A_j
    \end{aligned}
\right.
$$
对于$h_i^T L h_i$有：
$$
\begin{align}
h_i^T L h_i
&=\frac{1}{2}\sum_{m=1}\sum_{n=1}w_{mn}(h_{im}-h_{in})^2\\
&=\frac{1}{2}(\sum_{m\in A_i,n\notin A_i}w_{mn}(\frac{1}{\sqrt{vol(A_{i})}}-0)^2
+\sum_{m\notin A_i,n\in A_i}w_{mn}(0-\frac{1}{\sqrt{vol(A_{i})}})^2)\\
&=\frac{1}{2}(\sum_{m\in A_i,n\notin A_i}w_{mn}\frac{1}{vol(A_{i})} + \sum_{m\notin A_i,n\in A_i}w_{mn}\frac{1}{vol(A_{i})})\\
&=\frac{1}{2}(cut(A_i,\bar{A}_i)\frac{1}{vol(A_{i})}+cut(\bar{A}_i,A_i)\frac{1}{vol(A_{i})})\\
&=\frac{cut(A_i,\bar{A}_i)}{vol(A_{i})}\\
&=RationCut(A_i,\bar{A}_i)
\end{align}
$$
推导方式和RatioCut完全一致，优化目标为：
$$
NCut(A_1,A_2,...A_k) = \sum\limits_{i=1}^{k}h_i^TLh_i = \sum\limits_{i=1}^{k}(H^TLH)_{ii} = tr(H^TLH)
$$
此时$H^TH\ne I$，但是$H^TDH=I$，推导如下：
$$
h_i^TDh_i = \sum\limits_{j=1}^{n}h_{ij}^2d_j =\frac{1}{vol(A_i)}\sum\limits_{v_j \in A_i}w_{v_j} = \frac{1}{vol(A_i)}vol(A_i) =1
$$
最终的优化目标为：
$$
\arg \min_{H} tr(H^T L H) \quad s.t. \quad H^TDH=I
$$
此时$H$中的指示向量$h$并不是标准正交基，所以需要将$H$进行小小的转化，令$H=D^{-1/2}F$，则式(33)转化为：
$$
\arg \min_{F} tr(F^T D^{-1/2}LD^{-1/2}F) \quad s.t. \quad F^TF=I
$$
式(34)和RatioCut基本一致了，只是中间的$L$变成了$D^{-1/2}LD^{-1/2}$，可以求出$D^{-1/2}LD^{-1/2}$的最小的前$k$个特征值，然后求出对应的特征向量，并标准化，得到最后的特征矩阵$F$，最后对$F$进行一次传统的聚类即可。





## 谱聚类总结

谱聚类算法的主要优点有：

（1）谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。

（2）由于使用了降维，因此在处理高维数据聚类时的复杂度比传统聚类算法好。

谱聚类算法的主要缺点有：

（1）如果最终聚类的维度非常高，则由于降维的幅度不够，谱聚类的运行速度和最后的聚类效果均不好。

 （2）聚类效果依赖于相似矩阵，不同的相似矩阵得到的最终聚类效果可能很不同。









