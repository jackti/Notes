# 降维算法之LLE

局部线性嵌入(Locally Linear Embedding,简称LLE)也是非常重要的降维算法。和传统的PCA、LDA等关注样本方差的降维方法相比，LLE关注降维时保持样本局部的线性特征，由于LLE在降维时保持了样本的局部特征，广泛地用于图像识别、高维数据可视化等领域。



## LLE算法思想

LLE假设数据在较小的局部是线性的，即某一个数据可以有它领域中几个样本来线性表示。如一个样本$x_1$，在它的高维邻域里用$k-$近邻思想找到和它最近的三个样本$x_2,x_3,x_4$，然后假设$x_1$可以用$x_2,x_3,x_4$来表示。
$$
x_1=w_{12}x_2+w_{13}x_3+w_{14}x_4
$$
其中$w_{12},w_{13},w_{14}$为权重系数。通过LLE降维后，假设$x_1$在低维空间对应的投影$x_1^{'}$和$x_2,x_3,x_4$对应的投影$x_2^{'},x_3^{'},x_4^{'}$也尽量保持同样的线性关系，即
$$
x_1^{'} \approx w_{12}x_2^{'}+w_{13}x_3^{'}+w_{14}x_4^{'}
$$
也就是说投影前后线性关系的权重系数$w_{12},w_{13},w_{14}$尽量不变或最小改变。由于线性关系只在样本的附近起作用，离样本远的点对局部的线性关系没有影响，因此降维的复杂度降低了很多。



## LLE算法推导

对于LLE算法，首先需要确定邻域$k$的大小，即需要多少个邻域样本来线性表示某个样本。假设这个值为$k$，可以通过和knn一样的思想通过距离度量比如欧氏距离来选择某样本的$k$个最近邻。

假设现有$m$个$n$维样本$\{x_1,x_2,...,x_m\}$可以用均方差作为回归问题的损失函数，即：
$$
J(w)=\sum_{i=1}^m ||x_i-\sum_{j=1}^kw_{ij}x_j||_2^2
$$
一般会对权重系数$w_{ij}$做归一化的限制，即权重系数需满足
$$
\sum_{j=1}^k w_{ij}=1
$$
对于不在样本$x_i$邻域内的样本$x_j$，令其$w_{ij}=0$

对于式(3)，我们将其矩阵化：
$$
\begin{aligned}
J(W)
&=\sum_{i=1}^m ||x_i-\sum_{j=1}^kw_{ij}x_j||_2^2\\
&=\sum_{i=1}^m ||\sum_{j=1}^k w_{ij}\cdot x_i-\sum_{j=1}^k w_{ij}x_j||\\
&=\sum_{i=1}^m ||\sum_{j=1}^k w_{ij}(x_i-x_j)||_2^2\\
&=\sum_{i=1}^m W_i^T (x_i-x_j)^T(x_i-x_j)W_i
\end{aligned}
$$
其中$W_i=(w_{i1},w_{i2},...,w_{ik})^T$。令矩阵$Z_i=(x_i-x_j)^T (x_i-x_j)$则上式进一步简化为
$$
\begin{aligned}
J(W)=\sum_{i=1}^m W_i^T Z_i W_i\\
s.t. \sum_{i=1}^k w_{ij}=W_i^T 1_k=1
\end{aligned}
$$
利用拉格朗日乘子法合为一个优化目标：
$$
L(W)=\sum_{i=1}^mW_i^T Z_i W_i+\lambda (W_i^T 1_k-1)
$$
对式(5)求导令其为0，可以得到：
$$
2Z_iW_i +\lambda 1_k=0
$$
最后得到
$$
W_i=\lambda^{'}Z_i^{-1}1_k
$$
其中$\lambda^{'}=-\frac{1}{2}\lambda$为一个常数。利用$W_i^T 1_k=1$对$W_i$归一化，那么最终权重系数$W_i$为：
$$
W_i=\frac{Z_i^{-1}1_k}{1_k^T Z_i^{-1}1_k}
$$




通过式(5)就可以得到高维的权重系数，如果现有$n$维样本集$\{x_1,x_2,...,x_m\}$在低维的$d$维度投影为$\{y_1,y_2,...,y_m\}$由于希望保持线性关系，对应的均方差损失函数最小，即最小化损失函数$J(Y)$如下
$$
J(y)=\sum_{i=1}^m ||y_i - \sum_{j=1}^m w_{ij}y_j||_2^2
$$
式(9)和式(3)的损失函数几乎相同。唯一区别是在式(3)中高维数据已知，目标是求最小值对应的权重系数$W$；而在式(9)中系数权重$W$已知，求对应的低维数据。

为了得到标准化的数据，一般会加上如下约束条件：
$$
\sum_{i=1}^m y_i=0; \qquad \frac{1}{m}\sum_{i=1}^m y_i y_i^T = I
$$
对损失函数(9)矩阵化：
$$
\begin{align}
J(Y)
&=\sum_{i=1}^m ||y_i-\sum_{j=1}^k w_{ij}y_j||_2^2\\
&=\sum_{i=1}^m ||Y I_i-YW_i||_2^2\\
&=tr(Y^T(I-W)^T(I-W)Y)
\end{align}
$$
令$M=(I-W)^T(I-W)$，则优化函数转变为$J(Y)=tr(Y^T M Y)$，约束条件为$Y^TY=m I$

利用拉格朗日乘子法有：
$$
L(Y)=tr(Y^T MY)+\lambda(Y^T Y- m I)
$$
对$Y$求导并令其为0，得到$2 M Y+2\lambda Y=0$，即$MY=\lambda Y$，要得到最小的$d$维数据集，只用求出矩阵$M$最小的$d$个特征值的$d$个特征向量组成的矩阵$Y=(y_1,y_2,...,y_d)$

一般地，由于$M$的最小特征值为0不能反应数据特征，此时对应的特征向量全为1。通常会选择$M$的第2个到第$d+1$个最小的特征值对应的特征向量$M=(y_2,y_3,...,y_{d+1})$来得到最终的$Y$。

LLE算法过程是：

> 输入：样本集$D=\{x_1,x_2,...,x_m\}$，近邻数$k$，降维的维数$d$
>
> 输出：低维样本矩阵$D^{'}$
>
> (1)对$i=1,2,...,m$ 按欧氏距离作为度量，计算和$x_i$最近的$k$个最近邻$(x_{i1},x_{i2},...,x_{ik})$
>
> (2)对$i=1,2,...,m$ 求出局部协方差矩阵$Z_i=(x_i-x_j)^T(x_i-x_j)$，并求出对应的权重系数向量
> $$
> W_i=\frac{Z_i^{-1}1_k}{1_k^T Z_{i}^{-1}1_k} \notag
> $$
> (3)有权重系数向量$W_i$组成权重系数矩阵$W$，计算矩阵$W=(I-W)^T(I-W)$
> (4)计算矩阵$M$的前$d+1$特特征值，计算这$d+1$个特征值对应的特征向量$\{y_1,y_2,...,y_{d+1}\}$，由第二个特征向量到第$d+1$个特征向量所张成的矩阵即为输出低维样本集矩阵$D^{'}=\{y_2,y_3,...,y_{d+1}\}$
>
>





## LLE算法总结

LLE是广泛使用的图形图像降维方法，它实现简单，但是对数据的流形分布特征有严格的要求。比如不能是闭合流形，不能是稀疏的数据集，不能是分布不均匀的数据集等等，这限制了它的应用。下面总结下LLE算法的优缺点。

LLE算法的主要优点有：

（1）可以学习任意维的局部线性的低维流形

（2）算法归结为稀疏矩阵特征分解，计算复杂度相对较小，实现容易。



LLE算法的主要缺点有：

（1）算法所学习的流形只能是不闭合的，且样本集是稠密均匀的。

（2）算法对最近邻样本数的选择敏感，不同的最近邻数对最后的降维结果有很大影响。

































