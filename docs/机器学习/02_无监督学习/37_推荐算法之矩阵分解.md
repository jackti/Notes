# 推荐算法之矩阵分解

在推荐系统中，有很多的用户和物品，但只有很少的用户对很少的物品进行过评分，现在希望预测目标用户对其他未频分的物品的评分，进而将评分高的物品推荐给目标用户。将$m$个用户和$n$个物品对应的频分看成一个矩阵$M$，可以通过矩阵分解来解决推荐问题。

|         | 物品1 | 物品2 | 物品3 | ...... | 物品n |
| ------- | ----- | ----- | ----- | ------ | ----- |
| 用户1   | 5     | 2     |       |        |       |
| 用户1   |       | 2     | 1     |        | 4     |
| 用户2   | 3     |       |       |        | 1     |
| ....... |       |       |       |        |       |
| 用户1   | 4     |       | 2     |        | 5     |



## 传统奇异值分解SVD用于推荐

提到矩阵分解，首先想到的就是奇异值分解SVD。此时可以将这个用户物品矩阵$M\in \mathbb{R}^{m\times n}$，并且通过选择部分较大的一些奇异值来同时降维，即：
$$
M_{m\times n}=U_{m\times k}\Sigma_{k\times k} V_{k\times n}^T
$$
其中$k$是矩阵$M$中较大的部分奇异值的个数，一般会远远小于用户数和物品数。如果要预测用户$i$对第$j$个物品的评分$m_{ij}$则只需计算$u_i^T \Sigma v_j$。通过这种方法，可以将评分表里面所有没有评分的位置得到一个预测评分，最后将最高的若干评分对应的物品推荐给用户。

然而SVD分解要求矩阵是稠密的，即矩阵的所有位置不能有空白，此时的评分矩阵$M$是无法直接用SVD分解的。通过为了满足矩阵稠密的要求，传统SVD采用的方法是对评分矩阵中的缺失值进行简单的补全，比如用全局平均值或者用户物品平均值等，得到补全后的矩阵，就可以直接用SVD分解并降维。

即使有了上面的补全策略，传统的SVD在推荐算法上也还是很难使用，因为用户和物品一般都是超级大，做SVD分解是非常耗时的。



## FunkSVD算法用于推荐

FunkSVD将矩阵$M$分解为$P$和$Q$，求解方式采用了线性回归的思想，让用户的评分和用矩阵乘积得到的评分残差尽可能的小，即采用均方差作为损失函数，来寻找最终的$P$和$Q$。
$$
M_{m\times n}=P^T_{m\times k}Q_{k\times n}
$$
对于某一个用户评分$m_{ij}$，如果用FunkSVD进行矩阵分解，则对应的表示为$q_j^T p_i$，采用均方差作为损失函数，则期望$(m_{ij}-q^T_j p_i)^2$尽可能地小，如果考虑所有用户和物品的组合，则期望最小化下式：
$$
J(p,q)=\sum_{i,j}(m_{ij}-q_j^T p_i)
$$
在实际应用中，为了防止过拟合，往往会加入一个$L_2$的正则化项，因此最终FunkSVD的优化目标变为：

$$
\mathop {\arg\min}_{p_i,q_j}\sum_{i,j}(m_{ij}-q^T_j p_i)^2+\lambda(||p_i||_2^2+||q_j||_2^2)
$$

其中$\lambda$为正则化系数，需要调参。对于式(4)的优化问题，一般通过梯度下降法来进行优化。

$$
\begin{align}
\frac{\partial J}{\partial p_i}=-2(m_{ij}-q^T_j p_i)q_j+2\lambda p_i\\
\frac{\partial J}{\partial q_j}=-2(m_{ij}-q^T_j p_i)p_i+2\lambda q_j\\
\end{align}
$$

则在梯度下降迭代法中，$p_i,q_j$的迭代公式为：

$$
\begin{align}
p_i = p_i + \alpha((m_{ij}-q_j^T p_i)q_j-\lambda p_i)\\
q_j = q_j + \alpha((m_{ij}-q_j^T p_i)p_i-\lambda q_j)
\end{align}
$$

通过迭代最终可以得到$P$和$Q$，进而用于推荐。





## BiasSVD算法用于推荐

在FunkSVD算法之后出现了很多FunkSVD的改进算法，其中BiasSVD算法就是其中较为成功的一种。BiasSVD算法假设评分系统包括三部分的偏置因素：①一些和用户物品的评分因素；②用户有一些和物品无关的评分因素，称为用户偏置项；③物品有一些和用户无关的评分因素，称为物品偏置项。

假设评分系统平均分为$\mu$，第$i$个用户的用户偏置项为$b_i$，第$j$个物品的物品偏置项为$b_j$，则加入偏置项以后的优化目标$J(p,q)$为：

$$
\mathop {\arg\min}_{p_i,q_j}\sum_{i,j}(m_{ij}-\mu-b_i -b_j -q_j^T p_i)+\lambda(||p_i||_2^2+||q_j||_2^2+||b_i||_2^2+||b_j||_2^2)
$$

式(9)也可采用梯度下降法求解。不同的是此时我们多增加了两个偏置项$b_i,b_j$，$p_i,q_j$的迭代公式和FunkSVD类似，只是每一步的梯度导数稍有不同而已。

$$
\begin{align}
b_i = b_i +\alpha(m_{ij}-\mu - b_i - b_j -q_j^T p_i -\lambda b_i)\\
b_j = b_j +\alpha(m_{ij}-\mu - b_i - b_j -q_j^T p_i -\lambda b_j)\\
\end{align}
$$

通过迭代最终可以得到$P$和$Q$，进而用于推荐。BiasSVD增加了一些额外的因素考虑，在某些场合会比FunkSVD表现更好。





## SVD++算法用户推荐

SVD++算法在BiasSVD的基础上进一步做了增强。对于一个用户$i$，它提供了隐式反馈的物品集合定位为$N(i)$，这个用户对某个物品$j$对应的隐式反馈修正的评分值为$c_{ij}$，那么该用户所有的评分修正值为$\sum_{s\in N(i)}c_{sj}$。一般将它表示为用$q_j^T y_s$形式，则加入隐式反馈项后的优化目标函数$J(p,q)$是：

$$
\mathop {arg\;min}_{p_i,q_j}\;\sum\limits_{i,j}(m_{ij}-\mu-b_i-b_j-q_j^Tp_i - q_j^T|N(i)|^{-1/2}\sum\limits_{s \in N(i)}y_{s})^2\\+ \lambda(||p_i||_2^2 + ||q_j||_2^2 + ||b_i||_2^2 + ||b_j||_2^2 + \sum\limits_{s \in N(i)}||y_{s}||_2^2)
$$

其中引入$|N(i)|^{-1/2}$是为了消除不同$|N(i)|$个数引起的差异。





## 矩阵分解算法总结

FunkSVD将矩阵分解用于推荐方法推到了新的高度，在实际应用中使用也是非常广泛。

对于矩阵分解用于推荐方法本身来说，它容易编程实现，实现复杂度低，预测效果也好，同时还能保持扩展性。

矩阵分解方法有时候解释性还是没有基于概率的逻辑回归之类的推荐算法好，不过这也不影响它的流形程度。

小的推荐系统用矩阵分解应该是一个不错的选择。大型的话，则矩阵分解比起现在的深度学习的一些方法不占优势。