# 集成学习算法之GBDT

提升树(Boosting tree)是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。GBDT(Gradient Boosting Decision Tree)是Boosting家族中的一类重要算法，GBDT有很多简称，如GBT(Gradient Boosting Tree)，GTB(Gradient Tree Boosting)，GBRT(Gradient Boosting Regression Tree)，MART(Multiple Additive Regression Tree)这些都是指的同一种算法。



## GBDT算法模型

GBDT是Boosting集成学习算法的一种，但是和Adaboost有所差异。在Adaboost模型中利用前一轮弱迭代学习器的误差率来更新训练集的权重，一轮轮的反复迭代。GBDT也是迭代，使用前向分步算法，但是弱学习器限定了只是用CART模型，同时迭代思路也和Adaboost不同。

在GBDT的迭代中，假设前一轮迭代得到的强学习器是$f_{t-1}(x)$，损失函数是$L(y,f_{t-1}(x))$。本轮迭代的目标是找一个CART回归树模型的弱学习器$h_t(x)$，让本轮的损失函数$L(y,f_t(x))=L(y,f_{t-1}(x)+h_t(x))$最小。即本轮迭代找到决策树，要让样本的损失尽量变得更小。

GBDT的思想有一个通俗的例子：假如某人年龄30岁，首先用20岁取拟合，发现有10岁的损失。这时用6岁取拟合剩下的损失，发现差距还有4岁，第三轮用3岁拟合剩下的差距。那么最终的差距就只有1岁了。如果继续迭代下去，每一轮迭代拟合误差都会减小。

最终模型可以描述为

$$
f_m(x)=\sum_{m=1}^M T(x;\theta_m)
$$

其中$T(x;\theta)$表示决策树，$\theta$为决策树参数，$M$为树的个数。采用前向分步算法，第$m$步的模型为：

$$
f_m(x)=f_{m-1}(x)+T(x;\theta)
$$

通过经验风险极小化确定下一颗树的参数$\theta_m$

$$
\theta_m^*=\arg \min_{\theta_m}\sum_{i=1}^N L(y^{(i)},f_{m-1}(x^{(i)})+T(x^{(i)};\theta_m))
$$

当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，往往每一步优化并不那么容易。我们正真关注点：①损失函数能够不断减小；②损失函数尽可能快地减小。针对这一问题，可以利用损失函数的负梯度在当前模型的值

$$
-\left[\frac{\partial L(y,f(x^{(i)}))}{\partial f(x^{(i)})}      \right]_{f(x)=f_{m-1}(x)}
$$


## GBDT回归算法

梯度提升算法：

> 输入：训练数据集$\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}) ,...,(x^{(N)},y^{(N)})  \}, x^{(i)}\in \mathbb{R}^n,y^{(i)}\in \mathbb{R}$
>
> 输出：回归树$f(x)$
>
> (1)初始化
>
> $$
> f_0(x)=\arg \min\limits{c}\sum{i=1}^N L(y^{(i)},c) \notag
> $$

> (2)对$m=1,2,...,M$
>
> ​	①对$i=1,2,...,N$，计算
>
> $$
> r_{mi}=-\left[ \frac{\partial L(y^{(i)},f(x^{(i)}))}{\partial f(x^{(i)})}\right]_{f(x)=f_{m-1}(x)}  \notag
> $$
>
> ​	②使用$(x^{(i)},r_{mi})$拟合一个回归树，得到第$m$课树的叶节点区域$R_{mj},j=1,2,...,J$
>
> ​	③对叶子区域$j=1,2,..,J$计算最佳拟合值
>
> $$
> c_{mj}=\arg \min_{c}\sum_{x^{(i)}\in R_{mj}}L(y^{(i)},f_{m-1}(x^{(i)})+c)\notag
> $$
>
>
> ​	④更新$f_m(x)=f_{m-1}(x)+\sum_{j=1}^J c_{mj}I(x\in R_{mj})$
>
> (3)得到回归树
>
> $$
> f_M(x)=\sum_{m=1}^M \sum_{j=1}^Jc_{mj}I(x\in R_{mj})\notag
> $$
>



## GBDT分类算法

GBDT的分类算法思想和GBDT回归算法是一样的，但是由于样本输出不是连续值，而是离散的类别，导致无法直接从输出类别去拟合类别输出的误差。

为了解决这个问题，采用的方法有两个：①使用指数损失函数，此时GBDT退化为Adaboost算法；②使用类似于逻辑回归的对数似然函数的方法，即用类别的预测概率值和真实概率值来拟合损失。





## GBDT总结

GBDT主要的优点有：

（1） 可以灵活处理各种类型的数据，包括连续值和离散值；

（2） 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。

（3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。



GBDT的主要缺点有：由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。

























