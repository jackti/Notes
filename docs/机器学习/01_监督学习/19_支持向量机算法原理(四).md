# 支持向量机算法原理(四)

支持向量机的学习问题可以形式化为求解凸二次规划问题。这样的凸二次规划问题具有全局最优解，且有许多最优化算法可以用于这一问题的求解。但是当训练样本容量很大时，这些算法往往变得非常低效，以至于无法使用。SMO算法就是为了解决这一问题，高效地实现了支持向量机的学习算法。

## SVM优化目标函数

目标函数：

$$
\begin{aligned}
\min_{\alpha} \quad &\frac{1}{2}\sum_{i=1,j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)})-\sum_{i=1}^m \alpha_i\\
s.t. &\quad \sum_{i=1}^m \alpha_i y^{(i)}=0\\
& \quad 0\le \alpha_i \le C
\end{aligned}
$$

求得的最优解$\alpha^*$需要满足KKT对偶互补条件：

$$
\alpha_i^*(y^{(i)}(w^Tx_i+b)-1+\xi^*_i)=0
$$

通过这个条件以及$C-\alpha_i-\mu_i=0$，可以得到：

$$
\begin{align}
\alpha_i^*=0   \Longrightarrow  y^{(i)}(w^*\cdot\phi(x^{(i)})+b)\ge 1 \Longrightarrow y^{(i)}g(x^{(i)})\ge 1\\
0<\alpha_i^*<C   \Longrightarrow  y^{(i)}(w^*\cdot\phi(x^{(i)})+b) = 1 \Longrightarrow y^{(i)}g(x^{(i)}) = 1\\
\alpha_i^*=C   \Longrightarrow  y^{(i)}(w^*\cdot\phi(x^{(i)})+b)\le 1 \Longrightarrow y^{(i)}g(x^{(i)})\le 1
\end{align}
$$

其中定义$g(x)\overset{def}=w^*\cdot \phi(x)+b^*=\sum_\limits{j=1}^m\alpha_j^* y^{(j)}K(x,x^{(j)})+b^*$ 。

SMO算法是一种启发式的方法，每次都只优化两个变量，将其他的变量视为常数。由于$\sum_{i=1}^m\alpha_i y^{(i)}=1$。假如将$\alpha_3,\alpha_4,...,\alpha_m$固定，那么$\alpha_1,\alpha_2$之间的关系也确定了。这样SMO算法就将一个复杂的优化算法转化为一个比较简单的两变量优化问题。

定义$K_{ij}=\phi(x^{(i)})\cdot\phi(x^{(j)})$，由于$\alpha_3,\alpha_4,...,\alpha_m$都是常量，目标优化函数可以变为：

$$
\begin{align}
&\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)})-\sum_{i=1}^m \alpha_i\\
=&\frac{1}{2}\sum_{i=1}^m\{ \alpha_i \alpha_1 y^{(i)} y^{(1)} K_{i1} + \alpha_i \alpha_2 y^{(i)} y^{(2)} K_{i2}+\sum_{j=3}^m\alpha_i \alpha_j y^{(i)} y^{(j)} K_{ij} \}-(\alpha_1+\alpha_2+\sum_{i=3}^m\alpha_i)\\
=&\frac{1}{2}\{ ( \alpha_1 \alpha_1 y^{(1)} y^{(1)} K_{11} + \alpha_1 \alpha_2 y^{(1)} y^{(2)} K_{12}+\sum_{j=3}^m\alpha_1 \alpha_j y^{(1)} y^{(j)} K_{1j})  \\
&+ ( \alpha_2 \alpha_1 y^{(2)} y^{(1)} K_{21} + \alpha_2 \alpha_2 y^{(2)} y^{(2)} K_{22}+\sum_{j=3}^m\alpha_2 \alpha_j y^{(2)} y^{(j)} K_{2j})  \notag \\ 
&+  \sum_{i=3}^m( \alpha_i \alpha_1 y^{(i)} y^{(1)} K_{i1} + \alpha_i \alpha_2 y^{(i)} y^{(2)} K_{i2}+\sum_{j=3}^m\alpha_i \alpha_j y^{(i)} y^{(j)} K_{ij})   \} -(\alpha_1+\alpha_2+\sum_{i=3}^m\alpha_i) \notag \\
=&\frac{1}{2}\alpha_1^2K_{11}+\alpha_1\alpha_2y^{(1)}y^{(2)}K_{12}+\frac{1}{2}\alpha_2^2K_{22}+\alpha_1y^{(1)}\sum_{i=3}^m\alpha_iy^{(i)}K_{i1}+\alpha_2y^{(2)}\sum_{i=3}^m\alpha_iy^{(i)}K_{i2}\\
&+\frac{1}{2}\sum_{i=3}^m\sum_{j=3}^m \alpha_i\alpha_jy^{(i)}y^{(j)}K_{ij}- (\alpha_1+\alpha_2) -\sum_{i=3}^m\alpha_i \notag\\
=&\frac{1}{2}\alpha_1^2K_{11}+\alpha_1\alpha_2y^{(1)}y^{(2)}K_{12}+\frac{1}{2}\alpha_2^2K_{22}- (\alpha_1+\alpha_2)\\
&+\alpha_1y^{(1)}\sum_{i=3}^m\alpha_iy^{(i)}K_{i1}+\alpha_2y^{(2)}\sum_{i=3}^m\alpha_iy^{(i)}K_{i2} + Constant \notag
\end{align}
$$

把所有的常量从目标函数中去掉，那么优化的目标函数变为：

$$
\begin{align}
\min_{\alpha_1,\alpha_2} \frac{1}{2}K_{11}\alpha_1^2+y^{(1)}y^{(2)}K_{12}\alpha_1\alpha_2+&\frac{1}{2}K_{22}\alpha_2^2-\alpha_1- \alpha_2+\alpha_1y^{(1)}\sum_{i=3}^m\alpha_iy^{(i)}K_{i1}+\alpha_2y^{(2)}\sum_{i=3}^m\alpha_iy^{(i)}K_{i2}\notag \\
s.t. \quad &\alpha_1y^{(1)}+\alpha_2y^{(2)}=-\sum_{i=3}^m y^{(i)}\alpha_i=\zeta \\
& 0\le \alpha_1,\alpha_2\le C \notag
\end{align}
$$

进一步为了简化表示，我们定义：

$$
\begin{align}
v_i \overset{def}=&
\sum_{j=3}^m \alpha_jy^{(j)}K(x^{(i)},x^{(j)}) \notag \\
=& \sum_{j=1}^m\alpha_j y^{(j)}K(x^{(i)},x^{(j)})+b- \sum_{j=1}^2\alpha_jy^{(j)}K(x^{(i)},x^{(j)}) -b \notag\\
=& g(x^{(i)})- \sum_{j=1}^2\alpha_jy^{(j)}K(x^{(i)},x^{(j)}) -b   \qquad (i=1,2)
\end{align}
$$

利用式(11)带入到式(10)可以简化为：

$$
W(\alpha_1,\alpha_2)=\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2+y^{(1)}y^{(2)}K_{12}\alpha_1\alpha_2-(\alpha_1+\alpha_2)+y^{(1)}\alpha_1v_1+y^{(2)}\alpha_2v_2
$$

由于$\alpha_1y^{(1)}+\alpha_2y^{(2)}=\zeta$并且$(y^{(i)})^2=1$，可以得到$\alpha_1$用$\alpha_2$的表达式为：

$$
\alpha_1=y^{(1)}(\zeta - \alpha_2y^{(2)})
$$

将式(13)带入优化目标函数，可以消除$\alpha_1$得到仅仅包含$\alpha_2$的式子：

$$
W(\alpha_2) =
\frac{1}{2}K_{11}(\zeta  - \alpha_2y^{(2)})^2 + \frac{1}{2}K_{22}\alpha_2^2 +y^{(2)}K_{12}(\zeta - \alpha_2y^{(2)}) \alpha_2\\
-(\alpha_1 + \alpha_2) +(\zeta  - \alpha_2y^{(2)})v_1 +  y^{(2)}\alpha_2v_2
$$

现在对式(14)求偏导可以得到$\alpha_2^{new,unc}$

$$
\frac{\partial W}{\partial \alpha_2}=K_{11}\alpha_2+K_{22}\alpha_2-2K_{12}\alpha_2-K_{11}\zeta y^{(2)}
+K_{12}\zeta y^{(2)}+y^{(1)}y^{(2)}-1-v_1 y^{(1)}+v_2y^{(2)}
$$

令式(15)等于0，整理得：

$$
\begin{align}
(K_{11}+K_{22}-2K_{12})\alpha_2 
&= y^{(2)}(y^{(2)}-y^{(1)}+\zeta K_{11}-\zeta K_{12}+v_1-v_2)\\
&= y^{(2)}\{ y^{(2)}-y^{(1)}+\zeta K_{11}-\zeta K_{12} \notag \\
&\quad +(g(x^{(1)})- \sum_{j=1}^2\alpha_jy^{(j)}K_{1j} -b)-(g(x^{(2)})- \sum_{j=1}^2\alpha_jy^{(j)}K_{2j}-b) \}\\
\end{align}
$$

将$\zeta = \alpha_1^{old}y^{(1)}+ \alpha_2^{old}y^{(2)}$带入，得到：

$$
\begin{align}
&(K_{11}+K_{22}-2K_{12})\alpha_2^{new,unc}\\
=&y^{(2)}((K_{11}+K_{22}-2K_{12})\alpha_2^{old}y^{(2)}+y^{(2)}-y^{(1)}+g(x^{(1)})-g(x^{(2)})) \\
=&(K_{11}+K_{22}-2K_{12})\alpha_2^{old}y^{(2)}y^{(2)}+y^{(2)}[(g(x^{(1)})-y^{(1)})-(g(x^{(2)})-y^{(2)})] \\
\overset{def}=&(K_{11}+K_{22}-2K_{12})\alpha_2^{old}+y^{(2)}(E_1-E_2)
\end{align}
$$

其中$E_i=g(x^{(i)})-y^{(i)}=\left(\sum_\limits{j=1}^m\alpha_j y^{(j)}K(x^{(i)},x^{(j)})+b\right )-y^{(i)}$表示对输入$x^{(i)}$ 的预测值与真实输出$y^{(i)}$之差。

最终可以得到$\alpha_2^{new,unc}$的表达式：

$$
\alpha_2^{new,unc}=\alpha_2^{old}+\frac{y^{(2)}(E_1-E_2)}{K_{11}+K_{22}-2K_{12}}
$$

从式(19)可以得到$\alpha_2^{new,unc}$和$\alpha_2^{old}$的迭代公式。由于$\alpha_2^{new,unc}$需要满足下面的约束：

$$
\left\{
\begin{array}{} 
\alpha_2=y^{(2)}(\zeta - \alpha_1y^{(1)})\\
0\le\alpha_1\le C,\quad  0\le\alpha_2\le C, \quad  y^{(1)},y^{(2)}\in \{+1,-1\} \\
\end{array}
\right.
$$

由于$y^{(1)},y^{(2)}$均只能取值+1或者-1,那么$\alpha_1,\alpha_2$在[0,C]和[0,C]形成的盒子里面，并且两者关系直线的斜率只能为1或者-1，即$\alpha_1,\alpha_2$的关系直线平行于[0,C]和[0,C]形成的盒子对角线，如下所示：

(1)当$y^{(1)}$和$y^{(2)}$异号的时候($|k|=|\zeta|$)

$$
\left\{
\begin{array}{}
\alpha_1-\alpha_2=k 或者 \alpha_2-\alpha_1 = k\\
C \ge \alpha_i \ge 0 ,\quad i=1,2
\end{array}
\right.
$$

+ 以$\alpha_2$为例，它的上限为：

  $\alpha_2$的上限要么是点1的$C$，要么是点2的$C-k$。如果$\alpha_2\lt \alpha_1$，那么上限就是红色线段点2的$C-k$；如果$\alpha_2\gt \alpha_1$，那么上限就是紫色线段点1的$C$。即：

$$
H = \left\{
\begin{align}
  C-k & \alpha_2 \lt \alpha_1\\
  C & \alpha_2 \gt \alpha_1\\
\end{align}
\right.
$$

  式(22)可以整理为用一个式子表示$H=\min(C,C+\alpha_2-\alpha_1)$

+ 以$\alpha_2$为例，它的下限为：

  $\alpha_2$的下限要么是点3的$-k$，要么是点4的$0$。如果$\alpha_2\lt \alpha_1$，那么上限就是红色线段点4的$0$；如果$\alpha_2\gt \alpha_1$，那么上限就是紫色线段点3的$-k$。即：
  
$$
  L= \left\{
  \begin{array}{}
  0&\alpha_2 \lt \alpha_1\\
  -k& \alpha_2 \gt \alpha_1\\
  \end{array}
  \right.
$$

  式(23)可以整理为用一个式子表示$L=\max(0,\alpha_2-\alpha_1)$

(2)当$y^{(1)}$和$y^{(2)}$异号的时候($|k|=|\zeta|$)

和上面相同的分析方法，可以得到：

$$
L=\max(0,\alpha_2+\alpha_1-C)   \qquad    H=\min(C, \alpha_2+\alpha_1) 
$$

所以通过求导得到的$\alpha_2^{new,unc}$，最终的$\alpha_2^{new}$应该为：

$$
\alpha_2^{new}=\left\{
    \begin{array}{}
    H & \alpha_2^{new,unc}\gt H \\
    \alpha_2^{new,nuc} & L\le \alpha_2^{new,unc} \le H \\
    L &  \alpha_2^{new,unc} \lt L
    \end{array}
\right.
$$

由$\alpha_2^{new}$求得$\alpha_1^{new}$是

$$
\alpha_1^{new}=\alpha_1^{old}+y^{(1)}y^{(2)}(\alpha_2^{old}-\alpha_2^{new})
$$


## SMO算法两个变量的选择

SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件。

### 第一个变量的选择

SMO称选择第1个变量的过程为外层循环，外层循环在训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第1个变量。具体地，检查训练样本$(x^{(i)},y^{(i)})$是否满足KKT条件，即：

$$
\begin{align}{}
\alpha_i =0  &\Rightarrow y^{(i)}g(x^{(i)})\ge1\\
0 \lt \alpha_i \le C & \Rightarrow  y^{(i)}g(x^{(i)})=1\\
\alpha_i= C & \Rightarrow y^{(i)}g(x^{(i)})\le1
\end{align}
$$

检验实在$\varepsilon$范围内进行的。在检验过程中，首先选择违反$0 \lt \alpha_i \le C  \Rightarrow  y^{(i)}g(x^{(i)})=1$ 这个条件的点。如果这些支持向量都满足KKT条件，在选择违反$\alpha_i =0 \Rightarrow y^{(i)}g(x^{(i)})\ge1$ 或者$ \alpha_i= C \Rightarrow y^{(i)}g(x^{(i)})\le1$ 的点。

### 第二个变量的选择

SMO算法称选择第二个变量为内层循环，假设我们在外层循环已经找到了$\alpha_1$，第二个变量$\alpha_2$的选择标准是让$|E_1-E_2|$有足够大的变化。由于$\alpha_1$定了的时候$E_1$也确定了，所以要想$|E_1-E_2|$最大，只需要在$E_1$为正时，选择最小的$E_i$作为$E_2$，在$E_1$为负时，选择最大的$E_i$作为$E_2$。为了节省计算时间，将所有$E_i$值都保存在一个列表中。

如果内层循环找到的点不能让目标函数有足够的下降，可以采用遍历支持向量点来做$\alpha_2$，直到目标函数有足够的下降，如果所有的支持向量做$\alpha_2$都不能让目标函数有足够下降，可以跳出循环，重新选择$\alpha_2$。



### 计算阈值$b$和差值$E_i$

在每次完成两个变量的优化后，都要重新计算阈值$b$。

（1）当$0\lt \alpha_1^{new}\lt C$时，根据KKT条件有：

$$
y^{(1)}=\sum_{i=1}^m \alpha_i y^{(i)}K_{i1}+b
$$

于是有$b_1^{new}$为：

$$
b_1^{new}=y^{(1)}-\sum_{i=3}^m \alpha_i y^{(i)}K_{i1}-\alpha_1^{new}y^{(1)}K_{11}-\alpha_2^{new}y^{(2)}K_{21}
$$

计算$E_1$为：

$$
E_1=g(x^{(1)})-y^{(1)}=\sum_{i=3}^m \alpha_i y^{(i)}K_{i1}+\alpha_1^{old}y^{(1)}K_{11}+\alpha_2^{old}y^{(2)}K_{21}+b^{old}-y^{(1)}\\
$$

由式(34)和式(35)可以得到：

$$
b_1^{new}=-E_1 -y^{(1)}K_{11}(\alpha_1^{new}-\alpha_1^{old})-y^{(2)}K_{21}(\alpha_2^{new}-\alpha_2^{old})+b^{old}
$$

（1）当$0\lt \alpha_1^{new}\lt C$时，同理可以得到：

$$
b_2^{new}=-E_2 -y^{(1)}K_{12}(\alpha_1^{new}-\alpha_1^{old})-y^{(2)}K_{22}(\alpha_2^{new}-\alpha_2^{old})+b^{old}
$$

最终的$b^{new}$为：

$$
b^{new}=\frac{b_1^{new}+b_2^{new}}{2}
$$


得到了$b^{new}$就可以更新$E_i$：
$$
E_i = \sum_{S}y^{(i)}\alpha_iK(x^{(i)},x^{(j)})+b^{new}-y^{(i)}
$$
其中$S$是所有支持向量$x^{(i)}$的集合。



## SMO算法

> 输入：训练数据$D=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)}),\}$和精度$\varepsilon$，其中$x^{(i)}\in\mathbb{R}^n,y\in\{+1,-1\}$
>
> 输出：近似解$\alpha$
>
> (1)取初值$\alpha^0=0,k=0$
>
> (2)选取优化变量$\alpha_1^k,\alpha_2^k$，求出新的$\alpha_2^{new,unc}$：
> $$
> \alpha_2^{new,unc} = \alpha_2^{k} + \frac{y_2(E_1-E_2)}{K_{11} +K_{22}-2K_{12}}\notag
> $$
> (3)求出$\alpha_2^{new}$：
>
> $$
> \alpha_2^{k+1}= \begin{cases} H& {L \leq \alpha_2^{new,unc} > H}\\ \alpha_2^{new,unc}& {L \leq \alpha_2^{new,unc} \leq H}\\ L& {\alpha_2^{new,unc} < L} \end{cases}
> $$
>
> (4)利用$\alpha_2^{k+1}$和$\alpha_1^{k+1}$的关系求出$\alpha_1^{k+1}$：
>
> $$
> \alpha_1^{k+1}=\alpha_1^{k}+y^{(1)}y^{(2)}(\alpha_2^{k+1}-\alpha_2^{k})
> $$
>
> (5)计算$b^{k+1}$和$E_i$：
>
> (6)在精度$\varepsilon$范围内检查是否满足如下终止条件：
>
> $$
> \sum_{i=1}^m \alpha_i y^{(i)}=0 \notag\\
> 0 \le \alpha_i \le C \\
> y^{(i)}\cdot g(x^{(i)})=
> \left\{
> \begin{array}{}
> \ge 1\qquad \{ x^{(i)}|\alpha_i=0\}\\
> = 1   \qquad \{ x^{(i)}|0\le \alpha_i \le C\}\\
> \le 1 \qquad \{ x^{(i)}|\alpha_i=C\}
> \end{array}
> \right.
> $$
>
> (7)如果满足条件就结束并返回$\alpha^{k+1}$，否则转到(2)