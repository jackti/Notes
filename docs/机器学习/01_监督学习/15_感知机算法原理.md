# 感知机算法原理

感知机(Perceptron)是二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取值为+1和-1二值。感知机对应于输入空间(特征空间)中实例划分为正负两类的分离超平面，属于判别模型，是神经网络和支持向量机的基础。

## 感知机模型

感知机模型尝试找到一个超平面，能够将训练集正实例点和负实例点完全正确分开。使用感知机的一个前提：**数据是线性可分的**。如果数据是线性不可分，也就意味着感知机模型不适合当前的数据，这时需要考虑其他模型，如支持向量机通过核技巧让数据在高维空间可分，神经网络通过激活函数和增加隐层数量让数据可分。

给定一个数据集$D$：
$$
((x_1^{(0)},x_2^{(0)},..,y^{(0)}),(x_1^{(1)},x_2^{(1)},...,y^{(1)}),...,(x_1^{(m)},x_2^{(m)},...,y^{(m)}))
$$
其中$x\in \mathbb{R}^n,y\in \{+1,-1\}$。现在需要找到一个超平面，即：
$$
\theta_0+\theta_1 x_1+...+\theta_n x_n =0
$$
能够讲数据集的正实例和负实例完全正确地划分到超平面的两侧。让数据集$D$中所有$y^{(i)}=+1$的实例都满足$\theta_0+\theta_1 x_1^{(i)}+...+\theta_n x_n^{(i)} >0$，让所有$y^{(i)}=-1$的实例满足$\theta_0+\theta_1 x_1^{(i)}+...+\theta_n x_n^{(i)} <0$。

==通常这样的超平面也不唯一，即感知器模型可以有多个解==。为了简化上式可以增加一个$x_0=1$，公式(2)可以写成$\sum_{i=0}^n \theta_i x_i=0$。用向量表示为$\theta^T\cdot x=0$，其中$x\in \mathbb{R}^{n+1},\theta \in \mathbb{R}^{n+1}$。

最终感知机的模型可以写为：
$$
y=sign(\theta^T x)
$$
其中$sign(\cdot)$是指示函数，定义为
$$
sign(z)=\left \{
\begin{aligned}
    +1 \qquad z \ge 0\\
    -1 \qquad z \lt 0\\
 \end{aligned}
 \right.
$$


## 感知机损失函数

对于误分类的点$(x^{(i)},y^{(i)})$存在两种情况(这里注意$y^{(i)}\in \{+1,-1 \},x ,\theta \in \mathbb{R}^{n+1}$)：

+ 如果$\theta^T x^{(i)} \gt 0$，本来应该是属于$y^{(i)}=+1$类别，但误分类为$y^{(i)}=-1$类别，则有$y^{(i)}\theta^T\cdot x^{(i)}<0$；
+ 如果$\theta^T x^{(i)} \lt 0$，本来应该是属于$y^{(i)}=-1$类别，但误分类为$y^{(i)}=+1$类别，则有$y^{(i)}\theta^T\cdot x^{(i)}<0$；

所以误分类的点到超平面的距离为：
$$
-\frac{1}{||\theta||}y^{(i)}(\theta^T \cdot x^{(i)})
$$

> 点$(x_0,y_0)到直线ax+by+c=0$的距离为：
> $$
> \begin{aligned}
> \left \Arrowvert \frac{ax_0+by_0+c}{\sqrt{a^2+b^2}} \right \Arrowvert
> \end{aligned}
> $$
>

假设所有误分类的点的集合为$M$，则所有误分类点到超平面的距离之和为：
$$
-\frac{1}{||\theta||} \sum_{x^{(i)}\in M}y^{(i)}(\theta^T \cdot x^{(i)})
$$
可以发现分子和分母都含有$\theta$，且两者有固定的倍数关系，即当分子的$\theta$扩大$N$倍，分母也会扩大$N$倍。所以可以固定分子或者分母为1，然后求另一个分子或分母的倒数的最小化最为损失函数，这样就可以简化模型。在感知机模型中，采用的是保留分子，即感知机模型的损失函数简化为：
$$
J(\theta)=-\sum_{x^{(i)}\in M}y^{(i)}(\theta^T \cdot x^{(i)})
$$
==在感知机里面采用的是保留分子，分母固定为1，而在支持向量机中采用的是保留分母，分子固定为1，求$1/||\theta||$最大化==。

## 感知机算法

感知机的损失函数是一个凸函数，可以直接采用梯度下降法或牛顿法解决，常用的是梯度下降法。这里需要注意的是$x^{(i)}\in M$即只有误分类点集合$M$的样本才能参与损失函数的优化。感知机模型采用随机梯度下降法，每次仅仅需要一个误分类的点$y^{(i)}\theta^T\cdot x^{(i)}<0$来更新梯度。

损失函数基于$\theta$的偏导数是：
$$
\frac{\partial}{\partial \theta}J(\theta)=-\sum_{x^{(i)}\in M}y^{(i)}x^{(i)}
$$
$\theta$的梯度下降公式应该为：
$$
\theta = \theta +\alpha\sum_{x^{(i)}\in M}y^{(i)}x^{(i)}
$$
由于采用随机梯度下降法，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第$i$个样本来更新梯度，则$\theta$的迭代公式为：
$$
\theta =\theta + \alpha y^{(i)}x^{(i)}
$$
其中$\alpha$为步长(或学习率)，$y^{(i)}$为样本输出+1或者-1，$x^{(i)}\in \mathbb{R}^{n+1}$。

感知机算法的实现
```python
class Perceptron:
    def __init__(self, learning_rate=0.01, n_iterations=100):
        self._learning_rate = learning_rate
        self._n_iterations = n_iterations
        self._weight = None
        self._errors = []

    def predict(self, X):
        """
        predict input data
        :param X: {array-like},shape=[n_samples,n_features]
        :return: y:{array-like}, shape=[n_smaples,]
        """
        pred = X.dot(self._weight[1:]) + self._weight[0]
        return np.where(pred >= 0.0, 1, -1)

    def fit(self, X, y):
        """
        training process
        :param X: {array-like}, shape=[n_samples,n_features]
        :param y: {array-like}, shape=[n_smaples,]
        :return: None
        """
        n_samples, n_features = X.shape

        self._weight = np.zeros(1 + n_features)
        for _ in range(self._n_iterations):
            errors = 0
            for x, target in zip(X, y):
                update = self._learning_rate * (target - self.predict(x))
                self._weight[1:] += update * x
                self._weight[0] += update
                errors += int(update != 0.0)
            self._errors.append(errors)

```

感知机学习算法的原始形式：

> 输入：训练数据集$D={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})}$，学习率$\alpha$
>
> 输出：参数$\theta$
>
> (1)选取初始值$\theta_0$
>
> (2)在训练集中选取数据$(x^{(i)},y^{(i)})$
>
> (3)如果$y^{(i)}\theta^T\cdot x^{(i)} \le0$
> $$
> \begin{aligned}
> \theta = \theta+\alpha y^{(i)} x^{(i)}
> \end{aligned}
> $$
> (4)转至(2)，直至训练集中没有误分类点

这种算法的直观解释是：**当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整参数$\theta$的值，使得分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离，直至超平面越过该误分类点使其被正确分类。**

**对于线性可分数据集感知机学习算法原始形式手链，即经过有限次迭代可以得到一个将数据集完全正确划分的分离超平面**。



## 感知机对偶形式

感知机模型的原始形式$\theta = \theta+\alpha y^{(i)}x^{(i)}$可以看到，每次梯度的迭代都是选择一个样本来更新$\theta$向量，经过若干次的迭代得到最终的结果。对于从来都没有误分类过的样本，它被选择参与$\theta$的迭代次数是0，对于多次分类错误的样本$j$，设其参与$\theta$迭代的次数为$m_j$。令$\theta$向量的初始值为0，这样$\theta$向量的表达式可以写成：
$$
\theta = \alpha\sum_{j=1}^{m}m_j y^{(j)}x^{(j)}
$$
其中$m_j$为样本$(x^{(j)},y^{(j)})$在随机梯度下降到当前这一步之前因误分类而更新的次数。每一个样本$(x^{(j)},y^{(j)})$的$m_j$初始值为0，每当样本在梯度下降迭代中因误分类而被更新时$m_j\leftarrow m_j+1$。由于$\alpha$是常量，令$\beta_j=\alpha m_j$，则向量$\theta$的表达式为：
$$
\theta = \sum_{j=1}^m \beta_j y^{(j)}x^{(j)}
$$
对于每一步判断误分类的条件，可以从$y^{(i)}\theta^T\cdot x^{(i)} \le0$ 变为$y^{(i)} \sum_{j=1}^m \beta_j y^{(j)}x^{(j)} \cdot x^{(i)} \le0$来判断。在这个判断条件中存在两个样本$x^{(i)}$和$x^{(j)}$的内积，这个内积计算的结果在后续的迭代过程中是可以重复使用的。为了方便，可以预先将训练集中样本内积计算出来并以矩阵的形式存储，称为Gram矩阵，这是一个对称矩阵，记作$G=[x^{(i)},y^{(j)}]$。Gram矩阵就是支持向量机对偶模型比原始模型高效的原因。

感知机学习算法的对偶形式：

> 输入：训练数据集$D={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})},\beta=0,\alpha=1$
>
> 输出：参数$\theta$
>
> (1)计算所有样本内积形成的Gram矩阵G。
>
> (2)在训练集中选取数据$(x^{(i)},y^{(i)})$
>
> (3)如果$y^{(i)} \sum_{j=1}^m \beta_j y^{(j)}x^{(j)} \cdot x^{(i)} \le0$
> $$
> \begin{aligned}
> \beta_i=\beta_i+\alpha
> \end{aligned}
> $$
> (4)转至(2)，直至训练集中没有误分类点。
>
> (5)此时$\theta$向量的结果为
> $$
> \begin{aligned}
> \theta =\sum_{j=1}^m \beta_j y^{(j)}x^{(j)}
> \end{aligned}
> $$
>

