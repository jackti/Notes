# 最小二乘法

最小二乘法是用来做函数拟合或者求函数极值的方法．在机器学习中，尤其是回归模型中，经常可以看到最小二乘法的身影，这里对最小二乘法做个简单的总结．

## 最小二乘法的的原理

现在我们有$m$个样本,如下：

$$
(x^{(1)},y^{(1)}),(x^{(2)},y^{(2))},(x^{(3)},y^{(3)}),...,(x^{(m)},y^{(m)})
$$

样本采用拟合函数为:

$$
y^{(i)}=h_{\theta}(x^{(i)})=\theta^T x^{(i)}+\varepsilon^i
$$

我们假设误差项符合高斯分布，即:

$$
p(\varepsilon^{i})=\frac{1}{\sqrt{2\pi}\sigma}
\exp(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2})
$$

将拟合函数带入到上式，得到

$$
p(y^{(i)}|x^{(i)};\theta)
=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})
$$

然后采用最大似然估计得到似然函数：

$$
L(\theta)=\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)
=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})
$$

由于乘法比价难处理，这里我们可以对似然函数取对数，讲似然函数里面的乘法转换为加法，方便处理。

$$
\begin{align}
logL(\theta)
= log\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
= mlog\frac{1}{\sqrt{2\pi} \sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}{(y^{(i)}-\theta x^{(i)})^2}
\end{align}
$$

为了让似然函数最大，等价于$J(\theta)$最小

$$
J(\theta)=\sum_{i=1}^{m}(y^{(i)}-\theta^T x^{(i)})^2
$$

这样我们就得到了最小二乘法的基本形式。

## 最小二乘法求解

这里可以讲上面的式子用矩阵表示，如下：

$$
h_\theta(x)=X\theta
$$

其中，$h_\theta(x)\in \mathbb{R}^m$，$\theta\in \mathbb{R}^n$ ，$X\in \mathbb{R}^{m×n}$ ,$m$代表样本个数,$n$代表样本的特征数。

损失函数$J(\theta)$的定义为 :

$$
J(\theta)=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)
$$

这里的$Y\in \mathbb{R}^m$是样本的输出向量,$\frac{1}{2}$在这里主要是为了让后续求导系数为1，方便计算。

为了求$J(\theta)$的最小值，我们需要对损失函数对$\theta$求导取0。具体结果如下：

$$
\begin{align}
0=\frac{\partial}{\partial{\theta}}J(\theta)
&=\nabla_{\theta}J(\theta)\\
&= \nabla_{\theta}\frac{1}{2}(X\theta-Y)^T(X\theta-Y)\\
&= \nabla_{\theta}\frac{1}{2}(\theta^TX^T-Y^T)(X\theta-Y)\\
&=\nabla_{\theta}\frac{1}{2}{(\theta^TX^TX\theta-\theta^TX^TY-Y^TX\theta+Y^TY)}
\end{align}
$$

可以求得

$$
\theta = (X^TX)^{-1}X^TY
$$

这里用到了矩阵求导法则：

> 公式（1）：$\frac{\partial}{\partial X}(XX^T)=2X$
>
> 公式（2）：$\frac{\partial}{\partial \theta}(X\theta)=X^T$



## 最小二乘法的局限性

以上的所讲的就是正规方程，也就是最小二乘法求解析解的方法。要保证最小二乘法有解，就得保证$A^TA$是一个可逆阵（非奇异矩阵）。那么是否存在某些情况下$A^TA$是不可逆的？如果$A^TA$是不可逆的，该怎么处理？

（1）当样本数量小于参数向量的维度即$m<n$时，$A^TA$一定是不可逆的。

（2）在所有特征中如果存在一个特征和另外一个特征 或 一个特征与其他若干特征线性相关，$A^TA$也不可逆。

如果$A^TA$不可逆，我们可以采用的方法如下：

（1）增加样本数量

（2）对样本数据进行整理，去掉冗余特征，让$X^TX$的行列式不为0。

（3）采用正则化的方法。常见的是L1正则化和L2正则化。



## 最小二乘法的改进

最小二乘法由于是最小化均方差，它考虑了每个样本的贡献，也就是每个样本具有相同的权重；由于它采用距离作为度量，使得他对噪声比较敏感(最小二乘法==假设噪声服从高斯分布==)，即使得他它对异常点比较敏感。因此，人们提出了加权最小二乘法，相当于给每个样本设置了一个权重，以此来反应样本的重要程度或者对解的影响程度。


