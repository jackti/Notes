# 最大熵算法原理(三)

最大熵模型的最优化算法：
$$
w^*=\arg\max_{w} \Psi(w)=\arg \max_{w}\left( \sum_{x,y}\tilde{p}(x,y)\sum_{i=1}^{n}w_if_i(x,y)-\sum_{x}\tilde{p}(x)\log Z_w(x)\right )
$$
是无法求出显式的解析解，需要借助于数值算法。由于目标函数是一个光滑的凸函数，因此可采用多种最优化方法来进行求解，且能保证找到全局最优解。常用的方法有梯度下降法、牛顿法、拟牛顿法和专门为最大熵量身定制的两个方法——通用迭代尺度法(Generalized Iterative Scaling, GIS)和改进的迭代尺度法(Improved Iterative Scaling, IIS)。

IIS也是启发式方法，假设当前的参数向量是$w$，希望找到一个新的参数向量$w+\delta$使得对偶函数$\Psi(w)$增大。如果可以找到这样的向量，就重复使用该方法，直到找到对偶函数的最大值。IIS算法通过构造$\Psi(w+\delta)-\Psi(w)$的下界$B(w|\delta)$，通过对$B(w|\delta)$极小化来得到相应的$\delta$的值，进而来迭代求解$w$。对于$B(w|\delta)$的求极小值是通过对$\delta$求导数得到的。但是IIS算法一般只适用于最大熵模型，使用范围不广泛。

其它的一些最优化算法(如梯度下降法、牛顿法和拟牛顿法)来获得$\Psi(w)$的最大值点$w^*$都需要计算$\Psi(w)$关于$w$的梯度，这里给出相应的计算公式：
$$
\begin{aligned}
\frac{\partial \Psi}{\partial w_i}&=\frac{\partial}{\partial w_i}\left (\sum_{x,y}\tilde{p}(x,y)\sum_{i=1}^{n}w_if_i(x,y)-\sum_{x}\tilde{p}(x)\log Z_w(x)\right )\\
&=\sum_{x,y}\tilde{p}(x,y)f_i(x,y)-\sum_{x}\tilde{p}(x)\frac{1}{Z_w(x) } \cdot\frac{\partial }{\partial w_i}Z_w(x)\\
&=\sum_{x,y}\tilde{p}(x,y)f_i(x,y)-\sum_{x}\tilde{p}(x)\frac{1}{Z_w(x) } \cdot\frac{\partial }{\partial w_i}\left(\sum_{y} e^{\sum_{i=1}^n w_i f_i(x,y)}\right)\\
&=\sum_{x,y}\tilde{p}(x,y)f_i(x,y)-\sum_{x}\tilde{p}(x)\frac{1}{Z_w(x) } \cdot \left(\sum_{y}e^{\sum_{i=1}^n w_i f_i(x,y)}\cdot f_i(x,y)  \right)\\
&=\sum_{x,y}\tilde{p}(x,y)f_i(x,y)-\sum_{x}\tilde{p}(x)\left(\frac{1}{Z_w(x) }\sum_{y} e^{\sum_{i=1}^n w_i f_i(x,y)}\right )\cdot f_i(x,y) \\
&=\sum_{x,y}\tilde{p}(x,y)f_i(x,y)-\sum_{x}\tilde{p}(x)\sum_{y}p_w(y|x)f_i(x,y)\\
&=\mathbb{E}_{\tilde{p}}(f_i)-\sum_{x,y}\tilde{p}(x)p_w(y|x)f_i(x,y)
\end{aligned}
$$
公式(2)-(5)都是链式法则对$w_i$求导；公式(5)-(7)是利用了$p_w(y|x)=\frac{1}{Z_w(x)} \cdot  e^{\sum_{i=1}^n w_i f_i(x,y)}$；公式(7)-(8)是将两个求和符合写在一起

最终梯度公式为：
$$
\frac{\partial \Psi}{\partial w}=(\frac{\partial \Psi}{\partial w_1},\frac{\partial \Psi}{\partial w_2},...,\frac{\partial \Psi}{\partial w_n})^T
$$




此外这里还额外证明**对偶函数$\Psi(w)$的极大化等价于最大熵模型模型的极大似然估计**。

已知训练数据的经验概率分布为$\tilde{P}(X,Y)$，条件概率分布为$P(Y|X)$的对数似然函数表示为：
$$
L_{\tilde{p}}(p_w)=log\prod_{x,y}p(y|x)^{\tilde{p}(x,y)}=\sum_{x,y} \tilde{p}(x,y) \log p(y|x)
$$
当条件概率分布满足$p(y|x)=\frac{1}{Z_w(x)} \cdot  e^{\sum_{i=1}^n w_i f_i(x,y)}$。公式(10)可以写成：
$$
\begin{aligned}
L_{\tilde{p}}(p_w)
&=\sum_{x,y} \tilde{p}(x,y) \log p(y|x)\\
&=\sum_{x,y} \tilde{p}(x,y) \sum_{i=1}^{n}w_if_i(x,y)-\sum_{x,y} \tilde{p}(x,y)\log Z_w(x)\\
&=\sum_{x,y} \tilde{p}(x,y) \sum_{i=1}^{n}w_if_i(x,y)-\sum_{x} \tilde{p}(x)\log Z_w(x)\\
&=\Psi(x)
\end{aligned}
$$
公式(11)-(12)直接将$p(y|x)$带入展开；公式(12)-(13)是利用$\sum_{y}\tilde{p}(x,y)=\tilde{p}(x)$；可以发现$L_{\tilde{p}}(p_w)=\Psi(x)$证明了对对偶函数的最大化等价于最大熵模型的极大似然估计这一事实。





