# 决策树算法原理(二)

常见的决策树算法有ID3、C4.5、CART等，下表给出了三者的对比。

| 算法 | 支持模型  | 树结构 |    特征选择     | 连续值处理 | 缺失值处理 | 是否剪枝 |
| :--: | :-------: | :----: | :-------------: | :--------: | :--------: | :------: |
| ID3  |   分类    | 多叉树 |    信息增益     |   不支持   |   不支持   |    否    |
| C4.5 |   分类    | 多叉树 |   信息增益比    |    支持    |    支持    |    是    |
| CART | 分类/回归 | 二叉树 | 基尼系数/均方差 |    支持    |    支持    |    是    |

## ID3算法

### ID3算法流程

ID3算法利用信息增益大小来判断当前节点应该选用什么特征来构建决策树，用计算出的信息增益最大的特征来建立决策树的当前节点。

> 输入：训练数据集$D$，特征集$A$，阈值$\epsilon$
>
> 输出：决策树
>
> (1)若$D$中所有实例都属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该节点的类标志，返回$T$；
>
> (2)若$A= \emptyset$，则$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$；
>
> (3)否则，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$；
>
> (4)如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该节点的类标记，返回$T$；
>
> (5)否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树$T$，返回$T$；
>
> (6)对第$i$个子节点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用(1)~(5)，得到子树$T_i$，返回$T_i$。

### ID3算法的不足

1. ID3算法没有考虑连续特征；
2. ID3算法采用信息增益选择特征。在相同条件下，取值较多的特征往往具有较大的信息增益；
3. ID3算法没有考虑缺失值的情况；
4. ID3算法没有考虑过拟合的情况



## C4.5算法

### C4.5算法的优化
针对ID3算法的四个不足之处，C4.5进行了改进：

#### 支持连续特征处理
针对ID3算法不能处理连续特征：C4.5的思路是将连续的特征离散化。如有$m$个样本的连续特征$A$，从小到达排列为$a_1,a_2,...,a_m$，则C4.5取相邻两样本的平均数，一共取得$m-1$个划分点，其中第$i$个划分点$T_i$表示为$T_i=\frac{a_i+a_{i+1}}{2}$。对于这$m-1$个点，分别计算以改点作为二元分类点时的信息增益，选择信息增益最大的点作为该连续特征的二元离散分类点。这样就可以将连续特征离散化。需要注意的是：==与离散特征不同的是，如果当前节点是连续特征，则该属性后面还可以参与子节点的产生选择过程==。


#### 采用信息增益率
信息增益作为选择标准容易倾向于取值特征较多的特征。在C4.5算法中，采用了信息增益率，表达式如下：
$$
IGR(D,A)=\frac{I(A,D)}{H_A(D)}
$$
其中$D$为样本特征输出的集合，$A$为样本特征，对于特征熵$H_A(D)$，表达式如下：
$$
H_A(D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$
其中$n$为特征$A$的类别数，$|D_i|$为特征$A$的第$i$个取值对应的样本数，$|D|$为样本数。特征数越多的特征对应的特征熵越大，它作为分母可以校正信息增益容易偏向与取值较多的特征的问题。


#### 支持缺失值特征处理
对于缺失值的处理问题，主要需要解决两个问题：

（1）**如何在属性值缺失的情况下进行划分属性选择？**

给定样本集$D$和特征$A$，令$\tilde{D}$表示$D$中属性$A$没有缺失的样本子集。对于问题(1)我们可以直接仅根据$\tilde{D}$来判断属性特征$A$的优劣。假设属性特征$A$有$V$个可取值$A\in\{ a^1,a^2,...,a^V\}$，令$\tilde{D}^v$表示$\tilde{D}$中在特征$A$上取值为$a^v$的样本子集，$\tilde{D}_k$表示$\tilde{D}$中属于第$k$类$(k=1,2,...,\mathcal{|Y|})$的样本子集，则显然有$\tilde{D}=\bigcup_{k=1}^{|\mathcal{Y}|}{\tilde{D}_k}$，$\tilde{D}=\bigcup_{v=1}^{V}{\tilde{D}^v}$。假定为每个样本$x$赋予一个权重，定义为：

$$
\begin{align}
\rho&=\frac{\sum_{x\in \tilde{D}}w_x}{\sum_{x\in D}w_x}\\
\tilde{p}_k&=\frac{\sum_{x\in \tilde{D}_k}w_x}{\sum_{x\in \tilde{D}}w_x} (1<=K<=|\mathcal{Y}|)\\
\tilde{r}_v&=\frac{\sum_{x\in \tilde{D}^v}w_x}{\sum_{x\in \tilde{D}}w_x} (1<=v<=|V|)
\end{align}
$$

​直观地看，对特征属性$A$，$\rho$表示无缺失值样本所占的比例，$\tilde{p}_k$表示无缺失值样本中第$k$类所占的比例，$\tilde{r}_v$表示		无缺失值样本在特征属性$A$上取值$a^v$的样本所占的比例。基于以上定义，我们可以将信息增益的计算公式推广为：

$$
\begin{align}
I(D,A)&=\rho\times I(\tilde{D},A)\\
&=\rho\times (H(\tilde{D})-\sum_{v=1}^{V}{\tilde{r}^v H(\tilde{D}^v)})
\end{align}
$$

​推广的信息增益公式解释为：在无缺失值样本中计算信息增益，最后将得到的信息增益乘以无缺失样本在所有样本中的占比。

（2）**给定划分属性，若样本在该属性上的值缺失，如果对样本进行划分？** 

若样本$x$在划分特征属性$A$上的取值已知，则将$x$划入与其取值对应的子节点，样本权重保持$w_x$；

若样本$x$在划分特征属性$A$上的取值未知，则将$x$同时划入所有子节点，且样本权重在与属性值$a^v$对应的子节点中调整为$\tilde{r}_v \cdot w_x$。直观地看，就是让同一个样本以不同的概率划入到不同的子节点中取。



同时，针对过拟合的问题，C4.5算法引入了正则化系数进行初步的剪枝。



### C4.5算法的不足

1. C4.5只能用于分类，这样限制了它的应用范围；
2. C4.5生成的是多叉树，即一个父节点可以有多个子节点。在计算机中二叉树模型会比多叉树运算效率高，如果采用二叉树可以提高效率；
3. C4.5由于使用了熵模型，里面有大量的对数运算。如果是连续值甚至还有大量的排序操作；
4. 由于决策树算法很容易拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有很多，C4.5的剪枝方法还有优化空间





