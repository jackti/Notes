# 最大熵算法原理(一)

最大熵模型(maximum entropy model, MaxEnt)是很典型的分类算法，和逻辑回归类似，都属于对数线性分类模型。在损失函数优化的过程中，使用了和支持向量机类似的凸优化技术。对熵的使用，又可以联想到决策树算法中的ID3和C4.5算法。理解最大熵模型，对逻辑回归、支持向量机和决策树算法都会有帮助。

## 基础知识

### 信息熵 

设$X$是一个取有限个值的离散随机变量，其概率分布为：
$$
P(X=x_i)=p_i,i=1,2,3,...,n
$$
则随机变量$X$的熵定义为：
$$
H(X)=-\sum_{i=1}^{n}p_ilog p_i
$$
若$p_i=0$，则定义$0log0=0$。$log$以$2$或者$e$为底的对数。

### 条件熵

设随机变量$(X,Y)$，其联合概率分布为：
$$
P(X=x_i,Y=y_i)=p_{ij},i=1,2,3,...,n;j=1,2,3,...,m
$$
条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。其定义为$X$在给定条件下$Y$的条件概率分布熵对$Y$的数学期望：

$$
H(Y|X)=\sum_{i=1}^{n}p(x_i)H(Y|X=x_i)=-\sum_{i=1}^{n}p(x_i)\sum_{j=1}^{m}p(y_j|x_i)\log p(y_j|x_i)
$$


### 似然与最大似然估计

在数理统计中，似然函数是一种关于统计模型中参数的函数，在统计推断中有重大作用，如最大似然估计等。"似然性"、"概率"和"或然性"的意思相近，都是在某种事件发生的可能性，但是在统计学中，它们有明确的区分。概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果；而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

设$X$为离散随机变量，其概率分布为$p(x;\theta)$，$\theta$为参数，则$X$的$N$个独立同分布的样本$x_1,x_2,...,x_n$的联合概率分布为：

$$
p(x_1,x_2,...,x_n; \theta)=\prod_{i=1}^{n}p(x_i;\theta)
$$

当参数$\theta$固定时，上式表示$x_1,x_2,...,x_n$的概率；当$x_1,x_2,...,x_n$固定时，上式是$\theta$的函数，记作$\mathcal{L}(\theta;x)$，并称其为似然函数。似然函数$\mathcal{L}(\theta;x)$的值大小意味着该样本出现的可能性大小，既然已经得到了样本$x_1,x_2,...,x_n$那么它出现的可能性应该是大的，即似然函数的值应该是大的。因而可以选择使$\mathcal{L}(\theta;x)$达到最大值的那个$\theta^*$作为参数真实的$\theta$的估计。

在实际应用中一般会取似然函数的对数函数(称为对数似然函数)用来求最大值，由于函数$\log x$的严格单调性，这样求得的最大值和直接求最大值得到的结果是相同的。采用取对数主要是考虑到计算的方便性如求导运算等。



## 最大熵模型原理

### 经验(概率)分布
经验(概率)分布是指通过在训练数据$D$进行统计得到的分布，用$\tilde{p}$表示。通过数据可以得到两个经验分布为：

$$
\begin{align}
\tilde{p}(x,y)=&\frac{count(x,y)}{m}\\
\tilde{p}(x)=&\frac{count(x)}{m}
\end{align}
$$

其中$count(x,y)$和$count(x)$分别表示$(x,y)$和$x$在训练数据$D$中出现的次数。

### 特征函数
特征函数是值对于一个给定的样本$(x,y)$，特征函数可以定义为任意实值函数，现只考虑一种简单且常用的二值定义方式：

$$
f(x,y)=\left \{ 
\begin{aligned}
&1, 若(x,y)若符合某个条件 \\
&0,否则
 \end{aligned}
 \right.
$$

对于任意一个特征函数$f$，记$\mathbb{E}_\tilde{p}(f)$表示$f$在训练数据$D$上关于$\tilde{p}(x,y)$的数学期望，$\mathbb{E}_p(f)$表示$f$在模型上关于$p(x,y)$的数学期望，按照期望的定义，有：

$$
\begin{align}
\mathbb{E}_\tilde{p}(f)=\sum_{x,y}\tilde{p}(x,y)f(x,y) \tag{1}\\
\mathbb{E}_{p}(f)=\sum_{x,y}p(x,y)f(x,y) \tag{2}
\end{align}
$$

对于公式(2)利用Beyes定理有$p(x,y)=p(x)p(y|x)$，由于$p(x)$是未知的，只好利用$\tilde{p}(x)$进行近似。从而讲公式(2)重新定义为:

$$
\mathbb{E}_{p}(f)=\sum_{x,y}p(x,y)f(x,y)=\sum_{x,y}\tilde{p}(x)p(y|x)f(x,y) \tag{3}
$$

对于概率公式$p(y|x)$，希望特征$f$的期望应该和从训练数据中得到的特征期望值是一指的，因此，提出约束：

$$
\mathbb{E}_{p}(f)=\mathbb{E}_\tilde{p}(f) \tag{4}
$$

讲公式(3)和(1)带入公式(4)可以得到：

$$
\sum_{x,y}\tilde{p}(x)p(y|x)f(x,y)=\sum_{x,y}\tilde{p}(x,y)f(x,y)
$$

假设从训练数据中抽取了$N$个特征，那么就会有$N$个特征函数$f_i(i=1,2,...,N)$以及$N$个约束条件

$$
\mathcal{C}_i:\mathbb{E}_p(f_i)=\mathbb{E}_\tilde{p}(f_i):=\tau_i  \qquad i=1,2,...,N
$$

对于给定的训练数据$D$和特征$f_i,\tau_i$都是一个常数。最大熵模型的学习等价于约束最优化问题：

$$
\begin{aligned}
\max_{p\in \mathcal{C}}& H(p)=H(p(y|x))=-\sum_{x,y}\tilde{p}(x)p(y|x)\log p(y|x)\\
s.t. \qquad & \mathbb{E}_p(f_i)=\mathbb{E}_\tilde{p}(f_i) i=1,2,...,n\\
&\sum_y p(y|x)=1
\end{aligned}
$$

我们的目标是求在$H(p)$最大的时候对应的$p(y|x)$，这里可以对$H(p)$加个符号求极小值，目的是为了是$-H(p)$为凸函数，方便使用凸优化的方法来求极值。

$$
\begin{aligned}
\min_{p\in \mathcal{C}}& -H(p)=H(p(y|x))=\sum_{x,y}\tilde{p}(x)p(y|x)\log p(y|x)\\
s.t. \quad & \mathbb{E}_p(f_i)-\mathbb{E}_\tilde{p}(f_i)=0 \qquad i=1,2,...,n\\
&\sum_y p(y|x)=1
\end{aligned}
$$

