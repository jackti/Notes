# 支持向量机算法原理(一)

支持向量机(Support Vector Machine,SVM)是一种二元分类模型，它的基础模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机模型；支持向量机还包括了核技巧，使得它能够处理非线性问题；支持向量机的学习策略就是间隔最大化，转化为一个求解凸二次规划的问题。经过改进和扩展，现在的支持向量机算法也可支持多分类问题和回归问题。

## 函数间隔和几何间隔

在分离超平面固定$w^Tx+b=0$时候，$|w^Tx+b|$表示点$x$到超平面的距离。通过观察$w^Tx+b$与$y$是否同号来判断分类是否正确。定义函数间隔$\gamma^{'}$：
$$
\gamma^{’}=y(w^T x+b)
$$
可以看到，它就是感知机模型里面的误分类点到超平面距离的分子。函数间隔并不能反映点到超平面的距离。定义几何间隔：
$$
\gamma =\frac{y(w^Tx+b)}{||w||_2}=\frac{\gamma^{'}}{||w||_2}
$$
几何间隔才是点到超平面的真正距离。感知机模型用到的距离就是几何距离。





## 感知机模型

在二维平面中，感知机模型尝试找到一条直线，能够把二元数据隔离开。在三维或者更高的空间中，感知机模型能够找到一个超平面，将这个数据集中的二元类别数据隔离开。定义超平面为$w^T x+b=0$，在超平面$w^T x+b=0$上方的样例定义$y=1$，在超平面$w^Tx+b=0$下方的样例定义为$y=-1$，满足这个条件的超平面存在多个，我们的目标是从这多个分离超平面中选择一个泛化能力最强的超平面。

感知机模型思想是让所有误分类点(定义为集合$M$)到超平面的距离和最小，其损失函数表达式如下：
$$
\sum_{x_{i}\in M} \frac{-y^{(i)}(w^T x^{(i)}+b)}{||w||_2}
$$
分子和分母有固定的倍数关系，于是可以固定分子或者分母为1，然后求另一个即分子自己或者分母的导数的最小化损失函数。在感知机模型中采用保留分子，固定分子$||w||=1$，于是感知机模型的损失函数为：
$$
\sum_{x^{(i)\in M}}-y^{(i)}(w^T x^{(i)}+b)
$$
在感知机模型中可以找到多个分类的超平面将数据分开，对于那些离超平面很远的点，让其离超平面更远没有意义，需要关注那些距离超平面很近的点，这些点是很容易被误分类的，要让这些比较近的点尽可能远离超平面，这样的分类效果会更好一些。



## 支持向量机模型

假设超平面$(w,b)$能将训练样本正确分类，对$(x^{(i)},y^{(i)})\in D$，如$y^{(i)}=+1$，则有$w^Tx^{(i)}+b>0$；若$y^{(i)}=-1$，则有$w^Tx^{(i)}+b<0$，令
$$
\left \{
\begin{aligned}
    w^T x^{(i)}+b \ge +1,\qquad y^{(i)}=+1\\
    w^T x^{(i)}+b \lt -1,\qquad y^{(i)}=-1\\
\end{aligned}
\right.
$$
距离超平面最近的几个训练样本点是上面的等号成立，他们被称为"支持向量"，两个异类支持向量$x^+,x^-$到超平面的距离之和$d$为：
$$
\begin{align}
d &= \frac{|w^T x^++b|}{||w||_2}+\frac{|w^T x^-+b|}{||w||_2}\\
&=\frac{1}{||w||_2}( |w^Tx^++b|+|w^Tx^-+b|) \\
&=\frac{1}{||w||_2}|(+1)-(-1)|\\
& =\frac{2}{||w||_2}\\
\end{align}
$$
公式(5)是利用点到距离的公式，计算两个异类支持向量的距离之和；公式(6)~(7)两支持向量分别过直线$w^Tx+b=+1$和$w^T x+b=-1$ ，将$\pm1$带入。由于$\frac{2}{||w||_2}$最大化等价于$\frac{1}{2}||w||^2_2$的最小化，这样支持向量机的基本型：
$$
\begin{align}
                &\min_{w,b}\frac{1}{2}||w||^2_2 \notag \\
                & s.t. \quad y^{(i)}(w^T x^{i}+b)\ge 1,  \quad i=1,2,...,m
\end{align}
$$




## 支持向量机求解

由于目标函数$\frac{1}{2}||w||^2_2$是凸函数，约束条件不等式是仿射的，根据凸优化理论，可以通过拉格朗日函数将优化目标转化为无约束的优化问题(和最大熵模型的优化方法一样)。具体地，对每一个不等式约束引进拉格朗日乘子，$\alpha_i \ge 0,i=1,2,...,m$，定义拉格朗日函数：
$$
L(w,b,\alpha)=\frac{1}{2}||w||^2_2-\sum_{i=1}^{m}\alpha_i[ y^{(i)}(w^T x^{(i)}+b) -1 ]
$$
由于引入了拉格朗日乘子$\alpha=(\alpha_1,\alpha_1,...,\alpha_m)^T$，优化目标变成：
$$
\min_{w,b} \max_{\alpha} L(w,b,\alpha)
$$
根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
$$
\max_\alpha \min_{w,b}L(w,b,\alpha)
$$
可以首先优化函数对于$w$和$b$的极小值，然后对拉格朗日乘子$\alpha$的极大值。

**（1）求$\min_{w,b}L(w,b,\alpha)$**

将拉格朗日函数$L(w,b,\alpha)$分别对$w$和$b$求偏导数并令其等于0。
$$
\begin{align}
&\nabla_w L \overset{def}= \frac{\partial L}{\partial w}=w-\sum_{i=1}^{m}\alpha_i y^{(i)}x^{(i)}=0\\
&\nabla_b L \overset{def}= \frac{\partial L}{\partial b}=-\sum_{i=1}^{m}\alpha_i y^{(i)}=0\\
\end{align}
$$
得到：
$$
\begin{align}
& w=\sum_{i=1}^{m}\alpha_i y^{(i)}x^{(i)}\\
& \sum_{i=1}^m \alpha_i y^{(i)}=0
\end{align}
$$
从上面的式子中，可以得到$w$和$\alpha$的关系。将$w$替换为$\alpha$的表达式后的优化函数$\Psi(\alpha)$的表达式：
$$
\begin{align}
\Psi(\alpha)
&=\frac{1}{2}||w||^2_2-\sum_{i=1}^{m}\alpha_i[ y^{(i)}(w^T x^{(i)}+b) -1 ]\\
&=\frac{1}{2}w^T w-\sum_{i=1}^m\alpha_iy^{(i)}w^Tx^{(i)}-\sum_{i=1}^m\alpha_i y^{(i)}b+\sum_{i=1}^{m}\alpha_i\\
&=\frac{1}{2}w^T  ( \sum_{i=1}^{m}\alpha_i y^{(i)}x^{(i)})-w^T \sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}-\sum_{i=1}^m\alpha_i y^{(i)}b+\sum_{i=1}^{m}\alpha_i\\
&=-\frac{1}{2}w^T  ( \sum_{i=1}^{m}\alpha_i y^{(i)}x^{(i)})-\sum_{i=1}^m\alpha_i y^{(i)}b+\sum_{i=1}^{m}\alpha_i\\
&=-\frac{1}{2}(\sum_{i=1}^{m}\alpha_i y^{(i)}x^{(i)})^T  ( \sum_{i=1}^{m}\alpha_i y^{(i)}x^{(i)})-b\sum_{i=1}^m\alpha_i y^{(i)}+\sum_{i=1}^{m}\alpha_i\\
&=-\frac{1}{2} \lgroup \sum_{i=1}^{m}\alpha_i y^{(i)}(x^{(i)})^T  \rgroup  \lgroup \sum_{i=1}^{m}\alpha_i y^{(i)}x^{(i)}\rgroup -b\sum_{i=1}^m\alpha_i y^{(i)}+\sum_{i=1}^{m}\alpha_i\\
&=-\frac{1}{2}\lgroup \sum_{i=1}^{m}\alpha_i y^{(i)}(x^{(i)})^T \rgroup  \lgroup \sum_{i=1}^{m}\alpha_i y^{(i)}x^{(i)} \rgroup +\sum_{i=1}^{m}\alpha_i\\
&=-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^m\lgroup \alpha_iy^{(i)}(x^{(i)})^T \alpha_jy^{(j)}x^{(j)}  \rgroup+\sum_{i=1}^m \alpha_i\\
&=\sum_{i=1}^m \alpha_i-\frac{1}{2}\sum_{i=1,j=1}^m \alpha_i\alpha_jy^{(i)}y^{(j)}(x^{(i)})^T x^{(j)}
\end{align}
$$
公式(17)~(18)利用了范数的定义$||w||^2_2=w^T w$；公式(18)-(22)使用了$w=\sum_{i=1}^m \alpha_iy^{(i)}x^{(i)}$合并同类项进行化简，由于常量的转置就是其本身，所以只有只有向量$x^{(i)}$被转置；公式(22)~(23)利用了$\sum_{i=1}^m \alpha_i y^{(i)}=0$；公式(23)~(24)利用：
$$
\begin{aligned}
\sum_{i=1}^n x_i \cdot \sum_{i=1}^my_i
&=(x_1+x_2+...+x_n)(y_1+y_2+...+y_n)\\
&=(x_1y_1+x_1y_2+...x_1y_n)+(x_2y_1+x_2y_2+...+x_2y_n)+...+(x_ny_1+x_ny_2+...+x_ny_n)\\
&=x_1y_1+x_1y_2+...x_1y_n+x_2y_1+x_2y_2+...+x_2y_n+...+x_ny_1+x_ny_2+...+x_ny_n\\
&=\sum_{i=1,j=1}^m x_i y_j
\end{aligned}
$$
**（2）求$\max_\limits{\alpha}\Psi(\alpha)$** 

从上面的推导，通过对$w,b$极小化以后，优化函数$\Psi(\alpha)$仅仅只有向量$\alpha$为参数，此时对$\Psi(\alpha)$求极大化的数学表达式为：
$$
\begin{align}
&\max_\alpha -\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i \alpha_jy^{(i)}y^{(j)}\left ( x^{(i)} \cdot x^{(j)} \right ) +\sum_{i=1}^m\alpha_i \\
&s.t. \quad \sum_{i=1}^m \alpha_i y^{(i)}=0 \notag \\
&\alpha_i \ge 0  ,\quad i =1,2,...,m  \notag
\end{align}
$$
等价的极小化问题如下：
$$
\begin{align}
&\min_\alpha \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i \alpha_jy^{(i)}y^{(j)}\left ( x^{(i)} \cdot x^{(j)} \right ) -\sum_{i=1}^m\alpha_i \\
&s.t. \quad \sum_{i=1}^m \alpha_i y^{(i)}=0 \notag \\
&\alpha_i \ge 0  ,\quad i =1,2,...,m  \notag
\end{align}
$$
这是一个关于$\alpha$向量的二次规划问题，可以使用通用的的二次规划算法来求解。然而，该问题的规模正比于训练样本数，在求解过程中开销很大。因此利用问题本身的特征，提出了很多高效算法如SMO算法。

假设通过SMO算法，得到了$\alpha$的最优解为$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_m^*)$，根据$w=\sum_{i=1}^m \alpha_i y^{(i)}x^{(i)}$，可得到对应的$w$的值：
$$
w^* = \sum_{i=1}^m \alpha_i^* y^{(i)} x^{(i)}
$$
对于**任意的支持向量**$(x^{s},y^{s})$，都有
$$
\begin{align}
& y^s(w^T x^s + b)=y^s( \sum_{i=1}^m \alpha_i y^{(i)}( x^{(i)})^T x^s+b)=1\\
\Rightarrow &b = y^s-\sum_{i=1}^m \alpha_i y^{(i)}( x^{(i)})^T x^s
\end{align}
$$
如果有$S$个支持向量，则对应的可以求出$S$个$b^*$，理论上这$S$个$b^*$都可以作为最终的结果。这里采用一种更加健壮的办法，求出所有$S$个支持向量所对应的$b^*$，然后将其平均值作为最终的结果。(注：对于严格线性可分的SVM，$b$的值是有唯一解的，所有支持向量对应的$b^*$都是一样的，这里这么写是为了和后面加入软间隔SVM算法描述一致)

根据KKT条件中对偶互补条件$\alpha^*(y^{(i)}(w^T x^{(i)}+b)-1)=0$，如果$a_i^*>0$则有$y^{(i)}(w^T x^{(i)}+b)=1$即该点是支持向量，如果$a_i^*=0$则有$y^{(i)}(w^T x^{(i)}+b)\gt 1$即该点已经被正确分类。



## 线性可分SVM算法

> 输入：线性可分数据集$D=\{(x^{(1)},y^{(i)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$,其中$x^{(i)}\in \mathbb{R}^n,y^{(i)}\in\{+1,-1\}$
>
> 输出：分离超平面和分类决策函数
>
> (1)构造约束优化问题
> $$
> \begin{aligned}
> &\min_\alpha \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i \alpha_jy^{(i)}y^{(j)}\left ( x^{(i)} \cdot x^{(j)} \right ) -\sum_{i=1}^m\alpha_i \\
> &s.t. \quad \sum_{i=1}^m \alpha_i y^{(i)}=0 \notag \\
> &\alpha_i \ge 0  ,\quad i =1,2,...,m  \notag
> \end{aligned}
> $$
> (2)利用二次规划优化算法或者SMO算法得到最优解$\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_m^*)$ 
>
> (3)计算$w^*=\sum_{i=1}^m \alpha_i^* y^{(i)} x^{(i)}$
>
> (4)找出所有的$S$个支持向量即$\alpha_i \gt 0$所对应的点$(x^s,y^s)$，计算每个点对应的$b^s = y^s-\sum_{i=1}^m \alpha_i y^{(i)}( x^{(i)})^T x^s$ ，对所有的$b^s$求和取平均值得到$b^*=\frac{1}{S}\sum_{i=1}^Sb^s$
>
> (5)求分离超平面为$w^*\cdot x + b^*=0$，分类决策函数为$f(x)=sign(w^*\cdot x +b^*)$ 

