# 决策树算法原理(三)

分类与回归树(classification and regression tree, CART)模型是应用广泛的决策树学习算法。CART由特征选择、树的生成以及树的剪枝三部分组成，既可以用于分类也可以用于回归。

CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART算法中决策树是二叉树，内部节点特征的取值为"是"和"否"，左分支的取值为"是"的分支，右分支是取值为"否"的分支。这样的决策树等价于递归地二分每个特征，将输入的特征空间划分为有限个单元，并在这些单元上确定预测的概率分布。

## 最优特征选择方法

在ID3算法中使用信息增益来选择特征，信息增益大的优先选择。为了减少信息增益容易选择特征值较多的特征的问题，在C4.5算法中采用了信息增益比来选择特征。但是无论ID3还是C4.5，都是基于信息论的熵模型，计算过程涉及了大量的对数运算。CART分类树算法使用基尼系数来代替信息增益比，即可以简化模型又不至于完全丢失熵模型的优点。基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这个信息增益(比)是相反的。

在分类问题中，假设有$K$个类别，第$k$个类别的概率为$p_k$，则基尼系数的表达式：

$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2
$$

如果是二分类问题，计算公式(1)可以进一步简化，假设属于第一个样本输出的概率是$p$，那么基尼系数的表达式为：

$$
Gini(p)=1-\sum_{k=1}^{2}p_k^2=1-p^2-(1-p)^2=2p(1-p)
$$

对于给定的样本$D$，假设有$K$个类别，第$k$个类别的数量为$C_k$，则样本$D$的基尼系数表达式为：

$$
Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2
$$

特别地，对于样本$D$，如果根据特征$A$的某个值$a$将数据集$D$划分为了$D_1$和$D_2$，则在特征$A$的条件下，$D$的基尼系数表达式为：

$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$

可以看出基尼系数表达式相比于熵模型的表达式，二次运算比对数运算简单得多，尤其是二类分类的计算。



## 特征处理的改进

### 连续值处理

对于CART分类树处理连续值的问题，其思想和C4.5是相同的，都是将连续的特征离散化。唯一的区别在于选择划分点时的度量方式不同：C4.5算法使用的是信息增益比，CART算法使用的是基尼系数。

如有$m$个样本的连续特征$A$，从小到达排列为$a_1,a_2,...,a_m$，则CART算法取相邻两样本的平均数，一共取得$m-1$个划分点，其中第$i$个划分点$T_i$表示为$T_i=\frac{a_i+a_{i+1}}{2}$。对于这$m-1$个点，分别计算以改点作为二元分类点时的基尼系数，选择基尼系数最小的点作为该连续特征的二元离散分类点。这样就可以将连续特征离散化。

需要注意的是：**与离散特征不同的是，如果当前节点是连续特征，则该属性后面还可以参与子节点的产生选择过程**。

### 离散值处理

在ID3算法或者C4.5算法中，如果某个特征$A$被选取建立决策树节点，如果它有$a_1,a_2,a_3$三种类别，那么在决策树上会建立一个三叉的节点，最后的决策树往往是一个多叉树。但是在CART决策树中采用的方法是不停的二分。

比如在CART决策树中会考虑将$A$分成$\{ a_1\}$和$\{a_2,a_3\}$，$\{ a_2\}$和$\{a_1,a_3\}，$$\{a_3\}$和$\{a_1,a_2\}$三种情况，找到基尼系数最小的组合如$\{ a_2\}$和$\{a_1,a_3\}$，然后建立二叉树节点，一个节点对应样本$a_2$，另一个节点对应是$\{a_1,a_3\}$的节点。由于这次并没有把特征$A$的值完全分开，后续我们还有机会在子节点中继续选择特征$A$来划分$a_1$和$a_3$。

这里和ID3或者C4.5不同，在ID3或者C4.5中，离散特征只会参与一次节点的建立。

## CART分类树的建立

CART算法由两步组成：①决策树的生成：基于训练数据集生成决策树，生成的决策树要尽量大；②决策树的剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，采用损失函数最小作为剪枝的标准。如下给出了CART分类树的建立算法：

> 输入：训练数据集$D$，基尼系数的阈值$\epsilon$，样本个数阈值
>
> 输出：决策树$T$
>
> (1)对于当前节点的数据集$D$，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归；
>
> (2)计算样本集$D$的基尼系数，如果基尼系数大于阈值，则返回决策子树，当前节点停止递归；
>
> (3)计算当前节点现有的各个特征的各个特征值对数据集$D$的基尼系数，对于离散值、连续值和缺失值的处理如上介绍；
>
> (4)在计算出来的各个特征值对数据集$D$的基尼系数中，选择基尼系数最小的特征$A$和对应的特征值$a$。根据这个最优特征和最优特征值，将数据集$D$划分为$D_1$和$D_2$，同时建立当前节点的左右节点，左节点的数据集$D_1$，右节点的数据集$D_2$。
>
> (5)对左右的子节点递归调用(1)~(4)，生成决策树。

使用生成的决策树做预测的时候，假设测试集里的样本$x$落到了某个叶子节点，而节点里面有很多的训练样本。则将这个预测样本预测为这个叶子节点里概率最大的类别。



## CART回归树的建立

CART回归树和CART分类树的建立算法大部分都是类似的，不同的地方主要有：

(1)分类树的样本输出是离散值，回归树的样本输出是连续值

(2)连续值的处理方法不同

(3)决策树建立后样本预测的方式不同

对于连续值的处理，CART分类树采用的是基尼系数的大小来度量特征的各个划分点的优劣情况。对于回归模型，采用的是常见的方差度量方式，选择第$j$个变量$x_j$和它取值$s$，作为划分变量和切分点，并定义两个区域：
$$
\begin{align}
D_1(j,s)=\{ x|x_j\le s \}\\
D_2(j,s)=\{ x|x_j\gt s \}
\end{align}
$$
求出使$D_1$和$D_2$各自集合的均方差最小，同时$D_1$和$D_2$的均方差之和最小所对应的特征和特征值划分点。表达式如下：
$$
\min_{j,s}[\min_{c_1}{\sum_{x_i\in D_1}(y^i-c_1)^2}+\min_{c_2}{\sum_{x_i\in D_2}(y^i-c_2)^2}]
$$
其中$c_1$是数据集$D_1$的样本输出均值，$c_2$是数据集$D_2$的样本输出均值。

对于决策树建立后做预测的方式，CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类型。由于回归树输出的不是类别，采用的预测方式是用最终叶子节点的均值或者中位数来作为预测结果。



## CART树的剪枝

由于决策时算法很容易对训练集过拟合，从而导致泛泛化能力差。为了解决这个问题，需要对CART树进行剪枝，类似与线性回归的正则化。具体地，剪枝(puring)就是从已生成的树上裁掉一些子树或者叶节点，并将其根节点或者父节点作为新的叶节点，从而简化分类树模型。

剪枝的方法主要有两种：一种是预剪枝，即在生产决策树的时候就决定是否剪枝；另一种是后剪枝，即先生成决策树，在通过交叉验证来剪枝。

CART的剪枝策略主要后剪枝，剪枝采用损失函数作为度量，在剪枝的过程中，对于任意的一颗子树，其损失函数为：
$$
C_\alpha(T_t)=C(T_t)+\alpha|T_t|
$$
其中$\alpha$为正则化参数，$C(T_t)$为训练数据的预测误差，分类树采用基尼系数度量，回归树采用均方差度量，$|T_t|$是子树$T$的叶子节点的数量。

当$\alpha=0$时即没有正则化，原始的生成树CART树即为最优子树。当 $\alpha =\infty$ 时即正则化强度达到最大，此时原始的生成树CART树的根节点组成的单节点数为最优子树。这是两种极端的情况。一般来说，$\alpha$越大，剪枝越厉害，生成的最优子树相比原始的的决策树越偏小。



对于节点$t$的任意子树$T_t$，如果没有剪枝，其损失函数是：
$$
C_\alpha(T_t)=C(T_t)+\alpha|T_t|
$$
如果对其进行剪枝，仅仅保留根节点，其损失函数是：
$$
C_\alpha(t)=C(t)+\alpha
$$
当$\alpha=0$或者$\alpha$很小时，$C_\alpha(T_t)<C_\alpha(t)$；当$\alpha$增大到一定的程度有$C_\alpha(T_t)=C_\alpha(t)$；当$\alpha$继续增大时不等式反向。当$\alpha$满足下式：
$$
\alpha = \frac{C(t)-C(T_t)}{|T_t|-1}
$$
$T_t$和$T$有相同的损失函数，但是$T$节点更少，因此可以对子树$T_t$进行剪枝——将它的子节点全部剪掉，变为一个叶子节点$T$。

> 输入：CART算法生成的决策树$T_0$
>
> 输出：最优决策树$T_\alpha$
>
> (1)设$k=0,T=T_0,\alpha=+\infty$
>
> (2)自下而上地对各个内部节点$t$计算$C(T_t)$，$|T_t|$以及
> $$
> \begin{align}
> g(t)&=\frac{C(t)-C(T_t)}{|T_t|-1}\\
> \alpha &=min(\alpha,g(t))
> \end{align}
> $$
> 这里$T_t$表示以$t$为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶节点个数
>
> (3)对$g(t)=\alpha$的内部节点$t$进行剪枝，并计算叶节点$t$的值：如果是分类树则选择概率最高的类别，如果是回归树则选择所有样本的均值。得到树$T$。
>
> (4)设$k=k+1,\alpha_k=\alpha,T_k=T$
>
> (5)如果$T_k$不是由根节点及两个叶节点独构成的树，则返回步骤(2)；否则令$T_k=T_n$
>
> (6)采用交叉验证法在子树序列$T_0,T_1,...,T_n$中选择最优子树$T_\alpha$。



**CART算法的缺点**：

(1)无论ID3、C4.5还是CART，在特征选择的时候都是选择一个特征来做分类决策，但是大多数情况，分类决策不仅仅由一个特征决定，而是由一组特征决定的。这样的决策树叫做多变量决策树

(2)如果样本发生一点点的改动，会导致树结构剧烈变动。



下面总结了常见的决策树算法有ID3、C4.5、CART等，给出了三者的对比。

| 算法 | 支持模型  | 树结构 |    特征选择     | 连续值处理 | 缺失值处理 | 是否剪枝 |
| :--: | :-------: | :----: | :-------------: | :--------: | :--------: | :------: |
| ID3  |   分类    | 多叉树 |    信息增益     |   不支持   |   不支持   |    否    |
| C4.5 |   分类    | 多叉树 |   信息增益比    |    支持    |    支持    |    是    |
| CART | 分类/回归 | 二叉树 | 基尼系数/均方差 |    支持    |    支持    |    是    |