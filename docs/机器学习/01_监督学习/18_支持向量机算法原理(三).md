# 支持向量机算法原理(三)

线性可分SVM的硬间隔最大和软间隔最大算法可以较好的处理线性可分数据，然而还是无法处理完全线性不可分的数据。

## 多项式回归的启发

假设有一个只有两个特征的多项式回归模型：
$$
h_\theta(x_1,x_2)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x^2_1+\theta_4x_2^2+\theta_5x_1x_2
$$
令$x_1=x_1,x_2=x_2,x_3=x_1^2,x_4=x_2^2,x_5=x_1x_2$，将式(1)转换为：
$$
h_\theta(x_1,x_2)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4+\theta_5x_5
$$
这样式(5)是一个五元线性回归，可以用线性回归的方法来求解。对于每个二元样本特征$(x_1,x_2)$，都可以按照上面方式得到一个五元样本特征$(1,x_1,x_2,x_1^2,x_2^2,x_1x_2)$，通过这个的变换，将原来不是线性回归的问题转变为了线性回归问题。

启发：对于在低维线性不可分的数据，在映射到高维以后，就变成了线性可分的了(如上面例子中二维的不是线性可分的数据，将其映射到五维之后，就变成了线性的数据)。对于SVM线性不可分的低维特征数据，将其映射到高维，就能线性可分。

对于线性可分SVM的优化目标函数：
$$
\begin{align}
\min_\alpha \quad & \frac{1}{2}\sum_{i=1,j=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)}-\sum_{i=1}^m\alpha_i\\
s.t.&\quad \sum_{i=1}^m \alpha_iy^{(i)}=0 \notag\\
&\quad 0\le \alpha_i \le C \notag
\end{align}
$$
其中内积$x^{(i)} \cdot x^{(j)}$是在低维空间进行，现在可定义一个低维特征空间到高维特征空间的映射$\phi$(如从二维到五维的映射)，将所有特征映射到一个更高的维度，从而使得数据在高维空间是线性可分的。那么优化目标函数变成：
$$
\begin{align}
\min_\alpha \quad & \frac{1}{2}\sum_{i=1,j=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)}\phi(x^{(i)})\cdot \phi(x^{(j)})-\sum_{i=1}^m\alpha_i\\
s.t.&\quad \sum_{i=1}^m \alpha_iy^{(i)}=0 \notag\\
&\quad 0\le \alpha_i \le C\notag
\end{align}
$$
式(4)相比与式(3)只是将内积$x^{(i)}\cdot x^{(j)}$替换为$\phi(x^{(i)})\cdot\phi(x^{(j)})$，将特征从低维空间映射到了高维空间。然而在很多情况下，映射成高维过程中维度是爆炸性增长的(如2维映射5维，3维映射到19维...)，导致直接在高维空间求内积是非常困难的。为了避免这个障碍，设想一个函数满足：
$$
K(x^{(i)},x^{(j)})=\phi(x^{(i)})\cdot \phi(x^{(j)})
$$
式(5)的好处在于$K(x^{(i)},x^{(j)})$的计算是在低维度特征空间中计算的，可以避免在高维特征空间内积的计算$\phi(x^{(i)})\cdot \phi(x^{(j)})$。这样我们既可以将数据映射到高维特征空间从而又避免了高维特征空间庞大的内积计算量。这样的函数便是核函数(Kernel Function)。



## 核函数

令$\mathcal{X}$为输入空间，$k(\cdot,\cdot)$是定义在$\mathcal{X}\times \mathcal{X}$上的对称函数，则$k$是核函数当且仅当对于任意数据$D=\{ x^{(1)},x^{(2)},...,x^{(m)}\}$，"核矩阵"(Kernel Matrix)$\mathrm{K}$总是半正定的：
$$
\mathrm{K}=
\begin{bmatrix}
k(x^{(1)},x^{(1)}) & \cdots & (x^{(1)},x^{(j)}) & \cdots &(x^{(1)},x^{(m)})\\
\vdots& \cdots & \vdots& \cdots &\vdots\\
k(x^{(i)},x^{(1)}) & \cdots & (x^{(i)},x^{(j)}) & \cdots &(x^{(i)},x^{(m)})\\
\vdots& \cdots & \vdots& \cdots &\vdots\\
k(x^{(m)},x^{(1)}) & \cdots & (x^{(m)},x^{(j)}) & \cdots &(x^{(m)},x^{(m)})\\
\end{bmatrix}\notag
$$
只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射$\phi$。换言之，任何一个核函数都隐式地定义了一个称为"再生核希尔伯特空间"(简称RKHS)的特征空间。

常见的核函数有：

**线性核函数**

线性核函数(Linear Kernel)即线性可分SVM，表达式为：
$$
K(x^{(i)},x^{(j)})=x^{(i)}\cdot x^{(j)}
$$
**多项式核函数**

多项式核函数是线性不可分SVM常用的核函数之一，表达式为：
$$
K(x^{(i)},x^{(j)})=(\gamma x^{(i)}\cdot x^{(j)}+r)^d
$$
其中$\gamma,r,d$都需要调参设定。

**径向基核函数**

径向基核函数即高斯核函数，是非线性SVM最主流的核函数，表达式为：
$$
K(x^{(i)},x^{(j)})=\exp(-\gamma||x^{(i)}-x^{(j)}||^2)
$$
其中$\gamma>0$需要调参设定。

**Sigmoid核函数**

Sigmoid核函数也是线性不可分SVM常用的核函数之一，表达式为：
$$
K(x^{(i)},x^{(j)})=tanh(\gamma x^{(i)}\cdot x^{(j)}+r)
$$
其中$\gamma,r$需要调参设定。





## 分类SVM学习算法

> 输入：训练数据$D=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)}),\}$，其中$x^{(i)}\in\mathbb{R}^n,y\in\{+1,-1\}$
>
> 输出：分离超平面和分类决策函数
>
> (1)选择适当的核函数$K(x^{(i)},x^{(j)})$和惩罚系数$C>0$，构造约束优化问题
> $$
> \begin{aligned}
> \min_{\alpha}&\sum_{i=1,j=1}^m \alpha_i\alpha_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)})-\sum_{i=1}^m\alpha_i\\
> s.t.&\quad \sum_{i=1}^m \alpha_i y^{(i)}=0\\
> &\quad 0\le \alpha_i \le C
> \end{aligned}
> $$
> (2)用SMO算法求出上式最小时对应的$\alpha$向量值$\alpha^*$
>
> (3)计算$w^*=\sum_{i=1}^m \alpha_i^*y^{(i)}\phi (x^{(i)})$
>
> (4)找出所有的$S$个支持向量即满足$0\lt \alpha_s\lt C$对应的样本$(x_s,y_s)$，通过$y_s(\sum_{i=1}^S \alpha_i y^{(i)}K(x^{(i)},x^s)+b)=1$，计算出每个支持向量$(x^{s},y^{s})$对应的$b^*_s=y^{(s)}-\sum_{i=1}^S \alpha_i y^{(i)}K(x^{(i)},x^{s})$，所有的$b_s^*$对应的平均值即为最终的$b^*=\frac{1}{S}\sum_{i=1}^S b^*_s$
>
> (5)求得分离超平面$\sum_{i=1}^m \alpha_i^*y^{(i)}K(x^{(i)},x)+b^*=0$，分类决策函数$f(x)=sign(\sum_{i=1}^m \alpha_i^*y^{(i)}K(x^{(i)},x)+b^*)$ 



**总结：**对于线性不可分的数据集，常用的做法是把样例特征映射到高维空间，但是一律映射到高维空间，这个维度是很庞大的。为了解决这个问题，引入了核函数，既可以将特征从低维到高维的转换，又可以在低维度上进行计算，避免了高维空间中复杂的计算，真正解决了SVM线性不可分的问题。