# 线性回归算法原理

线性回归可以说是机器学习中最基本的问题类型了，这里就对线性回归的原理和算法做一个小结。

## 线性回归模型

有$m$个样本，每个样本对应$n$维特征和一个输出结果，如下：

$$
(x_1^{(0)},x_2^{(0)},...,x_n^{(0)},y^{(0)}),(x_1^{(1)},x_2^{(1)},...,x_n^{(1)},y^{(1)}),...,(x_1^{(m)},x_2^{(m)},...,x_n^{(m)},y^{(m)})
$$

使用线性回归，那么对应的模型如下：

$$
h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_n x_n
$$

这个表达式可以进一步简化，假设$x_0=1$，那么$h_\theta(x_0,x_1,...,x_n)=\sum_{i=0}^{n}{\theta_i x_i}$

如果用矩阵形式表达更加简洁如下：

$$
h_\theta(X)=X\theta
$$

其中，假设函数$h_\theta(X)\in\mathbb{R}^m$，$\theta \in \mathbb{R}^n$，$X\in \mathbb{R}^{m\times n}$，$m$代表样本个数，$n$代表样本特征数。

一般对于线性回归，我们使用均方误差作为损失函数。损失函数的表示方法如下：

$$
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta(x_0,x_1,...,x_n)-y^{(i)})^2
$$

进一步用矩阵形式表达损失函数：

$$
J(\theta)=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)
$$


## 线性回归求解

针对上面的损失函数，我们常用的求解方法有两种：一种是 **最小二乘法**,一种是 **梯度下降法**。

如果采用最小二乘法，则$\theta$的结果公式如下：

$$
\theta = (X^TX)^{-1}X^TY
$$

如果采用梯度下降法，则$\theta$的迭代公式如下：

$$
\theta = \theta -\alpha X^T(X\theta-Y)
$$



## 线性回归的正则化

为了防止模型过拟合，在建立模型的时候经常需要加入正则化项，常用的有$L_1$正则化和$L_2$正则化。

### Lasso回归

$L_1$正则化的线性回归称为Lasso回归，在线性回归的损失函数上增加一个$L_2$正则化项，利用一个常数系数$\alpha$来调节损失函数的均方误差项和正则化权重，形式如下：
$$
J(\theta)=\frac{1}{2m}(X\theta-Y)^T(X\theta-Y)+\alpha||\theta||_1
$$
其中$m$是样本个数，$\alpha$是常数系数，需要进行调优，$||\theta||_1$是$L_1$范数。

Lasso回归可以使得一些特征的系数变小，甚至还使一些绝对值较小的系数为0，增强模型的泛化能力。

Lasso回归的求解方法一般有 **坐标下降法**(Coordinate Descent)和 **最小角回归法**(Least Angle Regression)。

### Ridge回归

$L_2$正则化的线性回归称为Ridge回归，在线性回归的损失函数上增加一个$L_2$正则化项，利用一个常数系数$\alpha$来调节损失函数的均方误差项和正则化权重，形式如下：

$$
J(\theta)=\frac{1}{2m}(X\theta-Y)^T(X\theta-Y)+\alpha||\theta||_2^2
$$

其中$\alpha$是常数系数，需要进行调优。$||\theta||^2_2$是$L_2$范数。

Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对比较稳定，但和Lasso回归相比，这会使得模型保留较多的特征，模型解释性差。

Ridge求解可以直接使用最小二乘法，推导形式和现行回归类似。

令$J(\theta)$的导数为0，得到：

$$
X^T(X\theta-Y)+\alpha\theta=0
$$

整理即可得到$\theta$的结果：

$$
\theta = (X^TX+\alpha I)^{-1}X^TY
$$

其中$I$为单位矩阵。



## 线性回归推广一：多项式回归

在原始的线性回归模型中$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n$如果这里不仅仅是$x$的一次方，比如增加二次方，那么回归模型就变成多项式归回。这里写一个只有两个特征的二次方多项式回归模型：

$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2+\theta_5x_1x_2
$$

令$x_0=1,x_1=x_1,x_2=x_2,x_3=x_1^2,x_4=x_2^2,x_5=x_1x_2$，这样上述公式可变为：

$$
h_\theta(x_1,x_2)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4+\theta_5x_5
$$

可以发现又变成了线性回归，这是一个五元线性回归，可以采用线性回归的方法求解。对于一个二元特征，我们变换得到了一个五元特征$(1,x_1,x_2,x_1^2,x_2^2,x_1x_2)$，通过这个变换，就可以将原来不是线性回归的函数变成线性回归。



## 线性回归推广二：广义线性回归

上面的多项式回归可以看做是对样本特征做了推广，现在对于输出结果$Y$做推广。如输出$Y$不满足和$X$的线性关系，但是$lnY$和$X$满足线性关系，模型函数如下：

$$
lnY = X\theta
$$

对每个样本的输出$Y$，可以使用$lnY$去对应，从而任然可以使用线性回归的算法处理这个问题。这里我们给出一般化的广义线性回归形式如下：

$$
g(Y)=X\theta    或者  Y=g^{-1}(X\theta)
$$

函数$g(\cdot)$是单调可微函数，通常称为 **联系函数**。


