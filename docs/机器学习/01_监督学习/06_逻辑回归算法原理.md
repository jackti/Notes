# 逻辑回归算法原理

逻辑回归是一个分类算法，可以处理二元分类以及多元分类。虽然名字里面有"回归"两字，却不是一个回归算法。但是它的原理里面却又着回归模型的影子。

线性回归的模型是求出输出特征向量$Y$和输入样本矩阵$X$之间的线性变换系数$\theta$，满足$Y=X\theta$。此时要求$Y$是连续的，所以这是一个回归模型。如果要求$Y$是离散的，那么需要对$Y$再做一次函数转换，变成$g(Y)$。令$g(Y)$的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到一个分类模型。

## 二元逻辑回归模型

对线性回归的结果做一个函数$g$的转换，可以转换为逻辑回归，这个函数$g$的一般取sigmoid函数，形式如下:

$$
g(z)=\frac{1}{1+e^{-z}}
$$

`sigmoid`函数具有的性质：

1. 当$z$趋于正无穷时，$g(z)$趋于1；当$z$趋于负无穷时，$g(z)$趋于0；
2. $g^{'}(z)=g(z)(1-g(z))$

令$g(z)$中的$z=\theta^Tx$，就得到二元逻辑回归模型的一般形式：

$$
h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^T x}}
$$

其中$x$是样本输入，$h_\theta(x)$为模型输出，可以理解为某一分类的概率大小，$\theta$为分类模型的参数。

对于模型的输出$h_\theta(x)$，让它和二元样本输出$y$有如下的对应关系：

+ 如果$h_\theta(x)>0.5$，即$\theta^T x>0$，则$y$为1；
+ 如果$h_\theta(x)<0.5$，即$\theta^T x<0$，则$y$为0；
+ 如果$h_\theta(x)=0.5$，即$\theta^T x=0$，是临界情况，模型无法确定分类。

## 二元逻辑回归推导

假设样本输出只有0或者1两个类别，那么有：

$$
P(y=1|x,\theta)=h_\theta(x)\\
P(y=0|x,\theta)=1-h_\theta(x)
$$

可以把上式的式子整合为一个公式，如下：

$$
P(y|x,\theta)=h_\theta(x)^y(1-h_\theta(x))^{1-y}
$$

这里的$y\in\{0,1\}$ 只能取值0或者1。如果$y=0$只保留$(1-h_\theta(x))^{1-y}$，如果$y=1$只保留$(h_\theta(x))^y$。

似然函数的表达式如下：

$$
L(\theta)=\prod_{i=1}^{m} (h_\theta(x^{(i)}))^{y^{(i)}}  (1-h_\theta(x^{(i)}))^{1-y^{(i)}}
$$

其中$m$是样本个数。对似然函数取对数取反的表达式，即为损失函数：

$$
J(\theta)=-logL(\theta)=-\sum_{i=1}^{m}(y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)})))
$$

求导过程：

$$
\begin{align}
\frac{\partial }{\partial \theta_j}J(\theta)
&=-\sum_{i=1}^{m}\{ y^{(i)}\frac{1}{h_\theta(x^{(i)}) }\frac{\partial}{\partial \theta_j}h_\theta(x^{(i)}) -(1-y^{(i)})\frac{1}{1-h_\theta(x^{(i)})} \frac{\partial}{\partial \theta_j}h_\theta(x^{(i)}) \} \\
&=-\sum_{i=1}^{m}\{ y^{(i)}\frac{1}{g(\theta^Tx^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^Tx^{(i)})} \}\frac{\partial}{\partial \theta_j}g(\theta^Tx^{(i)})\\
&=-\sum_{i=1}^{m}\{ y^{(i)}\frac{1}{g(\theta^Tx^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^Tx^{(i)})} \}g(\theta^Tx^{(i)})(1-g(\theta^Tx^{(i)}))\frac{\partial}{\partial \theta_j}\theta^Tx^{(i)}\\
&=-\sum_{i=1}^{m}(y^{(i)}(1-g(\theta^Tx^{(i)}))-(1-y^{(i)})g(\theta^Tx^{(i)}))x_j^{(i)}\\
&=-\sum_{i=1}^{m}(y^{(i)}-g(\theta^T x^{(i)}))x_{j}^{(i)}\\
&=\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
\end{align}
$$

如下公式迭代$\theta_j$即可：

$$
\theta_j = \theta_j -\alpha \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j
$$

可以用矩阵简化上式：

$$
\frac{\partial}{\partial\theta}J(\theta)=X^T(h_\theta(X)-Y)
$$

从而在梯度下降算法中向量$\theta$的迭代公式：

$$
\theta = \theta -\alpha X^T(h_\theta(X)-Y)
$$



## 多元逻辑回归推导

现实中很多多分类任务，有些二分类学习方法可以直接推广到多分类，但是很多情形下，我们基于一些策略，利用二分类学习器来解决多分类问题。

多分类学习的基本思路是"拆解法"——讲多分类任务拆为若干个二分类任务求解。最经典的拆分策略有三种：一对一(One vs. One 简称OvO)、一对其余(One vs. Rest，简称OvR)、多对多(Many vs. Many 简称 MvM)。

给定数据集

$$
D=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\},y^{(i)}\in \{ C_1,C_2,...,C_N\}
$$

+ OvO将这$N$个类别两两配对，产生$N(N-1)/2$个二分类器。如将类别$C_i$和$C_j$训练一个分类器，该分类器把$D$中$C_i$类别当做正例，$C_j$类别作为反例。在测试阶段，新样本将同时提交给所有分类器，预测得到$N(N-1)/2$个分类结果，最终通过投票把新样本预测为最多的类别。
+ OvR每次将一个类的样例作为正例，所有其他类的样例作为反例训练$N$个分类器。在测试的时候若仅有一个分类器预测为正例，则对应的类别标记为最终分类结果。若有多个分类器预测为正例，则通常考虑各个分类器的预测置信度，选择置信度最大的类别标记为分类结果。
+ MvM每次将若干个类作为正例，若干个其他类作为反例。显示OvO和OvR都是MvM的特例。MvM的正、反例构造必须有特殊的设计。一种最常用的MvM技术：纠错输出码(简称ECOC)。

针对二元逻辑回归有：

$$
P(y=1|x,\theta)=h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}\\
P(y=0|x,\theta)=1-h_\theta(x)=\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}}
$$

两式相除取对数有：

$$
ln\frac{P(y=1|x,\theta)}{P(y=0|x,\theta)}=\theta^Tx
$$

这个称为"对数几率"(log odds)，反映了$x$作为正例的相对可能性。假设现在有$K$元分类任务，根据二元逻辑回归的经验，可以得到：

$$
\begin{align}
ln\frac{P(y=1|x,\theta)}{P(y=K|x,\theta)}=\theta_1^{T}x \\
ln\frac{P(y=2|x,\theta)}{P(y=K|x,\theta)}=\theta_2^{T}x \\
...\\
ln\frac{P(y=K-1|x,\theta)}{P(y=K|x,\theta)}=\theta_{K-1}^{T}x \\
\end{align}
$$

又因为概率之和为1，所以有下式：

$$
\sum_{i=1}^{K}P(y=i|x,\theta)=1
$$

联立上述公式，可以得到：

$$
\begin{align}
P(y=k|x,\theta)&=\frac{e^{\theta_{k}^Tx}}{1+\sum_{i=1}^{K-1}e^{\theta_{k}^Tx}},k=1,2,...,K-1\\
P(y=K|x,\theta)&=\frac{1}{1+\sum_{i=1}^{K-1}e^{\theta_{k}^Tx}} 
\end{align}
$$





