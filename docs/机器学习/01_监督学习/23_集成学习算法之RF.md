# 集成学习算法之RF

在集成学习中，主要有两大类流派，一个是Boosting流派，它的特点是各个弱学习器之间有依赖关系；另一个是Bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。

随机森林是Bagging中的重要代表，可以很方便的并行训练。





## Bagging算法原理

Bagging的弱学习器之间没有Boosting那样的联系。它的特点在于"随机采样"。

自助采样法(boostrap sampling)给定包含$m$个样本的数据集$D$，对它进行采样产生数据集$D^*$：每次随机从$D$中挑选一个样本，将其拷贝放入$D^*$，然后再将该样本放回初始数据集$D$中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行$m$次后，就可以得到包含$m$个样本的数据集$D^*$ 。显然，$D$中有一部分样本会在$D^*$中多次出现，而另一部分样本不出现。样本在$m$次采样中始终不被采样到的概率是$(1-\frac{1}{m})^m$，取极限得到

$$
\lim_{m \rightarrow\infty}(1-\frac{1}{m})^m \rightarrow\frac{1}{e}\approx0.368
$$

即通过自助采样，初始数据集$D$中有$36.8\%$的样本未出现在才采样数据集$D^*$中。于是可以将$D^*$用作训练集，$D-D^*$用作测试集。这样实际评估的模型与期望评估的模型都使用$m$个训练样本，而仍有约$1/3$的、没在训练集中出现的样本用于测试。

Bagging对于弱学习器没有限制，这点和Adaboost 是一样的，但是最常用的一般也是决策树和神经网络。

Bagging对于分类问题，通常使用简单投票法，得到最多的类别或类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对多个弱学习器的回归结果进行算术平均得到最终的模型输出。

> 输入：样本数据集$D=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)}) \}$，弱学习器算法$G(x)$，迭代次数$T$
>
> 输出：最终的分类器$f(x)$
>
> (1)对于$t=1,2,...,T$
>
> ​	①对训练集进行自助采样，得到包含$m$个样本的采样集$D_t$
>
> ​	②用采样集$D_t$训练第$t$个弱学习器$G_t(x)$
>
> (2)若是分类算法，则进行投票；若是回归算法，则算术平均得到最终的模型输出





## 随机森林算法原理

随机森林(Random Forest)是Bagging算法的进化版，它的思想仍然是Bagging，但是进行了独有的改进。

RF使用了CART决策树作为弱学习器。在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树一般在**所有的$n$个特征中选择一个最优特征来做决策树的左右子树划分**，但是在RF中只是随机选择节点上的一部分样本特征，特征数量$n_{sub}<n$ ，在这$n_{sub}$中选择一个最优的特征来做决策树的左右子树划分。这样可以进一步增强模型的泛化能力。在实际应用中，一般会通过交叉验证调参来选择一个合适的$n_{sub}$值。

> 输入：样本数据集$D=\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)}) \}$，弱学习器算法$G(x)$，迭代次数$T$
>
> 输出：最终的分类器$f(x)$
>
> (1)对于$t=1,2,...,T$
>
> ​	①对训练集进行自助采样，得到包含$m$个样本的采样集$D_t$
>
> ​	②用采样集$D_t$训练第$t$个弱学习器$G_t(x)$，在训练决策树模型的节点时，在节点的所有样本特征中选择一部分样本特征，在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分
>
> (2)若是分类算法，则进行投票；若是回归算法，则算术平均得到最终的模型输出





## 随机森林的推广

由于RF在实际应用中有良好特征，于是基于RF，出现了很多改进变种算法，应用也很广泛。不光可以用于分类回归，还可以用于特征转换、异常点检测等等。

### Extremely Randomized Tree

 Extremely Randomized Tree(简称Extra Tree)是RF的一个变种，原理和RF几乎一模一样，仅有区别为：

(1)对于每个决策树的训练集，RF采用的是随机采样boostrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，而是每个决策树都使用原始训练集

(2)在选定划分特征后，RF的决策树和传统的决策树相同，会基于信息增益、基尼系数、均方差等原则选择一个最优的特征值划分点。但是extra trees则比较激进，会随机的选择一个特征值来划分决策树。

从上面可以看出，由于随机选择了特征值的划分点为，而不是最优点位，这样导致生成的决策树规模一般会大于RF所生成的决策树。这样地话，模型的方差相对于RF进一步减小，但是偏差进一步增大。在某些时候，extra trees的泛化能力比RF更好。



### Totally Random Trees Embedding

Totally Random Trees Embedding(简称TRTE)是一种非监督学习的数据转化方法，能够将低维的数据集映射到高维，从而让映射到高维的数据更好的运用于分类回归问题。在支持向量机中运用了核函数的方法将数据集映射到高维，此处的TRTE提供了另外一种方法。

TRTE在数据转化的过程中也使用了类似RF的方法，建立$T$个决策树来拟合数据。当决策树建立完成以后，数据集里的每个数据在$T$个决策树中叶子节点的位置也定下来了。

现有3棵决策树，每棵决策树有5个叶子节点，某个数据$x$划分到了第一棵决策树的第二个叶子节点，第二棵树的第三个叶子节点，第三棵决策树的第五个叶子节点。则$x$映射后的特征编码为$(0,1,0,0,0, \quad 0,0,1,0,0 \quad 0,0,0,0,1)$共有15个维度的高维特征。映射到高维特征后，就可以继续使用监督学习的各种分类回归算法。



### Isolation Forest

Isolation Forest(简称IForest)是一种异常点检测的方法，使用了类似RF的方法来检测异常点。

IForest对训练数据进行随机采样，但是采样个数不需要和RF一样（对于RF采样的样本数等于数据集个数）IForest不需要采样那么多，一般其采样个数远远小于训练集个数。因为现在的目的是异常点检测，只需要部分的样本，一般就可以将异常点区别出来。

IForest在建立每一个决策树的时候，和RF不同的是，采用随机选择一个划分特征，对划分特征随机选择一个划分阈值。

另外IForest一般会选择一个比较小的最大决策树深度`max_depth`，原因是同样本采集，对于少量的异常点检测问题一般不需要大规模的决策树。

对异常点的判别，则是将测试样本点$x$拟合到$T$课决策树。计算在每课决策树上该样本点的叶子节点深度$h_t(x)$，从而可以计算出平均高度$h(x)=\frac{1}{T}\sum_{t=1}^Th_t(x)$。此时用下面公式计算样本点$x$的异常概率：

$$
s(x,m)=2^{-\frac{h(x)}{c(m)}}
$$

其中$m$为样本个数。$c(m)$的表达式为：

$$
c(m)=2\ln(m-1)+\xi-2\cdot\frac{m-1}{m}
$$

其中$\xi$为欧拉常数，$s(x,m)$的取值范围是[0,1]，取值越接近1，则异常点的概率也越大。





## 随机森林小结

RF的主要优点：

（1）训练可以高度并行化，对于大样本训练速度有优势

（2）由于可以随机选择决策树节点划分特征，这样在特征维度很高的时候仍能高校训练模型

（3）在训练后可以给出各个特征的对于输出的重要性

（4）采用随机采样，训练得到的模型方差小，泛化能力强

（5）对部分特征缺失不敏感

 RF的主要缺点：

（1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合

（2）取值划分比较多的特征容易对RF的决策产生较大影响，从而影响拟合效果

 