
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="采用Mkdocs-material生成的文档管理网站支持的markdown语法，包括传统语法和扩展语法">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>注意力机制 - 我的知识笔记</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.9647289d.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="pink">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="我的知识笔记" class="md-header__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            我的知识笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              注意力机制
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../NLP/00_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E8%8B%B1%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86/" class="md-tabs__link">
        NLP
      </a>
    </li>
  

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-tabs__link">
        机器学习
      </a>
    </li>
  

  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-tabs__link md-tabs__link--active">
        深度学习
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="我的知识笔记" class="md-nav__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    我的知识笔记
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          NLP
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          NLP
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/00_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E8%8B%B1%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86/" class="md-nav__link">
        文本挖掘之英文预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/01_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86/" class="md-nav__link">
        文本挖掘之中文预处理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/02_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E5%88%86%E8%AF%8D%E5%8E%9F%E7%90%86/" class="md-nav__link">
        文本挖掘之分词原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/03_%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98%E4%B9%8B%E5%90%91%E9%87%8F%E5%8C%96/" class="md-nav__link">
        文本挖掘之向量化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/04_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLSI/" class="md-nav__link">
        主题模型之LSI
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/05_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BNMF/" class="md-nav__link">
        主题模型之NMF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/06_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BpLSA/" class="md-nav__link">
        主题模型之pLSA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/07_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLDA%28%E4%B8%80%29/" class="md-nav__link">
        主题模型之LDA(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/08_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLDA%28%E4%BA%8C%29/" class="md-nav__link">
        主题模型之LDA(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/09_%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B9%8BLDA%28%E4%B8%89%29/" class="md-nav__link">
        主题模型之LDA(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/10_%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bword2vec%28%E4%B8%80%29/" class="md-nav__link">
        词向量之word2vec(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/11_%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bword2vec%28%E4%BA%8C%29/" class="md-nav__link">
        词向量之word2vec(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/12_%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bword2vec%28%E4%B8%89%29/" class="md-nav__link">
        词向量之word2vec(三)
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_1" type="checkbox" id="__nav_3_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_1">
          01 监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="01 监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          01 监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-nav__link">
        数学符号
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E6%80%BB%E7%BB%93/" class="md-nav__link">
        线性模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/01_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" class="md-nav__link">
        最小二乘法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/02_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" class="md-nav__link">
        梯度下降算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/03_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8E%9F%E7%90%86/" class="md-nav__link">
        交叉验证原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/04_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="md-nav__link">
        模型评估
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/05_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        线性回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/06_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        逻辑回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/07_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        决策树算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/08_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        决策树算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/09_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        决策树算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/10_%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        近邻算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/11_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        朴素贝叶斯算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/12_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        最大熵算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/13_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        最大熵算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/14_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        最大熵算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/15_%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        感知机算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/16_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        支持向量机算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/17_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        支持向量机算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/18_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        支持向量机算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/19_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E5%9B%9B%29/" class="md-nav__link">
        支持向量机算法原理(四)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/20_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%94%29/" class="md-nav__link">
        支持向量机算法原理(五)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/21_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        集成学习算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/22_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BAdaboost/" class="md-nav__link">
        集成学习算法之Adaboost
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/23_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BRF/" class="md-nav__link">
        集成学习算法之RF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/24_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/" class="md-nav__link">
        集成学习算法之GBDT
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_2">
          02 无监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="02 无监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          02 无监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/25_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BK-Means/" class="md-nav__link">
        聚类算法之K-Means​
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/26_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BBIRCH/" class="md-nav__link">
        聚类算法之BIRCH
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/27_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BDBSCAN/" class="md-nav__link">
        聚类算法之DBSCAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/28_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BMean%20Shift/" class="md-nav__link">
        聚类算法之Mean Shift
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/29_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E8%B0%B1%E8%81%9A%E7%B1%BB/" class="md-nav__link">
        聚类算法之谱聚类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/30_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BPCA/" class="md-nav__link">
        降维算法之PCA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/31_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLDA/" class="md-nav__link">
        降维算法之LDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/32_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BIsomap/" class="md-nav__link">
        降维算法之Isomap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/33_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLLE/" class="md-nav__link">
        降维算法之LLE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/34_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BApriori/" class="md-nav__link">
        关联算法之Apriori
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/35_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BFP-Tree/" class="md-nav__link">
        关联算法之FP-Tree
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/36_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        推荐算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/37_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/" class="md-nav__link">
        推荐算法之矩阵分解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/38_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BSimRank/" class="md-nav__link">
        推荐算法之SimRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/39_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BPersonalRank/" class="md-nav__link">
        推荐算法之PersonalRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/40_EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        EM算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/41_%E5%88%86%E8%A7%A3%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        分解机算法原理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="深度学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        神经网络基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_DNN%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        DNN前馈神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        CNN卷积神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        RNN循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_%E5%9F%BA%E4%BA%8E%E9%97%A8%E6%8E%A7%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        基于门控的循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%281%29/" class="md-nav__link">
        神经网络优化概述(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%282%29/" class="md-nav__link">
        神经网络优化概述(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" class="md-nav__link">
        网络正则化
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          注意力机制
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        注意力机制
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    注意力分布
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    加权平均
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    注意力机制的变体
  </a>
  
    <nav class="md-nav" aria-label="注意力机制的变体">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    硬性注意力
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    健值对注意力
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    多头注意力
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    自注意力模型
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    注意力分布
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    加权平均
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    注意力机制的变体
  </a>
  
    <nav class="md-nav" aria-label="注意力机制的变体">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    硬性注意力
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    健值对注意力
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    多头注意力
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    自注意力模型
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="_1">注意力机制<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<p>注意力是一种人类不可或缺的复杂认知能力，指人可以在关注一些信息的同时忽略另一些信息的选择能力，注意力一般分为两种：</p>
<p>（1）自上而下的有意识的注意力，称为聚焦式注意力(<code>Focus Attention</code>)，聚焦式注意力是指有预定目的、依赖任务的，主动有意识得聚焦于某一对象的注意力，后面提到的注意力通常都是指自上而下的聚焦式注意力。</p>
<p>（2）自上下而上的无意识得注意力，称为基于显著性的注意力(<code>Saliency-Based Attention</code>)，基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关。如果一个对象的刺激信息不同于其周围信息，一种无意识的"赢者通吃"(<code>Winner-Take-All</code>)或者门控(<code>Gating</code>)机制就可以把注意力转向这个对象。不管这些注意力是有意还是无意，大部分的人脑活动都需要依赖注意力，比如记忆信息、阅读或思考等。</p>
<p>在目前的神经网络模型中，可以将最大汇聚(<code>Max Pooling</code>)、门控(<code>Gating</code>)机制近似地看作自下而上的基于显著性的注意力机制。</p>
<p>除此之外，自 上而下的聚焦式注意力也是一种有效的信息选择方式.以阅读理解任务为例，给 定一篇很长的文章，然后就此文章的内容进行提问.提出的问题只和段落中的一 两个句子相关，其余部分都是无关的.为了减小神经网络的计算负担，只需要把 相关的片段挑选出来让后续的神经网络来处理，而不需要把所有文章内容都输
入给神经网络.</p>
<blockquote>
<p>聚焦式注意力一般会随着环境、情景或任务的不同而选择不同的信息.比如当要从人群中寻找某个人时，我们会专注于每个人的脸部;而当要统计人群的人数时，我们只需要专注于每个人的轮廓.</p>
</blockquote>
<p>在计算能力有限的情况下，注意力机制作为一种资源分配方案，将有限的计算资源用来处理更重要的信息，是解决信息超载问题主要手段。用<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{X}=[\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N]\in \mathbb{R}^{D\times N}</span><script type="math/tex">\boldsymbol{X}=[\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N]\in \mathbb{R}^{D\times N}</script></span>表示<span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>组输入信息，其中<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>维向量<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{x}_n\in\mathbb{R}^D,n\in [1,\cdots,N]</span><script type="math/tex">\boldsymbol{x}_n\in\mathbb{R}^D,n\in [1,\cdots,N]</script></span>表示一组输入信息，为了节省计算资源，不需要将所有信息都输入神经网络，只需要从<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{X}</span><script type="math/tex">\boldsymbol{X}</script></span>中选择一些和任务相关的信息。注意力机制的计算可以分为两步：
1. 在所有输入信息上计算注意力分布
2. 根据注意力分布来计算输入信息的加权平均</p>
<h2 id="_2">注意力分布<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>为了从<span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>个输入向量<span class="arithmatex"><span class="MathJax_Preview">[\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N]</span><script type="math/tex">[\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N]</script></span>中选择出和某个特定任务相关的信息，需要引入一个和任务相关的表示，称为查询向量(<code>Query Vector</code>)并通过一个打分函数来计算每个输入向量和查询向量之间的相关性.</p>
<p>给定一个和任务相关的查询向量<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{q}</span><script type="math/tex">\boldsymbol{q}</script></span>，用注意力变量<span class="arithmatex"><span class="MathJax_Preview">z\in [1,\cdots,N]</span><script type="math/tex">z\in [1,\cdots,N]</script></span>来表示被选择信息的索引位置，即<span class="arithmatex"><span class="MathJax_Preview">z=n</span><script type="math/tex">z=n</script></span>表示选择了第<span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>个输入向量，为了计算方便，采用一种"软性"的信息选择机制。首先计算在给定<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{q}</span><script type="math/tex">\boldsymbol{q}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{X}</span><script type="math/tex">\boldsymbol{X}</script></span>下，选择第<span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>个输入向量的概率<span class="arithmatex"><span class="MathJax_Preview">\alpha_n</span><script type="math/tex">\alpha_n</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\alpha_n 
&amp;= p(z=n|\boldsymbol{X},\boldsymbol{q}) \\
&amp;=\operatorname{softmax}(s(\boldsymbol{x}_n,\boldsymbol{q}))\\
&amp;=\frac{\operatorname{exp}(s(\boldsymbol{x}_n,\boldsymbol{q}))}
{\sum_{j=1}^N \operatorname{exp}(s(\boldsymbol{x}_j,\boldsymbol{q})) }    
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\alpha_n 
&= p(z=n|\boldsymbol{X},\boldsymbol{q}) \\
&=\operatorname{softmax}(s(\boldsymbol{x}_n,\boldsymbol{q}))\\
&=\frac{\operatorname{exp}(s(\boldsymbol{x}_n,\boldsymbol{q}))}
{\sum_{j=1}^N \operatorname{exp}(s(\boldsymbol{x}_j,\boldsymbol{q})) }    
\end{aligned}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">\alpha_n</span><script type="math/tex">\alpha_n</script></span>称为注意力分布，<span class="arithmatex"><span class="MathJax_Preview">s(\boldsymbol{x},\boldsymbol{q})</span><script type="math/tex">s(\boldsymbol{x},\boldsymbol{q})</script></span>为注意力打分函数，其中可以使用以下几种方式来计算</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>加权模型</td>
<td><span class="arithmatex"><span class="MathJax_Preview">s(\boldsymbol{x},\boldsymbol{q})=\boldsymbol{v}^\top\operatorname{tanh(\boldsymbol{W}\boldsymbol{x}+\boldsymbol{U}\boldsymbol{q})}</span><script type="math/tex">s(\boldsymbol{x},\boldsymbol{q})=\boldsymbol{v}^\top\operatorname{tanh(\boldsymbol{W}\boldsymbol{x}+\boldsymbol{U}\boldsymbol{q})}</script></span></td>
</tr>
<tr>
<td>点积模型</td>
<td><span class="arithmatex"><span class="MathJax_Preview">s(\boldsymbol{x},\boldsymbol{q})=\boldsymbol{x}^\top \boldsymbol{q}</span><script type="math/tex">s(\boldsymbol{x},\boldsymbol{q})=\boldsymbol{x}^\top \boldsymbol{q}</script></span></td>
</tr>
<tr>
<td>缩放点模型</td>
<td><span class="arithmatex"><span class="MathJax_Preview">s(\boldsymbol{x},\boldsymbol{q})=\frac{\boldsymbol{x}^\top \boldsymbol{q}}{\sqrt{D}}</span><script type="math/tex">s(\boldsymbol{x},\boldsymbol{q})=\frac{\boldsymbol{x}^\top \boldsymbol{q}}{\sqrt{D}}</script></span></td>
</tr>
<tr>
<td>双线性模型</td>
<td><span class="arithmatex"><span class="MathJax_Preview">s(\boldsymbol{x},\boldsymbol{q})=\boldsymbol{x}^\top \boldsymbol{W} \boldsymbol{q}</span><script type="math/tex">s(\boldsymbol{x},\boldsymbol{q})=\boldsymbol{x}^\top \boldsymbol{W} \boldsymbol{q}</script></span></td>
</tr>
</tbody>
</table>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{W},\boldsymbol{U},\boldsymbol{v}</span><script type="math/tex">\boldsymbol{W},\boldsymbol{U},\boldsymbol{v}</script></span>为可学习的参数，<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>为输入向量的维度.</p>
<p>理论上，加性模型和点积模型的复杂度差不多，但是点积模型在实现上可以更好地利用矩阵乘积，从而计算效率更高.</p>
<p>当输入向量的维度<span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>比较高时，点积模型的值通常有比较大的方差，从而导致<span class="arithmatex"><span class="MathJax_Preview">\operatorname{Softmax}</span><script type="math/tex">\operatorname{Softmax}</script></span>函数的梯度会比较小。因此，缩放点积模型可以比较好地解决这个问题。</p>
<p>双线性模型是一种泛化的点积模型，当<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{W}=\boldsymbol{U}^\top \boldsymbol{U}</span><script type="math/tex">\boldsymbol{W}=\boldsymbol{U}^\top \boldsymbol{U}</script></span>双线性模型可以写为<span class="arithmatex"><span class="MathJax_Preview">s(\boldsymbol{x},\boldsymbol{q})=\boldsymbol{x}\boldsymbol{U}^\top \boldsymbol{V}\boldsymbol{q}=(\boldsymbol{U}\boldsymbol{x})^\top (\boldsymbol{V}\boldsymbol{q})</span><script type="math/tex">s(\boldsymbol{x},\boldsymbol{q})=\boldsymbol{x}\boldsymbol{U}^\top \boldsymbol{V}\boldsymbol{q}=(\boldsymbol{U}\boldsymbol{x})^\top (\boldsymbol{V}\boldsymbol{q})</script></span>即分别对<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{x}</span><script type="math/tex">\boldsymbol{x}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{q}</span><script type="math/tex">\boldsymbol{q}</script></span>进行线性变换后计算点积，相比点积模型，双线性模型在计算相似度时引入了非对称性.</p>
<h2 id="_3">加权平均<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<p>注意力分布<span class="arithmatex"><span class="MathJax_Preview">\alpha_n</span><script type="math/tex">\alpha_n</script></span>可以解释为在给定任务相关的查询<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{q}</span><script type="math/tex">\boldsymbol{q}</script></span>时，第<span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>个输入向量受关注的程度，可以采用一种"软性"的信息选择机制对输入信息进行汇总，即</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\operatorname{att}(\boldsymbol{X},\boldsymbol{q})
&amp;=\sum_{n=1}^N \alpha_n \boldsymbol{x}_n\\
&amp;=\mathbb{E}_{\boldsymbol{z} \sim p(\boldsymbol{z}| \boldsymbol{X},\boldsymbol{q})}[\boldsymbol{x}_z]
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{att}(\boldsymbol{X},\boldsymbol{q})
&=\sum_{n=1}^N \alpha_n \boldsymbol{x}_n\\
&=\mathbb{E}_{\boldsymbol{z} \sim p(\boldsymbol{z}| \boldsymbol{X},\boldsymbol{q})}[\boldsymbol{x}_z]
\end{aligned}
</script>
</div>
<p>注意力机制可以单独使用，但更多地用作神经网络中的一个组件.</p>
<h2 id="_4">注意力机制的变体<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<p>除了上面介绍的基本模式外，注意力机制还存在一些变化的模型。</p>
<h3 id="_5">硬性注意力<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<p>前面的注意力是软性注意力，其选择地信息是所有输入向量在注意力分布下的期望。此外还有一种注意力只关注某一个输入向量，叫做硬注意力机制。硬注意力机制有两种实现方式：
（1）一种是选取最高概率的一个输入向量，即</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{att}(\boldsymbol{X},\boldsymbol{q})
=\boldsymbol{x}_{\hat{n}}
</div>
<script type="math/tex; mode=display">
\operatorname{att}(\boldsymbol{X},\boldsymbol{q})
=\boldsymbol{x}_{\hat{n}}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">\hat{n}</span><script type="math/tex">\hat{n}</script></span>为概率最大的输入向量下标，即<span class="arithmatex"><span class="MathJax_Preview">\hat{n}=\underset{n=1}{\arg \max } \alpha_{n}</span><script type="math/tex">\hat{n}=\underset{n=1}{\arg \max } \alpha_{n}</script></span></p>
<p>（2）另一种硬性注意力可以通过在注意力分布上随机采样的方式实现。</p>
<p>硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息，使得最终的损失函数与注意力分布之间的函数关系不可导，无法使用反向传播算 法进行训练.因此，硬性注意力通常需要使用强化学习来进行训练.为了使用反向传播算法，一般使用软性注意力来代替硬性注意力.</p>
<h3 id="_6">健值对注意力<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<p>更一般地，我们可以用键值对(<code>key-value pair</code>)格式来表示输入信息，其中“键”用来计算注意力分布<span class="arithmatex"><span class="MathJax_Preview">\alpha_n</span><script type="math/tex">\alpha_n</script></span>，“值”用来计算聚合信息.用<span class="arithmatex"><span class="MathJax_Preview">(\boldsymbol{K},\boldsymbol{V}) = [(\boldsymbol{k}_1,\boldsymbol{v}_1),⋯,(\boldsymbol{k}_N,\boldsymbol{v}_N)]</span><script type="math/tex">(\boldsymbol{K},\boldsymbol{V}) = [(\boldsymbol{k}_1,\boldsymbol{v}_1),⋯,(\boldsymbol{k}_N,\boldsymbol{v}_N)]</script></span>表示<span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>组输入信息，给定任务相关的查询向量<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{q}</span><script type="math/tex">\boldsymbol{q}</script></span>时，注意力函数为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\operatorname{att}((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{q}) &amp;=\sum_{n=1}^{N} \alpha_{n} \boldsymbol{v}_{n} \\
&amp;=\sum_{n=1}^{N} \frac{\exp \left(s\left(\boldsymbol{k}_{n}, \boldsymbol{q}\right)\right)}{\sum_{j} \exp \left(s\left(\boldsymbol{k}_{j}, \boldsymbol{q}\right)\right)} \boldsymbol{v}_{n},
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{att}((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{q}) &=\sum_{n=1}^{N} \alpha_{n} \boldsymbol{v}_{n} \\
&=\sum_{n=1}^{N} \frac{\exp \left(s\left(\boldsymbol{k}_{n}, \boldsymbol{q}\right)\right)}{\sum_{j} \exp \left(s\left(\boldsymbol{k}_{j}, \boldsymbol{q}\right)\right)} \boldsymbol{v}_{n},
\end{aligned}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">s(\boldsymbol{k}_n,\boldsymbol{q})</span><script type="math/tex">s(\boldsymbol{k}_n,\boldsymbol{q})</script></span>为打分函数。当<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{K}=\boldsymbol{V}</span><script type="math/tex">\boldsymbol{K}=\boldsymbol{V}</script></span>时，键值对模式就等价于普通的注意力机制.</p>
<p><img _="," alt="image" height="80%" src="../assets/image-20210123201346323.png" width="80%" /></p>
<p><img _="," alt="image" height="80%" src="../assets/image-20210626190228372.png" width="80%" /></p>
<h3 id="_7">多头注意力<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h3>
<p>多头注意力(<code>Multi-Head Attention</code>)是利用多个查询<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{Q}=[\boldsymbol{q}_1,\cdots,\boldsymbol{q}_M]</span><script type="math/tex">\boldsymbol{Q}=[\boldsymbol{q}_1,\cdots,\boldsymbol{q}_M]</script></span>来并行地输从输入信息中选取多组信息，每个注意力关注输入信息的不同部分</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{att}((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{Q})=\operatorname{att}\left((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{q}_{1}\right) \oplus \cdots \oplus \operatorname{att}\left((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{q}_{M}\right)
</div>
<script type="math/tex; mode=display">
\operatorname{att}((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{Q})=\operatorname{att}\left((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{q}_{1}\right) \oplus \cdots \oplus \operatorname{att}\left((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{q}_{M}\right)
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">\oplus</span><script type="math/tex">\oplus</script></span>表示向量拼接.</p>
<h2 id="_8">自注意力模型<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h2>
<p>当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，如图所示</p>
<p><img _="," alt="image" height="80%" src="../assets/image-20210125231949944.png" width="80%" /></p>
<p>基于卷积或循环网络的序列编码都是一种局部的编码方式，只建模了输入信息的局部依赖关系.虽然循环网络理论上可以建立长距离依赖关系，但是由于 信息传递的容量以及梯度消失问题，实际上也只能建立短距离依赖关系。</p>
<p>如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法:(1)一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互;(2)另一种方 法是使用全连接网络.全连接网络是一种非常直接的建模远距离依赖的模型，但是无法处理变长的输入序列.不同的输入长度，其连接权重的大小也是不同的. 这时我们就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型(<code>Self-Attention Model</code>)</p>
<p>为了提高模型能力，自注意力模型经常采用查询-键-值(<code>Query-Key-Value， QKV</code>)模式，其计算过程如图8.4所示，其中红色字母表示矩阵的维度.</p>
<p><img _="," alt="image" height="80%" src="../assets/image-20210125233301077.png" width="80%" /></p>
<p>假设输入序列为<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{X}= [\boldsymbol{x}_1,⋯,\boldsymbol{x}_N] ∈ \mathbb{R}^{D_x \times N}</span><script type="math/tex">\boldsymbol{X}= [\boldsymbol{x}_1,⋯,\boldsymbol{x}_N] ∈ \mathbb{R}^{D_x \times N}</script></span>，输出序列为<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{H} = [\boldsymbol{h}_1,⋯,\boldsymbol{h}_𝑁] ∈ \mathbb{R}^{𝐷_v \times 𝑁}</span><script type="math/tex">\boldsymbol{H} = [\boldsymbol{h}_1,⋯,\boldsymbol{h}_𝑁] ∈ \mathbb{R}^{𝐷_v \times 𝑁}</script></span> ，自注意力模型的具体计算过程如下:</p>
<p>(1)对于每个输入<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{x}_i</span><script type="math/tex">\boldsymbol{x}_i</script></span>，我们首先将其线性映射到三个不同的空间，得到查询向量<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{q}_i \in \mathbb{R}^{D_k}</span><script type="math/tex">\boldsymbol{q}_i \in \mathbb{R}^{D_k}</script></span> 、键向量<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{k}_i \in \mathbb{R}^{D_k}</span><script type="math/tex">\boldsymbol{k}_i \in \mathbb{R}^{D_k}</script></span> 和值向量<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{v}_i \in \mathbb{R}^{D_v}</span><script type="math/tex">\boldsymbol{v}_i \in \mathbb{R}^{D_v}</script></span></p>
<p>对于整个输入序列<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{X}</span><script type="math/tex">\boldsymbol{X}</script></span>，线性映射过程可以简写为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{array}{l}
\boldsymbol{Q}=\boldsymbol{W}_{q} \boldsymbol{X} \in \mathbb{R}^{D_{k} \times N} \\
\boldsymbol{K}=\boldsymbol{W}_{k} \boldsymbol{X} \in \mathbb{R}^{D_{k} \times N} \\
\boldsymbol{V}=\boldsymbol{W}_{v} \boldsymbol{X} \in \mathbb{R}^{D_{v} \times N}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
\boldsymbol{Q}=\boldsymbol{W}_{q} \boldsymbol{X} \in \mathbb{R}^{D_{k} \times N} \\
\boldsymbol{K}=\boldsymbol{W}_{k} \boldsymbol{X} \in \mathbb{R}^{D_{k} \times N} \\
\boldsymbol{V}=\boldsymbol{W}_{v} \boldsymbol{X} \in \mathbb{R}^{D_{v} \times N}
\end{array}
</script>
</div>
<p>其中 <span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{W}_{q} \in \mathbb{R}^{D_{k} \times D_{x}}, \boldsymbol{W}_{k} \in \mathbb{R}^{D_{k} \times D_{x}}, \boldsymbol{W}_{v} \in \mathbb{R}^{D_{v} \times D_{x}}</span><script type="math/tex">\boldsymbol{W}_{q} \in \mathbb{R}^{D_{k} \times D_{x}}, \boldsymbol{W}_{k} \in \mathbb{R}^{D_{k} \times D_{x}}, \boldsymbol{W}_{v} \in \mathbb{R}^{D_{v} \times D_{x}}</script></span> f分别为线性映射的参数矩阵，<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{Q}=\left[\boldsymbol{q}_{1}, \cdots, \boldsymbol{q}_{N}\right], \boldsymbol{K}=\left[\boldsymbol{k}_{1}, \cdots, \boldsymbol{k}_{N}\right], \boldsymbol{V}=\left[\boldsymbol{v}_{1}, \cdots, \boldsymbol{v}_{N}\right]</span><script type="math/tex">\boldsymbol{Q}=\left[\boldsymbol{q}_{1}, \cdots, \boldsymbol{q}_{N}\right], \boldsymbol{K}=\left[\boldsymbol{k}_{1}, \cdots, \boldsymbol{k}_{N}\right], \boldsymbol{V}=\left[\boldsymbol{v}_{1}, \cdots, \boldsymbol{v}_{N}\right]</script></span> 分别是由查询向量、键向量和值向量构成的矩阵.</p>
<p>(2)对于每一个查询向量<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{q}_n \in \boldsymbol{Q}</span><script type="math/tex">\boldsymbol{q}_n \in \boldsymbol{Q}</script></span>，利用键值对注意力机制，可以得到输出向量<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{h}_n</span><script type="math/tex">\boldsymbol{h}_n</script></span> </p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{align}
\boldsymbol{h}_{n}
&amp;=\operatorname{att}\left((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{q}_{n}\right)\\
&amp;=\sum_{j=1}^{N} \alpha_{n j} \boldsymbol{v}_{j} \\
&amp;=\sum_{j=1}^{N} \operatorname{softmax}\left(s\left(\boldsymbol{k}_{j}, \boldsymbol{q}_{n}\right)\right) \boldsymbol{v}_{j}
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
\boldsymbol{h}_{n}
&=\operatorname{att}\left((\boldsymbol{K}, \boldsymbol{V}), \boldsymbol{q}_{n}\right)\\
&=\sum_{j=1}^{N} \alpha_{n j} \boldsymbol{v}_{j} \\
&=\sum_{j=1}^{N} \operatorname{softmax}\left(s\left(\boldsymbol{k}_{j}, \boldsymbol{q}_{n}\right)\right) \boldsymbol{v}_{j}
\end{align}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">n,j\in [1,\cdots,N]</span><script type="math/tex">n,j\in [1,\cdots,N]</script></span>为输出和输入向量序列的位置，<span class="arithmatex"><span class="MathJax_Preview">\alpha_{nj}</span><script type="math/tex">\alpha_{nj}</script></span>表示第<span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>个输出关注到第<span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>个输入的权重.
如果使用缩放点积来作为注意力打分函数，输出向量序列可以简写为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\boldsymbol{H}=\boldsymbol{V} \operatorname{softmax}\left(\frac{\boldsymbol{K}^{\top} \boldsymbol{Q}}{\sqrt{D_{k}}}\right)
</div>
<script type="math/tex; mode=display">
\boldsymbol{H}=\boldsymbol{V} \operatorname{softmax}\left(\frac{\boldsymbol{K}^{\top} \boldsymbol{Q}}{\sqrt{D_{k}}}\right)
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">\operatorname{softmax}(\cdot)</span><script type="math/tex">\operatorname{softmax}(\cdot)</script></span>为按列进行归一化的函数。下图中给出全连接模型和自注意力模型的对比，其中实线表示可学习的权重，虚线表示动态生成的权重.由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列.</p>
<p><img _="," alt="image" height="80%" src="../assets/image-20210125233301077.png" width="80%" /></p>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../06_%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 网络正则化" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              网络正则化
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs"], "search": "../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>