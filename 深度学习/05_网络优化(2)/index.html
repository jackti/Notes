
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="采用Mkdocs-material生成的文档管理网站支持的markdown语法，包括传统语法和扩展语法">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>数据预处理 - 我的知识笔记</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.9647289d.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="pink">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="我的知识笔记" class="md-header__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            我的知识笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              数据预处理
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-tabs__link">
        机器学习
      </a>
    </li>
  

  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-tabs__link md-tabs__link--active">
        深度学习
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="我的知识笔记" class="md-nav__button md-logo" aria-label="我的知识笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    我的知识笔记
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          机器学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="机器学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          机器学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          01 监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="01 监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          01 监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E5%85%B6%E4%BB%96%E8%AE%B0%E5%BD%95/" class="md-nav__link">
        数学符号
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/00_%E6%80%BB%E7%BB%93/" class="md-nav__link">
        线性模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/01_%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/" class="md-nav__link">
        最小二乘法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/02_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/" class="md-nav__link">
        梯度下降算法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/03_%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8E%9F%E7%90%86/" class="md-nav__link">
        交叉验证原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/04_%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="md-nav__link">
        模型评估
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/05_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        线性回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/06_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        逻辑回归算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/07_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        决策树算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/08_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        决策树算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/09_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        决策树算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/10_%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        近邻算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/11_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        朴素贝叶斯算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/12_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        最大熵算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/13_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        最大熵算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/14_%E6%9C%80%E5%A4%A7%E7%86%B5%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        最大熵算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/15_%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        感知机算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/16_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%80%29/" class="md-nav__link">
        支持向量机算法原理(一)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/17_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%8C%29/" class="md-nav__link">
        支持向量机算法原理(二)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/18_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%B8%89%29/" class="md-nav__link">
        支持向量机算法原理(三)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/19_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E5%9B%9B%29/" class="md-nav__link">
        支持向量机算法原理(四)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/20_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%28%E4%BA%94%29/" class="md-nav__link">
        支持向量机算法原理(五)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/21_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        集成学习算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/22_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BAdaboost/" class="md-nav__link">
        集成学习算法之Adaboost
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/23_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BRF/" class="md-nav__link">
        集成学习算法之RF
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/24_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8BGBDT/" class="md-nav__link">
        集成学习算法之GBDT
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          02 无监督学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="02 无监督学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          02 无监督学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/25_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BK-Means/" class="md-nav__link">
        聚类算法之K-Means​
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/26_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BBIRCH/" class="md-nav__link">
        聚类算法之BIRCH
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/27_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BDBSCAN/" class="md-nav__link">
        聚类算法之DBSCAN
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/28_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8BMean%20Shift/" class="md-nav__link">
        聚类算法之Mean Shift
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/29_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E4%B9%8B%E8%B0%B1%E8%81%9A%E7%B1%BB/" class="md-nav__link">
        聚类算法之谱聚类
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/30_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BPCA/" class="md-nav__link">
        降维算法之PCA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/31_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLDA/" class="md-nav__link">
        降维算法之LDA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/32_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BIsomap/" class="md-nav__link">
        降维算法之Isomap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/33_%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E4%B9%8BLLE/" class="md-nav__link">
        降维算法之LLE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/34_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BApriori/" class="md-nav__link">
        关联算法之Apriori
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/35_%E5%85%B3%E8%81%94%E7%AE%97%E6%B3%95%E4%B9%8BFP-Tree/" class="md-nav__link">
        关联算法之FP-Tree
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/36_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BB%BC%E8%BF%B0/" class="md-nav__link">
        推荐算法之综述
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/37_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/" class="md-nav__link">
        推荐算法之矩阵分解
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/38_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BSimRank/" class="md-nav__link">
        推荐算法之SimRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/39_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E4%B9%8BPersonalRank/" class="md-nav__link">
        推荐算法之PersonalRank
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/40_EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        EM算法原理
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02_%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/41_%E5%88%86%E8%A7%A3%E6%9C%BA%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" class="md-nav__link">
        分解机算法原理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          深度学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="深度学习" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          深度学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../00_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        神经网络基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_DNN%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        DNN前馈神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        CNN卷积神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        RNN循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_%E5%9F%BA%E4%BA%8E%E9%97%A8%E6%8E%A7%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="md-nav__link">
        基于门控的循环神经网络
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%281%29/" class="md-nav__link">
        神经网络优化概述
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          数据预处理
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        数据预处理
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    最小最大值归一化
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    标准化
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    白化
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" class="md-nav__link">
        网络正则化
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="md-nav__link">
        07 注意力机制
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    最小最大值归一化
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    标准化
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    白化
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="_1">数据预处理<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<p>一般而言，样本特征由于来源以及度量单位不同，它们的<code>尺度（Scale）</code>即取值范围往往很大，如果一个机器学习算法在缩放全部或部分特征后不影响它的学习和预测，那么称改算法具有<code>尺度不变性（Scale Invariance）</code>.从理论上，神经网络应该具有尺度不变性，可以通过参数的调整来适应不同特征的尺度，但尺度不同的输入特征会增加训练难度.
归一化（Normalization）方法泛指把数据特征转换为相同尺度的方法，比如把数据特征映射到[0,1]或者[-1,1]区间内，或者映射为服从均值为0，方差为1的标准正态分布。归一化的方法有很多种，常用有==最小最大值归一化、标准化和白化==.</p>
<h2 id="_2">最小最大值归一化<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>最小最大值归一化（Min-Max Normalization）是一种简单的归一化方法，通过缩放将每一个特征的取值范围归一到[0,1]或[-1,1]之间，假设有<span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>个样本<span class="arithmatex"><span class="MathJax_Preview">\{x^n\}_{n=1}^N</span><script type="math/tex">\{x^n\}_{n=1}^N</script></span>,对于每一维特征<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>归一化后的特征为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{x}=\frac{x-\operatorname{min}\{x^n\}_{n=1}^N}{\operatorname{max}\{x^n\}_{n=1}^N-\operatorname{min}\{x^n\}_{n=1}^N}
</div>
<script type="math/tex; mode=display">
\hat{x}=\frac{x-\operatorname{min}\{x^n\}_{n=1}^N}{\operatorname{max}\{x^n\}_{n=1}^N-\operatorname{min}\{x^n\}_{n=1}^N}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">\operatorname{min}</span><script type="math/tex">\operatorname{min}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\operatorname{max}</span><script type="math/tex">\operatorname{max}</script></span>分别是特征<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>在所有样本上的最大值和最小值.</p>
<h2 id="_3">标准化<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<p>标准化（Standandization）也叫做Z值归一化（Z-Score Normalaization）将每一维特征都调整为均值为0，方差为1.假设有<span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>个样本<span class="arithmatex"><span class="MathJax_Preview">\{x^n\}_{n=1}^N</span><script type="math/tex">\{x^n\}_{n=1}^N</script></span>,对于每一维特征<span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>归一化后的特征为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
    \mu &amp;= \frac{1}{N}\sum_{n=1}^N x^n \\
    \sigma^2 &amp;= \frac{1}{N}\sum_{n=1}^N (x^n-\mu)\\
    \hat{x} &amp;= \frac{x-\mu}{\sigma}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
    \mu &= \frac{1}{N}\sum_{n=1}^N x^n \\
    \sigma^2 &= \frac{1}{N}\sum_{n=1}^N (x^n-\mu)\\
    \hat{x} &= \frac{x-\mu}{\sigma}
\end{aligned}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>不能为0，如果标准差为0，说明这一维度特征没有任务区分性，可以直接删掉.</p>
<h2 id="_4">白化<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<p>白化（Whitening）是一种重要的预处理方法，用来降低输入数据特征之间的冗余性，输入数据经过白化处理后，特征之间的相关性降低，并且所有特征具有相同的方差，白化的一个主要实现方式是使用主成分分析（Principal Component Analysis,PCA）方法去除掉各个成分之间的相关性.</p>
<p><img src="assets/image-20210117123835707.png" alt="image-20210117123835707" style="zoom:100%;" /></p>
<h1 id="_5">逐层归一化<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h1>
<p>逐层归一化（Layer-wise Normalization）是将传统机器学习中的数据归一化方法应用到深度神经网络中，对神经网络中隐藏层的**输入**进行归一化，从而使得网络更加容易训练。逐层归一化可以提高训练效率的原因有以下几个方面：
1. <strong>更好的尺度不变性</strong>：将每个神经元的输入分布都归一化为标准正态分布，使得每个神经层对其输入具有更好的尺度不变性。无论低层的参数如何变化，高层的输入保持相对稳定.此外，尺度不变性可以更加高效地进行参数初始化以及超参数选择；
2. <strong>更平滑地优化地形</strong>：逐层归一化一方面可以使得大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失问题;另一方面还可以使得 神经网络的优化地形(Optimization Landscape)更加平滑，以及使梯度变得更加稳定，从而允许我们使用更大的学习率，并提高收敛速度.</p>
<p>常见的几种逐层归一化方法有：<mark>批量归一化、层归一化、权重归一化和局部响应归一化</mark>.</p>
<h2 id="_6">批量归一化<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h2>
<p>批量归一化（Batch Normalization，BN）方法是一种有效的逐层归一化方法，可以对神经网络中任意中间层进行归一化操作.对于一个深度神经网络，令第<span class="arithmatex"><span class="MathJax_Preview">l</span><script type="math/tex">l</script></span>层的净输入为<span class="arithmatex"><span class="MathJax_Preview">z^l</span><script type="math/tex">z^l</script></span>，神经元的输出为<span class="arithmatex"><span class="MathJax_Preview">a^l</span><script type="math/tex">a^l</script></span>，即</p>
<div class="arithmatex">
<div class="MathJax_Preview">
a^l = f(z^l)=f(W a^{l-1}+b)
</div>
<script type="math/tex; mode=display">
a^l = f(z^l)=f(W a^{l-1}+b)
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">f(\cdot)</span><script type="math/tex">f(\cdot)</script></span>是激活函数，<span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>和<span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span>是可学习参数.</p>
<p>为了提高优化效率，就需要使得净输入<span class="arithmatex"><span class="MathJax_Preview">z^l</span><script type="math/tex">z^l</script></span>的分布一致，比如都归一化到标准正态分布.虽然归一化操作也可以应用在输入<span class="arithmatex"><span class="MathJax_Preview">a^{l-1}</span><script type="math/tex">a^{l-1}</script></span>,但归一化<span class="arithmatex"><span class="MathJax_Preview">z^l</span><script type="math/tex">z^l</script></span>更加有利于优化。可以使用数据预处理方法对<span class="arithmatex"><span class="MathJax_Preview">z^l</span><script type="math/tex">z^l</script></span>进行归一化，相当于每一层都进行一次数据预处理，从而加速收敛速度，但是逐层归一化需要在中间层进行操作，要求效率比较高，因此复杂度比较高的白化方法不太适合.</p>
<p>为了提高归一化的效率，一般使用标准化将净输入<span class="arithmatex"><span class="MathJax_Preview">z^l</span><script type="math/tex">z^l</script></span>的每一维度都归一到标准正态分布。因为目前主要的优化算法是基于小批量的随机梯度下降法，所以准确地计算<span class="arithmatex"><span class="MathJax_Preview">z^l</span><script type="math/tex">z^l</script></span>的期望和方差是不可行的.因此，<span class="arithmatex"><span class="MathJax_Preview">z^l</span><script type="math/tex">z^l</script></span>的期望和方差通常用当前小批量样本集的均值和方差近似估计.</p>
<p>给定一个包含<span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>个样本的小批量样本集合，第<span class="arithmatex"><span class="MathJax_Preview">l</span><script type="math/tex">l</script></span>层神经元的净输入<span class="arithmatex"><span class="MathJax_Preview">\mathcal{B}=\{z^{(1,l)},\cdots,z^{(K,l)}\}</span><script type="math/tex">\mathcal{B}=\{z^{(1,l)},\cdots,z^{(K,l)}\}</script></span>的均值和方差为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
  \mu_{\mathcal{B}} &amp;= \frac{1}{K}\sum_{k=1}^K z^{(k,l)}  \\
  \sigma_{\mathcal{B}}^2 &amp;= \frac{1}{k}\sum_{k=1}^{K}(z^{(k,l)}-\mu_{\mathcal{B}}) \odot (z^{(k,l)}-\mu_{\mathcal{B}})
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
  \mu_{\mathcal{B}} &= \frac{1}{K}\sum_{k=1}^K z^{(k,l)}  \\
  \sigma_{\mathcal{B}}^2 &= \frac{1}{k}\sum_{k=1}^{K}(z^{(k,l)}-\mu_{\mathcal{B}}) \odot (z^{(k,l)}-\mu_{\mathcal{B}})
\end{aligned}
</script>
</div>
<p>对净输入<span class="arithmatex"><span class="MathJax_Preview">z^l</span><script type="math/tex">z^l</script></span>的标准归一化使得其取值集中到0附近，如果使用<code>Sigmoid</code>型激活函数时，这个取值区间刚好是接近线性变换的区间，减弱了神经网络的非线性性质。因此，为了使得归一化不对网络的表示能力造成负面影响，可以通过加一个==缩放==和==平移==变换改变取值区间</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\hat{z}^l &amp;= \frac{z^l -\mu_\mathcal{B}}{\sqrt{\sigma^2_{\mathcal{B}}+\epsilon}}\odot \boldsymbol{\gamma} +\boldsymbol{\beta} \\
&amp;\triangleq \operatorname{BN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(z^l)  
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{z}^l &= \frac{z^l -\mu_\mathcal{B}}{\sqrt{\sigma^2_{\mathcal{B}}+\epsilon}}\odot \boldsymbol{\gamma} +\boldsymbol{\beta} \\
&\triangleq \operatorname{BN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(z^l)  
\end{aligned}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{\gamma},\boldsymbol{\beta}</span><script type="math/tex">\boldsymbol{\gamma},\boldsymbol{\beta}</script></span>分别表示缩放和平移的参数向量.批量归一化操作可以看作一个特殊的神经层，加在每一层非线性激活函数之前，即</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\boldsymbol{a}^l
=f(\operatorname{BN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(z^l))
=f(\operatorname{BN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(W \boldsymbol{a}^{l-1}))
</div>
<script type="math/tex; mode=display">
\boldsymbol{a}^l
=f(\operatorname{BN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(z^l))
=f(\operatorname{BN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(W \boldsymbol{a}^{l-1}))
</script>
</div>
<p>其中因为批量归一化本身具有平移变换，所以仿射变换<span class="arithmatex"><span class="MathJax_Preview">W\boldsymbol{a}^{l-1}</span><script type="math/tex">W\boldsymbol{a}^{l-1}</script></span>不再需要偏执参数.需要注意的有两点：
+ <span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{\gamma},\boldsymbol{\beta}</span><script type="math/tex">\boldsymbol{\gamma},\boldsymbol{\beta}</script></span>是可学习的变量，<span class="arithmatex"><span class="MathJax_Preview">\operatorname{BN}</span><script type="math/tex">\operatorname{BN}</script></span>可看作是以<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{\gamma},\boldsymbol{\beta}</span><script type="math/tex">\boldsymbol{\gamma},\boldsymbol{\beta}</script></span>为参数的映射函数
+ 当预测阶段，用整个数据集上的均值<span class="arithmatex"><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span>和方差<span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>来分别代替每次小批量样本的<span class="arithmatex"><span class="MathJax_Preview">\mu_\mathcal{B}</span><script type="math/tex">\mu_\mathcal{B}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\sigma_\mathcal{B}</span><script type="math/tex">\sigma_\mathcal{B}</script></span>，在实践中，<span class="arithmatex"><span class="MathJax_Preview">\mu_\mathcal{B}</span><script type="math/tex">\mu_\mathcal{B}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\sigma_\mathcal{B}</span><script type="math/tex">\sigma_\mathcal{B}</script></span>也可以用移动平均来计算.</p>
<blockquote>
<p>逐层归一化不但可以提高优化效率，还可以作为一种隐形的正则化方法.在训练时，神经网络对一个样本的预测不仅和该样本自身相关，也和同一批次中的其他样本相关.由于在选取批次时具有随机性，因此使得神经网 络不会“过拟合”到某个特定样本，从而提高网络的泛化能力 </p>
</blockquote>
<h2 id="_7">层归一化<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h2>
<p>批量归一化是对一个中间层的单个神经元进行归一化操作，因此要求小批量样本的数量不能太小，否则难以计算单个神经元的统计信息.此外，如果一个神经元的净输入的分布在神经网络中是动态变化的，比如循环神经网络，那么就无法应用批量归一化操作.</p>
<p>层归一化(Layer Normalization)是和批量归一化非常类似的方法，和批量归一化不同的是，层归一化是对一个中间层的所有神经元进行归一化.对于一个深度神经网络，令第<span class="arithmatex"><span class="MathJax_Preview">l</span><script type="math/tex">l</script></span>层的神经元的净输入为<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{z}^l</span><script type="math/tex">\boldsymbol{z}^l</script></span>，其均值和方差为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
  \mu^{l}&amp;=\frac{1}{M_{l}}\sum_{i=1}^{M_l} \boldsymbol{z}_i^l \\
(\sigma^{l})^2 &amp;=\frac{1}{M_{l}}\sum_{i=1}^{M_l} (\boldsymbol{z}_i^l-\mu^l)^2
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
  \mu^{l}&=\frac{1}{M_{l}}\sum_{i=1}^{M_l} \boldsymbol{z}_i^l \\
(\sigma^{l})^2 &=\frac{1}{M_{l}}\sum_{i=1}^{M_l} (\boldsymbol{z}_i^l-\mu^l)^2
\end{aligned}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">M_l</span><script type="math/tex">M_l</script></span>是第<span class="arithmatex"><span class="MathJax_Preview">l</span><script type="math/tex">l</script></span>层神经元的数量
层归一化定义为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\hat{\boldsymbol{z}}^l &amp;= \frac{\boldsymbol{z}^l -\mu}{\sqrt{(\sigma^l)^2+\epsilon}}\odot \boldsymbol{\gamma} +\boldsymbol{\beta} \\
&amp;\triangleq \operatorname{LN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(\boldsymbol{z}^l)  
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{\boldsymbol{z}}^l &= \frac{\boldsymbol{z}^l -\mu}{\sqrt{(\sigma^l)^2+\epsilon}}\odot \boldsymbol{\gamma} +\boldsymbol{\beta} \\
&\triangleq \operatorname{LN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(\boldsymbol{z}^l)  
\end{aligned}
</script>
</div>
<p>其中<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{\gamma}</span><script type="math/tex">\boldsymbol{\gamma}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{\beta}</span><script type="math/tex">\boldsymbol{\beta}</script></span>分别代表缩放和平移向量，和<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{z}^l</span><script type="math/tex">\boldsymbol{z}^l</script></span>维度相同.</p>
<p>层归一化可以应用在循环神经网络中，对循环神经网络进行归一化操作，假设在时刻<span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>，循环神经网络的隐藏层为<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{h}_t</span><script type="math/tex">\boldsymbol{h}_t</script></span>，其归一化的更新为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
  \boldsymbol{z}_l = \boldsymbol{U} \boldsymbol{h}_{t-1}+\boldsymbol{W} \boldsymbol{x}_t \\
  \boldsymbol{h}_t = f( \operatorname{LN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(\boldsymbol{z}^l) )
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
  \boldsymbol{z}_l = \boldsymbol{U} \boldsymbol{h}_{t-1}+\boldsymbol{W} \boldsymbol{x}_t \\
  \boldsymbol{h}_t = f( \operatorname{LN}_{\boldsymbol{\gamma},\boldsymbol{\beta}}(\boldsymbol{z}^l) )
\end{aligned}
</script>
</div>
<p>其中输入为<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{x}_t</span><script type="math/tex">\boldsymbol{x}_t</script></span>为第<span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>时刻的输入，<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{U}</span><script type="math/tex">\boldsymbol{U}</script></span>和<span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{W}</span><script type="math/tex">\boldsymbol{W}</script></span>为网络参数.在标准循环神经网络中，循环神经层的净输入一般会随着时间慢慢变大或变小，从而导致梯度爆炸或消失.而层归一化的循环神经网络可以有效地缓解这种状况.</p>
<p>层归一化和批量归一化整体上是十分类似的，差别在于归一化的方法不同. 对于<span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>个样本的一个小批量集合<span class="arithmatex"><span class="MathJax_Preview">𝒁^{(𝑙)} = [𝒛^{(1,𝑙)}; ⋯ ; 𝒛^{(𝐾,𝑙)}]</span><script type="math/tex">𝒁^{(𝑙)} = [𝒛^{(1,𝑙)}; ⋯ ; 𝒛^{(𝐾,𝑙)}]</script></span>，层归一化是对矩阵<span class="arithmatex"><span class="MathJax_Preview">𝒁^{(𝑙)}</span><script type="math/tex">𝒁^{(𝑙)}</script></span>的每一列进行归一化，而批量归一化是对每一行进行归一化.一般而言，批量归一化是一种更好的选择.当小批量样本数量比较小时，可以选择层归一化.</p>
<p>此外还有一些归一化方法如权重归一化和局部响应归一化等.</p>
<h1 id="_8">超参数优化<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h1>
<p>在神经网络中，除了可学习的参数之外，还存在很多超参数,这些超参数对网络性能的影响也很大，不同的机器学习任务往往需要不同的超参数，常见的超参数有以下三类：
1. 网络结构：包括神经元之间的连接关系、层数、每层神经元数量和激活函数的类型等
2. 优化参数：包括优化方法、学习率和小批量的样本数量等
3. 正则化系数</p>
<p>超参数优化(Hyperparameter Optimization)主要存在两方面的困难:
1. 超参数优化是一个组合优化问题，无法像一般参数那样通过梯度下降方法来优化，也没有一种通用有效的优化方法;
2. 评估一组超参数配置(Configuration) 的时间代价非常高，从而导致一些优化方法(比如演化算法(Evolution Algo- rithm))在超参数优化中难以应用.</p>
<p>对于超参数的配置，比较简单的方法有==网格搜索、随机搜索、贝叶斯优化、动 态资源分配和神经架构搜索==.</p>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="页脚">
      
        
        <a href="../05_%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%281%29/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 神经网络优化概述" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              神经网络优化概述
            </div>
          </div>
        </a>
      
      
        
        <a href="../06_%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%88%99%E5%8C%96/" class="md-footer__link md-footer__link--next" aria-label="下一页: 网络正则化" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              网络正则化
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs"], "search": "../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>